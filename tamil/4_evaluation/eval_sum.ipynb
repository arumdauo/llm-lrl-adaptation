{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0VyKC64ugVUb2U6vDQ6vX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JzEU9G0mEz0Z"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import logging\n","import re\n","import random\n","import getpass\n","from pathlib import Path\n","from typing import Dict, List, Optional\n","import numpy as np\n","from tqdm import tqdm\n","from transformers import (\n","    LlamaForCausalLM,\n","    GemmaForCausalLM,\n","    AutoTokenizer,\n","    set_seed\n",")\n","from rouge_score import rouge_scorer\n","import sys\n","from dataclasses import dataclass\n","from functools import lru_cache\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    handlers=[logging.StreamHandler(sys.stdout)]\n",")\n","logger = logging.getLogger(__name__)\n","\n","@dataclass\n","class ModelConfig:\n","    name: str\n","    model_path: str\n","    tokenizer_path: str\n","    model_type: str\n","    use_extended_tokenizer: bool = False\n","    bpe_model_path: Optional[str] = None\n","    hf_token_var: Optional[str] = None\n","\n","@dataclass\n","class EvaluationConfig:\n","    train_data_path: str\n","    test_data_path: str\n","    seed: int = 42\n","    max_context_length: int = 2048\n","    max_new_tokens: int = 150\n","    temperature: float = 0.7\n","    top_p: float = 0.9\n","    top_k: int = 50\n","    repetition_penalty: float = 1.05\n","\n","class BPETokenizer:\n","    def __init__(self, model_path: str):\n","        self.model_path = Path(model_path)\n","        with open(self.model_path / \"simple_bpe_model.json\", 'r', encoding='utf-8') as f:\n","            model_data = json.load(f)\n","\n","        self.vocab = model_data['vocab']\n","        self.id2token = {int(k): v for k, v in model_data['id2token'].items()}\n","        self.merges = [(m['left'], m['right']) for m in model_data['merges']]\n","        self._merge_pairs = set(self.merges)\n","\n","        self.unk_token = '<unk>'\n","        self.bos_token = '<s>'\n","        self.eos_token = '</s>'\n","        self.pad_token = '<pad>'\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.vocab)\n","\n","    def get_vocab(self):\n","        return self.vocab.copy()\n","\n","    def _tokenize(self, text: str):\n","        if not text:\n","            return []\n","\n","        tokens = [char if char in self.vocab else self.unk_token for char in text]\n","        return self._apply_merges(tokens)\n","\n","    def _apply_merges(self, tokens):\n","        changed = True\n","        while changed:\n","            changed = False\n","            i = 0\n","            while i < len(tokens) - 1:\n","                if (tokens[i], tokens[i + 1]) in self._merge_pairs:\n","                    tokens[i:i + 2] = [tokens[i] + tokens[i + 1]]\n","                    changed = True\n","                else:\n","                    i += 1\n","        return tokens\n","\n","    @lru_cache(maxsize=10000)\n","    def _convert_token_to_id(self, token):\n","        return self.vocab.get(token, self.vocab.get(self.unk_token, 0))\n","\n","    @lru_cache(maxsize=10000)\n","    def _convert_id_to_token(self, index):\n","        return self.id2token.get(index, self.unk_token)\n","\n","    def convert_tokens_to_string(self, tokens):\n","        return ''.join(tokens)\n","\n","class ExtendedTokenizer:\n","    def __init__(self, base_tokenizer: AutoTokenizer, bpe_tokenizer: BPETokenizer, combined_vocab: Dict[str, int]):\n","        self.base_tokenizer = base_tokenizer\n","        self.bpe_tokenizer = bpe_tokenizer\n","        self.vocab = combined_vocab\n","        self.id2token = {token_id: token for token, token_id in combined_vocab.items()}\n","\n","        self.tamil_pattern = re.compile(r'[\\u0B80-\\u0BFF]')\n","        self.english_pattern = re.compile(r'[a-zA-Z]')\n","\n","        self.unk_token = base_tokenizer.unk_token\n","        self.bos_token = base_tokenizer.bos_token\n","        self.eos_token = base_tokenizer.eos_token\n","        self.pad_token = '<pad>'\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.vocab)\n","\n","    @property\n","    def pad_token_id(self):\n","        return self.vocab.get(self.pad_token, 32000)\n","\n","    @property\n","    def eos_token_id(self):\n","        return self.vocab.get(self.eos_token, 2)\n","\n","    def _classify_text_type(self, text: str) -> str:\n","        analyzable_chars = re.sub(r'[^\\w]', '', text, flags=re.UNICODE)\n","        if not analyzable_chars:\n","            return \"base\"\n","\n","        tamil_chars = len(self.tamil_pattern.findall(analyzable_chars))\n","        if tamil_chars == 0:\n","            return \"base\"\n","\n","        english_chars = len(self.english_pattern.findall(analyzable_chars))\n","        if english_chars == 0 and tamil_chars / len(analyzable_chars) > 0.8:\n","            return \"bpe\"\n","\n","        return \"base\"\n","\n","    def encode(self, text: str, add_special_tokens=True, return_tensors=None):\n","        text_type = self._classify_text_type(text)\n","\n","        if text_type == \"bpe\":\n","            try:\n","                tokens = self.bpe_tokenizer._tokenize(text)\n","                if all(token in self.vocab for token in tokens):\n","                    token_ids = [self.vocab[token] for token in tokens]\n","                else:\n","                    return self.base_tokenizer.encode(text, add_special_tokens=add_special_tokens, return_tensors=return_tensors)\n","            except:\n","                return self.base_tokenizer.encode(text, add_special_tokens=add_special_tokens, return_tensors=return_tensors)\n","        else:\n","            return self.base_tokenizer.encode(text, add_special_tokens=add_special_tokens, return_tensors=return_tensors)\n","\n","        if add_special_tokens:\n","            token_ids = [self.vocab[self.bos_token]] + token_ids + [self.vocab[self.eos_token]]\n","\n","        if return_tensors == \"pt\":\n","            return torch.tensor([token_ids])\n","        return token_ids\n","\n","    def decode(self, token_ids, skip_special_tokens=True):\n","        if hasattr(token_ids, 'tolist'):\n","            token_ids = token_ids.tolist()\n","        if isinstance(token_ids[0], list):\n","            token_ids = token_ids[0]\n","\n","        tokens = [self.id2token.get(tid, self.unk_token) for tid in token_ids]\n","\n","        if skip_special_tokens:\n","            special_tokens = {self.bos_token, self.eos_token, self.pad_token}\n","            tokens = [t for t in tokens if t not in special_tokens]\n","\n","        base_vocab = set(self.base_tokenizer.get_vocab().keys())\n","        non_base_tokens = sum(1 for token in tokens if token not in base_vocab)\n","\n","        if non_base_tokens > len(tokens) * 0.5:\n","            return ''.join(tokens)\n","        return self.base_tokenizer.convert_tokens_to_string(tokens)\n","\n","class MultiModelEvaluator:\n","    def __init__(self, config: EvaluationConfig, model_configs: List[ModelConfig]):\n","        set_seed(config.seed)\n","        self.config = config\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.models = {}\n","        self.tokenizers = {}\n","\n","        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n","\n","        for model_config in model_configs:\n","            self._load_model(model_config)\n","\n","    def _load_model(self, model_config: ModelConfig):\n","        logger.info(f\"Loading {model_config.name}\")\n","\n","        token = None\n","        if model_config.hf_token_var and model_config.hf_token_var in os.environ:\n","            token = os.environ[model_config.hf_token_var]\n","\n","        try:\n","            if model_config.use_extended_tokenizer:\n","                base_tokenizer = AutoTokenizer.from_pretrained(model_config.tokenizer_path, token=token)\n","                bpe_tokenizer = BPETokenizer(model_config.bpe_model_path)\n","\n","                vocab_file = Path(model_config.tokenizer_path) / 'vocab.json'\n","                with open(vocab_file, 'r', encoding='utf-8') as f:\n","                    combined_vocab = json.load(f)\n","\n","                tokenizer = ExtendedTokenizer(base_tokenizer, bpe_tokenizer, combined_vocab)\n","            else:\n","                tokenizer = AutoTokenizer.from_pretrained(model_config.tokenizer_path, token=token)\n","                if tokenizer.pad_token is None:\n","                    tokenizer.pad_token = tokenizer.eos_token\n","\n","            if model_config.model_type == 'llama2':\n","                model = LlamaForCausalLM.from_pretrained(\n","                    model_config.model_path,\n","                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n","                    device_map=\"auto\" if torch.cuda.is_available() else None,\n","                    low_cpu_mem_usage=True,\n","                    token=token\n","                )\n","            elif model_config.model_type == 'gemma':\n","                model = GemmaForCausalLM.from_pretrained(\n","                    model_config.model_path,\n","                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n","                    device_map=\"auto\" if torch.cuda.is_available() else None,\n","                    low_cpu_mem_usage=True,\n","                    token=token\n","                )\n","            else:\n","                raise ValueError(f\"Unsupported model type: {model_config.model_type}\")\n","\n","            model.eval()\n","\n","            self.models[model_config.name] = model\n","            self.tokenizers[model_config.name] = tokenizer\n","\n","            logger.info(f\"Successfully loaded {model_config.name}\")\n","\n","        except Exception as e:\n","            logger.error(f\"Failed to load {model_config.name}: {e}\")\n","\n","    def _encode_text(self, text: str, tokenizer, add_special_tokens: bool = True):\n","        if isinstance(tokenizer, ExtendedTokenizer):\n","            return tokenizer.encode(text, add_special_tokens=add_special_tokens)\n","        return tokenizer.encode(text, add_special_tokens=add_special_tokens)\n","\n","    def _decode_tokens(self, token_ids, tokenizer, skip_special_tokens: bool = True) -> str:\n","        if isinstance(tokenizer, ExtendedTokenizer):\n","            return tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n","        return tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n","\n","    def _get_special_token_ids(self, tokenizer):\n","        return {\n","            'pad_token_id': tokenizer.pad_token_id,\n","            'eos_token_id': tokenizer.eos_token_id\n","        }\n","\n","    def generate_summary(self, prompt: str, model_name: str) -> str:\n","        if model_name not in self.models:\n","            return \"\"\n","\n","        model = self.models[model_name]\n","        tokenizer = self.tokenizers[model_name]\n","\n","        try:\n","            input_ids = self._encode_text(prompt, tokenizer, add_special_tokens=True)\n","\n","            if len(input_ids) > self.config.max_context_length - self.config.max_new_tokens:\n","                input_ids = input_ids[-(self.config.max_context_length - self.config.max_new_tokens):]\n","\n","            input_tensor = torch.tensor([input_ids]).to(self.device)\n","            special_tokens = self._get_special_token_ids(tokenizer)\n","\n","            with torch.no_grad():\n","                generated_tokens = model.generate(\n","                    input_ids=input_tensor,\n","                    max_new_tokens=self.config.max_new_tokens,\n","                    do_sample=True,\n","                    temperature=self.config.temperature,\n","                    top_p=self.config.top_p,\n","                    top_k=self.config.top_k,\n","                    pad_token_id=special_tokens['pad_token_id'],\n","                    eos_token_id=special_tokens['eos_token_id'],\n","                    repetition_penalty=self.config.repetition_penalty,\n","                    early_stopping=True,\n","                    use_cache=True\n","                )\n","\n","            new_tokens = generated_tokens[0][len(input_ids):].tolist()\n","            summary = self._decode_tokens(new_tokens, tokenizer, skip_special_tokens=True)\n","            summary = self._post_process_summary(summary)\n","\n","            return summary\n","\n","        except Exception as e:\n","            logger.error(f\"Generation failed for {model_name}: {e}\")\n","            return \"\"\n","\n","    def _post_process_summary(self, summary: str) -> str:\n","        if not summary:\n","            return \"\"\n","\n","        summary = re.sub(r'^[:\\-\\s]+', '', summary)\n","        summary = re.sub(r'\\n+', ' ', summary)\n","        summary = re.sub(r'\\s+', ' ', summary)\n","\n","        if summary and not re.search(r'[.!?।॥]$', summary):\n","            summary += '.'\n","\n","        return summary.strip()\n","\n","    def create_prompt(self, examples: List[Dict], target_text: str, n_shots: int) -> str:\n","        selected_examples = random.sample(examples, min(n_shots, len(examples))) if n_shots > 0 else []\n","\n","        truncated_target = target_text[:800] + \"...\" if len(target_text) > 800 else target_text\n","\n","        if n_shots == 0:\n","            return f\"Summarize the following text concisely:\\n\\n{truncated_target}\\n\\nSummary:\"\n","\n","        prompt_parts = [\"Write a summary in தமிழ் based on the examples given below:\", \"\"]\n","\n","        for i, example in enumerate(selected_examples):\n","            ex_text = example['text'][:400] + \"...\" if len(example['text']) > 400 else example['text']\n","            ex_summary = example['summary'][:100] if len(example['summary']) > 100 else example['summary']\n","\n","            prompt_parts.append(f\"Text {i+1}: {ex_text}\")\n","            prompt_parts.append(f\"Summary {i+1}: {ex_summary}\")\n","            prompt_parts.append(\"\")\n","\n","        prompt_parts.append(f\"Text: {truncated_target}\")\n","        prompt_parts.append(\"Summary:\")\n","\n","        return \"\\n\".join(prompt_parts)\n","\n","    def load_dataset(self, file_path: str, max_samples: Optional[int] = None) -> List[Dict]:\n","        data = []\n","\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line_num, line in enumerate(f):\n","                if max_samples and len(data) >= max_samples:\n","                    break\n","\n","                try:\n","                    example = json.loads(line.strip())\n","\n","                    if all(key in example for key in ['text', 'summary']) and \\\n","                       example['text'].strip() and example['summary'].strip() and \\\n","                       len(example['text']) >= 20 and len(example['summary']) >= 5:\n","\n","                        data.append({\n","                            'text': example['text'].strip(),\n","                            'summary': example['summary'].strip(),\n","                            'id': example.get('id', f'item_{line_num}')\n","                        })\n","                except:\n","                    continue\n","\n","        logger.info(f\"Loaded {len(data)} examples from {file_path}\")\n","        return data\n","\n","    def evaluate_rouge(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n","        rouge_scores = {\n","            'rouge1': 0.0,\n","            'rouge2': 0.0,\n","            'rougeL': 0.0\n","        }\n","\n","        valid_pairs = 0\n","\n","        for pred, ref in zip(predictions, references):\n","            pred = pred.strip()\n","            ref = ref.strip()\n","\n","            if not pred or not ref:\n","                continue\n","\n","            try:\n","                scores = self.rouge_scorer.score(ref, pred)\n","                rouge_scores['rouge1'] += scores['rouge1'].fmeasure\n","                rouge_scores['rouge2'] += scores['rouge2'].fmeasure\n","                rouge_scores['rougeL'] += scores['rougeL'].fmeasure\n","                valid_pairs += 1\n","            except:\n","                continue\n","\n","        if valid_pairs > 0:\n","            for metric in rouge_scores:\n","                rouge_scores[metric] /= valid_pairs\n","\n","        return rouge_scores\n","\n","    def run_evaluation(self, train_data: List[Dict], test_data: List[Dict],\n","                      n_shots_list: List[int] = [0, 3], max_test_samples: int = 50) -> Dict:\n","        results = {}\n","\n","        test_sample = random.sample(test_data, max_test_samples) if len(test_data) > max_test_samples else test_data\n","\n","        logger.info(f\"Evaluating on {len(test_sample)} test samples\")\n","\n","        for model_name in self.models.keys():\n","            logger.info(f\"\\nEvaluating {model_name}\")\n","            model_results = {}\n","\n","            for n_shots in n_shots_list:\n","                config_key = f\"{n_shots}_shot\"\n","                logger.info(f\"  {config_key}\")\n","\n","                predictions = []\n","                references = []\n","\n","                for example in tqdm(test_sample, desc=f\"{model_name}_{config_key}\"):\n","                    prompt = self.create_prompt(train_data, example['text'], n_shots)\n","                    predicted_summary = self.generate_summary(prompt, model_name)\n","\n","                    predictions.append(predicted_summary)\n","                    references.append(example['summary'])\n","\n","                rouge_results = self.evaluate_rouge(predictions, references)\n","\n","                model_results[config_key] = {\n","                    'rouge1': rouge_results['rouge1'],\n","                    'rouge2': rouge_results['rouge2'],\n","                    'rougeL': rouge_results['rougeL']\n","                }\n","\n","                logger.info(f\"    R1: {rouge_results['rouge1']:.4f}, R2: {rouge_results['rouge2']:.4f}, RL: {rouge_results['rougeL']:.4f}\")\n","\n","            results[model_name] = model_results\n","\n","        return results\n","\n","def main():\n","    print(\"=\" * 60)\n","    print(\"TAMIL SUMMARIZATION EVALUATION\")\n","    print(\"=\" * 60)\n","\n","    llama_token = getpass.getpass(\"Enter HuggingFace token for Llama-2: \").strip()\n","    if llama_token:\n","        os.environ['LLAMA_HF_TOKEN'] = llama_token\n","    else:\n","        print(\"Error: Llama token is required.\")\n","        return\n","\n","    gemma_token = getpass.getpass(\"Enter HuggingFace token for Gemma: \").strip()\n","    if gemma_token:\n","        os.environ['GEMMA_HF_TOKEN'] = gemma_token\n","    else:\n","        print(\"Error: Gemma token is required.\")\n","        return\n","\n","    model_configs = [\n","        ModelConfig(\n","            name=\"llama2_7b_base\",\n","            model_path=\"meta-llama/Llama-2-7b-hf\",\n","            tokenizer_path=\"meta-llama/Llama-2-7b-hf\",\n","            model_type=\"llama2\",\n","            hf_token_var=\"LLAMA_HF_TOKEN\"\n","        ),\n","        ModelConfig(\n","            name=\"gemma_7b_base\",\n","            model_path=\"google/gemma-7b\",\n","            tokenizer_path=\"google/gemma-7b\",\n","            model_type=\"gemma\",\n","            hf_token_var=\"GEMMA_HF_TOKEN\"\n","        ),\n","        ModelConfig(\n","            name=\"llama2_mean_init_lapt\",\n","            model_path=\"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/models/llama2_mean_init_lapt/final_merged_model\",\n","            tokenizer_path=\"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/models/llama2_mean_init_lapt/final_merged_model\",\n","            model_type=\"llama2\",\n","            use_extended_tokenizer=True,\n","            bpe_model_path=\"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/tokenizers/grapheme_picky_bpe\"\n","        ),\n","        ModelConfig(\n","            name=\"gemma_mean_init_lapt\",\n","            model_path=\"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/models/gemma_mean_init_lapt/final_merged_model\",\n","            tokenizer_path=\"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/models/gemma_mean_init_lapt/final_merged_model\",\n","            model_type=\"gemma\",\n","            use_extended_tokenizer=True,\n","            bpe_model_path=\"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/tokenizers/grapheme_picky_bpe\"\n","        )\n","    ]\n","\n","    config = EvaluationConfig(\n","        train_data_path=\"/content/drive/MyDrive/Colab Notebooks/LRLs/tamil/4_evaluation/tamil_XLSum_v2.0/tamil_train.jsonl\",\n","        test_data_path=\"/content/drive/MyDrive/Colab Notebooks/LRLs/tamil/4_evaluation/tamil_XLSum_v2.0/tamil_test.jsonl\",\n","        seed=42,\n","        max_context_length=2048,\n","        max_new_tokens=150,\n","        temperature=0.5,\n","        top_p=0.9,\n","        top_k=50\n","    )\n","\n","    try:\n","        evaluator = MultiModelEvaluator(config, model_configs)\n","\n","        train_data = evaluator.load_dataset(config.train_data_path, max_samples=100)\n","        test_data = evaluator.load_dataset(config.test_data_path, max_samples=20)\n","\n","        logger.info(\"\\nStarting evaluation\")\n","\n","        results = evaluator.run_evaluation(\n","            train_data=train_data,\n","            test_data=test_data,\n","            n_shots_list=[0, 3],\n","            max_test_samples=10\n","        )\n","\n","        logger.info(\"\\n\" + \"=\"*60)\n","        logger.info(\"COMPARATIVE RESULTS\")\n","        logger.info(\"=\"*60)\n","\n","        for config_name in ['0_shot', '3_shot']:\n","            logger.info(f\"\\n{config_name.upper()}:\")\n","            logger.info(\"-\" * 50)\n","            for model_name in results.keys():\n","                if config_name in results[model_name]:\n","                    metrics = results[model_name][config_name]\n","                    logger.info(f\"{model_name:25s}: R1={metrics['rouge1']:.4f}, R2={metrics['rouge2']:.4f}, RL={metrics['rougeL']:.4f}\")\n","\n","        output_path = Path(\"tamil_evaluation_results.json\")\n","        with open(output_path, 'w', encoding='utf-8') as f:\n","            json.dump(results, f, ensure_ascii=False, indent=2)\n","        logger.info(f\"\\nResults saved to {output_path}\")\n","\n","        return results\n","\n","    except Exception as e:\n","        logger.error(f\"Evaluation failed: {e}\")\n","        raise\n","\n","if __name__ == \"__main__\":\n","    results = main()"],"metadata":{"id":"CA3o5rolE0Y9"},"execution_count":null,"outputs":[]}]}