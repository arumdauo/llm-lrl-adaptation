{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["ff2Ybq-gO1cN","feJymxZZXHg5"],"authorship_tag":"ABX9TyN4xtfTjbpm/PsbSGF/FHoS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wPOv4OoKOgem"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install tqdm cupy-cuda12x grapheme"],"metadata":{"id":"qTScHbbQO01p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Standard BPE"],"metadata":{"id":"ff2Ybq-gO1cN"}},{"cell_type":"code","source":["from __future__ import annotations\n","import json\n","import logging\n","import time\n","import argparse\n","import re\n","import sys\n","import unicodedata\n","from pathlib import Path\n","from collections import defaultdict, Counter\n","from typing import Union, Optional, Dict, List\n","import numpy as np\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    datefmt='%Y-%m-%d %H:%M:%S',\n","    handlers=[\n","        logging.StreamHandler(sys.stdout),\n","    ],\n","    force=True\n",")\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","WHITESPACE = '▁'\n","PAD = '<pad>'\n","UNK = '<unk>'\n","BOS = '<s>'\n","EOS = '</s>'\n","\n","class MCounter(Counter):\n","    \"\"\"extended Counter class with multiplication support\"\"\"\n","    def __mul__(self, other):\n","        if not isinstance(other, int):\n","            raise TypeError(\"Non-int factor\")\n","        return MCounter({k: other * v for k, v in self.items()})\n","\n","    def __rmul__(self, other):\n","        return self * other\n","\n","    def __add__(self, other):\n","        return MCounter(super().__add__(other))\n","\n","class TamilSimpleBPE:\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        pad_id: int = 3,\n","        unk_id: int = 0,\n","        bos_id: int = 1,\n","        eos_id: int = 2,\n","        coverage: float = 0.9999,\n","    ):\n","        self.desired_vocab_size = vocab_size\n","        self.coverage = coverage\n","\n","        self.special_tokens = {\n","            PAD: pad_id,\n","            UNK: unk_id,\n","            BOS: bos_id,\n","            EOS: eos_id\n","        }\n","\n","        # vocabulary mappings\n","        self.vocab = {}  # token_str -> token_id\n","        self.id2token = {}  # token_id -> token_str\n","        self.merges = []  # list of merge rules (left, right)\n","\n","        for token_str, token_id in self.special_tokens.items():\n","            self.vocab[token_str] = token_id\n","            self.id2token[token_id] = token_str\n","\n","        self.next_id = max(self.special_tokens.values()) + 1\n","\n","    def _preprocess_tamil_text(self, text: str) -> str:\n","\n","        text = unicodedata.normalize('NFC', text)\n","\n","        text = re.sub(r'\\s+', ' ', text)\n","\n","        # keep Tamil script, Tamil numerals, and common punctuation\n","        # Unicode range: U+0B80-U+0BFF\n","        text = re.sub(r'[^\\u0B80-\\u0BFF\\s\\w\\u0030-\\u0039\\u002E\\u002C\\u003F\\u0021\\u003A\\u003B\\u002D]', ' ', text)\n","\n","        # English mixed with Tamil\n","        text = re.sub(r'([a-zA-Z]+)', r' \\1 ', text)\n","\n","        # Tamil numerals and punctuation\n","        text = re.sub(r'([௦-௯]+)', r' \\1 ', text)\n","\n","        text = re.sub(r'\\s+', ' ', text)\n","\n","        return text.strip()\n","\n","    def _get_words(self, file: str) -> Dict[str, int]:\n","\n","        logging.info(f'Loading corpus from {file}...')\n","        start_time = time.time()\n","\n","        word_freqs = MCounter()\n","        line_count = 0\n","\n","        with open(file, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                line_count += 1\n","                if not line.strip():\n","                    continue\n","\n","                processed_line = self._preprocess_tamil_text(line)\n","                if not processed_line:\n","                    continue\n","\n","                words = processed_line.split()\n","\n","                words = [WHITESPACE + word for word in words if word]\n","                word_freqs.update(words)\n","\n","                if line_count % 50000 == 0:\n","                    logging.info(f'Processed {line_count:,} lines.')\n","\n","        num_words = len(word_freqs)\n","        logging.info(f'Loaded {num_words:,} unique words from {line_count:,} lines in {time.time() - start_time:.2f}s.')\n","\n","        return dict(word_freqs)\n","\n","    def _get_characters(self, word_freqs: Dict[str, int]) -> MCounter:\n","        \"\"\"extract character frequencies from words\"\"\"\n","        char_freqs = MCounter()\n","        for word, freq in word_freqs.items():\n","            for char in word:\n","                char_freqs[char] += freq\n","        return char_freqs\n","\n","    def _filter_characters(self, char_freqs: MCounter) -> MCounter:\n","        \"\"\"filter rare characters based on coverage threshold\"\"\"\n","        if self.coverage < 1:\n","            total_chars = sum(char_freqs.values())\n","            target_chars = round(self.coverage * total_chars)\n","\n","            # sort characters by frequency (descending)\n","            sorted_chars = char_freqs.most_common()\n","\n","            # keep characters until target coverage is reached\n","            kept_chars = MCounter()\n","            char_count = 0\n","            for char, freq in sorted_chars:\n","                kept_chars[char] = freq\n","                char_count += freq\n","                if char_count >= target_chars:\n","                    break\n","\n","            removed_count = len(char_freqs) - len(kept_chars)\n","            if removed_count > 0:\n","                logging.info(f'Filtered out {removed_count} rare characters.')\n","\n","            return kept_chars\n","        return char_freqs\n","\n","    def _initialize_vocab(self, word_freqs: Dict[str, int]) -> Dict[str, List[str]]:\n","        \"\"\"initialize vocabulary with characters and return word splits\"\"\"\n","        logging.info('Initializing vocabulary...')\n","\n","        # get character frequencies\n","        char_freqs = self._get_characters(word_freqs)\n","        filtered_chars = self._filter_characters(char_freqs)\n","\n","        # add characters to vocabulary\n","        for char in filtered_chars:\n","            if char not in self.vocab:\n","                self.vocab[char] = self.next_id\n","                self.id2token[self.next_id] = char\n","                self.next_id += 1\n","\n","        # initialize word splits\n","        word_splits = {}\n","        for word in word_freqs:\n","            splits = []\n","            for char in word:\n","                if char in self.vocab:\n","                    splits.append(char)\n","                else:\n","                    splits.append(UNK)\n","            word_splits[word] = splits\n","\n","        tamil_chars = sum(1 for char in filtered_chars if '\\u0B80' <= char <= '\\u0BFF')\n","        tamil_numerals = sum(1 for char in filtered_chars if '\\u0BE6' <= char <= '\\u0BEF')\n","\n","        logging.info(f'Initialized vocabulary with {len(filtered_chars)} characters.')\n","        logging.info(f'Found {tamil_chars} Tamil script characters.')\n","        logging.info(f'Found {tamil_numerals} Tamil numerals.')\n","\n","        return word_splits\n","\n","    def _get_pairs(self, word_splits: Dict[str, List[str]], word_freqs: Dict[str, int]) -> MCounter:\n","        \"\"\"count all adjacent pairs in the vocabulary\"\"\"\n","        pairs = MCounter()\n","\n","        for word, splits in word_splits.items():\n","            freq = word_freqs[word]\n","            for i in range(len(splits) - 1):\n","                pair = (splits[i], splits[i + 1])\n","                pairs[pair] += freq\n","\n","        return pairs\n","\n","    def _merge_pair(self, pair: tuple[str, str], word_splits: Dict[str, List[str]],\n","                    word_freqs: Dict[str, int]) -> Dict[str, List[str]]:\n","        \"\"\"merge a pair in all word splits\"\"\"\n","        left, right = pair\n","        merged = left + right\n","\n","        # add merged token to vocabulary\n","        if merged not in self.vocab:\n","            self.vocab[merged] = self.next_id\n","            self.id2token[self.next_id] = merged\n","            self.next_id += 1\n","\n","        # record merge rule\n","        self.merges.append(pair)\n","\n","        # update word splits\n","        new_word_splits = {}\n","        for word, splits in word_splits.items():\n","            new_splits = []\n","            i = 0\n","            while i < len(splits):\n","                if i < len(splits) - 1 and splits[i] == left and splits[i + 1] == right:\n","                    # Merge the pair\n","                    new_splits.append(merged)\n","                    i += 2\n","                else:\n","                    new_splits.append(splits[i])\n","                    i += 1\n","            new_word_splits[word] = new_splits\n","\n","        return new_word_splits\n","\n","    def fit(self, input_file: str, output_dir: str, logging_step: int = 200) -> None:\n","        \"\"\"train BPE tokenizer\"\"\"\n","        output_path = Path(output_dir)\n","        output_path.mkdir(parents=True, exist_ok=True)\n","\n","        word_freqs = self._get_words(input_file)\n","        word_splits = self._initialize_vocab(word_freqs)\n","\n","        current_vocab_size = len(self.vocab)\n","        logging.info(f'Starting BPE training with {current_vocab_size} initial tokens.')\n","\n","        # BPE training loop\n","        merge_times = []\n","        while current_vocab_size < self.desired_vocab_size:\n","            start_time = time.time()\n","\n","            # count all pairs\n","            pairs = self._get_pairs(word_splits, word_freqs)\n","\n","            if not pairs:\n","                logging.info('No more pairs to merge. Stopping training.')\n","                break\n","\n","            # find most frequent pair\n","            most_frequent_pair, freq = pairs.most_common(1)[0]\n","\n","            if freq <= 1:\n","                logging.info('No pairs with frequency > 1. Stopping training.')\n","                break\n","\n","            # merge pair\n","            word_splits = self._merge_pair(most_frequent_pair, word_splits, word_freqs)\n","            current_vocab_size += 1\n","\n","            merge_times.append(time.time() - start_time)\n","\n","            if current_vocab_size % logging_step == 0:\n","                left, right = most_frequent_pair\n","                avg_time = np.mean(merge_times) if merge_times else 0\n","                logging.info(\n","                    f'Vocab size: {current_vocab_size:,}/{self.desired_vocab_size:,}. '\n","                    f'Merged \"{left}\" + \"{right}\" (freq: {freq:,}). '\n","                    f'Avg merge time: {avg_time:.3f}s'\n","                )\n","                merge_times = []\n","\n","        logging.info(f'Training completed with final vocabulary size: {len(self.vocab):,}')\n","\n","        self._save_simple_bpe_model(output_path / 'simple_bpe_model.json')\n","        self._save_huggingface_files(output_path)\n","\n","        logging.info(f'Files saved to {output_path}')\n","\n","    def _save_simple_bpe_model(self, file_path: Path) -> None:\n","        logging.info(f'Saving Simple BPE model to {file_path}...')\n","\n","        model_data = {\n","            'vocab': self.vocab,\n","            'merges': [{'left': left, 'right': right} for left, right in self.merges],\n","            'vocab_size': len(self.vocab),\n","            'special_tokens': self.special_tokens,\n","            'language': 'tamil',\n","            'script': 'tamil'\n","        }\n","\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(model_data, f, indent=2, ensure_ascii=False)\n","\n","    def _save_huggingface_files(self, output_path: Path) -> None:\n","        logging.info('Saving HuggingFace compatible files...')\n","\n","        hf_merges = [f\"{left} {right}\" for left, right in self.merges]\n","\n","        # tokenizer.json\n","        tokenizer_data = {\n","            \"version\": \"1.0\",\n","            \"truncation\": None,\n","            \"padding\": None,\n","            \"added_tokens\": [],\n","            \"normalizer\": {\n","                \"type\": \"NFC\"\n","            },\n","            \"pre_tokenizer\": {\n","                \"type\": \"Sequence\",\n","                \"pretokenizers\": [\n","                    {\n","                        \"type\": \"WhitespaceSplit\"\n","                    },\n","                    {\n","                        \"type\": \"Metaspace\",\n","                        \"replacement\": WHITESPACE,\n","                        \"add_prefix_space\": True\n","                    }\n","                ]\n","            },\n","            \"post_processor\": {\n","                \"type\": \"TemplateProcessing\",\n","                \"single\": f\"{BOS}:1 $A:0 {EOS}:1\",\n","                \"pair\": f\"{BOS}:1 $A:0 {EOS}:1 $B:0 {EOS}:1\",\n","                \"special_tokens\": {\n","                    BOS: {\"id\": self.special_tokens[BOS], \"type_id\": 1},\n","                    EOS: {\"id\": self.special_tokens[EOS], \"type_id\": 1}\n","                }\n","            },\n","            \"decoder\": {\n","                \"type\": \"Metaspace\",\n","                \"replacement\": WHITESPACE,\n","                \"add_prefix_space\": True\n","            },\n","            \"model\": {\n","                \"type\": \"BPE\",\n","                \"dropout\": None,\n","                \"unk_token\": UNK,\n","                \"continuing_subword_prefix\": None,\n","                \"end_of_word_suffix\": None,\n","                \"fuse_unk\": False,\n","                \"vocab\": self.vocab,\n","                \"merges\": hf_merges\n","            }\n","        }\n","\n","        with open(output_path / 'tokenizer.json', 'w', encoding='utf-8') as f:\n","            json.dump(tokenizer_data, f, indent=2, ensure_ascii=False)\n","\n","        # tokenizer_config.json\n","        config_data = {\n","            \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","            \"auto_map\": {\n","                \"AutoTokenizer\": [\"tokenizer.json\", None]\n","            },\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD,\n","            \"model_max_length\": 2048,\n","            \"padding_side\": \"left\",\n","            \"truncation_side\": \"right\",\n","            \"chat_template\": None,\n","            \"clean_up_tokenization_spaces\": True,\n","            \"spaces_between_special_tokens\": False,\n","            \"language\": \"tamil\",\n","            \"script\": \"tamil\"\n","        }\n","\n","        with open(output_path / 'tokenizer_config.json', 'w', encoding='utf-8') as f:\n","            json.dump(config_data, f, indent=2, ensure_ascii=False)\n","\n","        # special_tokens_map.json\n","        special_tokens_data = {\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD\n","        }\n","\n","        with open(output_path / 'special_tokens_map.json', 'w', encoding='utf-8') as f:\n","            json.dump(special_tokens_data, f, indent=2, ensure_ascii=False)\n","\n","        # added_tokens.json\n","        with open(output_path / 'added_tokens.json', 'w', encoding='utf-8') as f:\n","            json.dump({}, f, indent=2, ensure_ascii=False)\n","\n","        # vocab.json\n","        with open(output_path / 'vocab.json', 'w', encoding='utf-8') as f:\n","            json.dump(self.vocab, f, indent=2, ensure_ascii=False)\n","\n","        logging.info(f'Final vocabulary size: {len(self.vocab):,}')\n","        logging.info(f'Number of merge rules: {len(self.merges):,}')\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        text = self._preprocess_tamil_text(text)\n","        words = text.split()\n","\n","        tokens = []\n","        for word in words:\n","            word_with_marker = WHITESPACE + word\n","            word_tokens = self._tokenize_word(word_with_marker)\n","            tokens.extend(word_tokens)\n","\n","        return tokens\n","\n","    def _tokenize_word(self, word: str) -> List[str]:\n","        \"\"\"Tokenize a word using BPE merges\"\"\"\n","        tokens = []\n","        for char in word:\n","            if char in self.vocab:\n","                tokens.append(char)\n","            else:\n","                tokens.append(UNK)\n","\n","        # apply merge rules\n","        for left, right in self.merges:\n","            new_tokens = []\n","            i = 0\n","            while i < len(tokens):\n","                if i < len(tokens) - 1 and tokens[i] == left and tokens[i + 1] == right:\n","                    merged = left + right\n","                    new_tokens.append(merged)\n","                    i += 2\n","                else:\n","                    new_tokens.append(tokens[i])\n","                    i += 1\n","            tokens = new_tokens\n","\n","        return tokens\n","\n","    def encode(self, text: str) -> List[int]:\n","        \"\"\"encode text to token IDs\"\"\"\n","        tokens = self.tokenize(text)\n","        return [self.vocab.get(token, self.special_tokens[UNK]) for token in tokens]\n","\n","    def decode(self, token_ids: List[int]) -> str:\n","        \"\"\"decode token IDs back to text\"\"\"\n","        tokens = [self.id2token.get(token_id, UNK) for token_id in token_ids]\n","        text = ''.join(tokens)\n","        text = text.replace(WHITESPACE, ' ')\n","        return text.strip()\n","\n","def train_tamil_bpe(\n","    input_file: str,\n","    output_dir: str = \"./tamil_tokenizer\",\n","    vocab_size: int = 10000,\n","    coverage: float = 0.9999,\n","    logging_step: int = 200\n","):\n","\n","    tokenizer = TamilSimpleBPE(\n","        vocab_size=vocab_size,\n","        coverage=coverage\n","    )\n","\n","    start_time = time.time()\n","    tokenizer.fit(input_file, output_dir, logging_step)\n","    training_time = time.time() - start_time\n","\n","    return tokenizer\n","\n","def test_tamil_tokenizer(tokenizer, test_sentences=None):\n","    if test_sentences is None:\n","        test_sentences = [\n","            \"வணக்கம், நீங்கள் எப்படி இருக்கிறீர்கள்?\",\n","            \"நான் தமிழ் மொழி கற்றுக்கொண்டிருக்கிறேன்.\",\n","            \"சென்னை தமிழ்நாட்டின் தலைநகரம்.\",\n","            \"புத்தகம் மேசையில் இருக்கிறது.\",\n","            \"இன்று வானிலை மிகவும் நல்லது.\",\n","        ]\n","\n","    print(\"\\nTesting tokenizer with sample Tamil sentences:\")\n","    print(\"=\" * 70)\n","\n","    for i, sentence in enumerate(test_sentences, 1):\n","        print(f\"\\n{i}. Original: {sentence}\")\n","\n","        tokens = tokenizer.tokenize(sentence)\n","        print(f\"   Tokens: {tokens}\")\n","        print(f\"   Count: {len(tokens)} tokens\")\n","\n","        token_ids = tokenizer.encode(sentence)\n","        print(f\"   IDs: {token_ids}\")\n","\n","        decoded = tokenizer.decode(token_ids)\n","        print(f\"   Decoded: {decoded}\")\n","\n","        if decoded.strip() == sentence.strip():\n","            print(\"   Perfect reconstruction\")\n","        else:\n","            print(\"   Reconstruction differs\")\n","\n","def main():\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/dataset/ta_reduced_train.txt\"\n","    output_dir = \"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/tokenizers/standard_bpe\"\n","\n","    vocab_size = 10000\n","    coverage = 0.9999\n","    logging_step = 200\n","\n","    tokenizer = train_tamil_bpe(\n","        input_file=input_file,\n","        output_dir=output_dir,\n","        vocab_size=vocab_size,\n","        coverage=coverage,\n","        logging_step=logging_step\n","    )\n","\n","    test_tamil_tokenizer(tokenizer)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"Pe-WlWIAO3nn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Picky BPE"],"metadata":{"id":"feJymxZZXHg5"}},{"cell_type":"code","source":["from __future__ import annotations\n","import json\n","import logging\n","import time\n","import argparse\n","import re\n","from transformers import AutoTokenizer\n","from pathlib import Path\n","from collections import defaultdict, Counter\n","from typing import Union, Optional, Dict, List\n","import numpy as np\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    datefmt='%Y-%m-%d %H:%M:%S',\n","    handlers=[\n","        logging.StreamHandler(),\n","    ],\n","    force=True\n",")\n","\n","WHITESPACE = '▁'\n","PAD = '<pad>'\n","UNK = '<unk>'\n","BOS = '<s>'\n","EOS = '</s>'\n","\n","class MCounter(Counter):\n","    \"\"\"extended Counter class with multiplication support\"\"\"\n","    def __mul__(self, other):\n","        if not isinstance(other, int):\n","            raise TypeError(\"Non-int factor\")\n","        return MCounter({k: other * v for k, v in self.items()})\n","\n","    def __rmul__(self, other):\n","        return self * other\n","\n","    def __add__(self, other):\n","        return MCounter(super().__add__(other))\n","\n","class Token:\n","    def __init__(\n","        self,\n","        id: int,\n","        str: str,\n","        freq: int = 0,\n","        special: bool = False,\n","        present: bool = True,\n","        left: Optional[Token] = None,\n","        right: Optional[Token] = None,\n","        split: Optional[list[Token]] = None\n","    ):\n","        self.id = id\n","        self.str = str\n","        self.freq = freq\n","        self.special = special\n","        self.present = present\n","        self.atomic = len(str) == 1 or special\n","        self.words = set()\n","        self.left = left\n","        self.right = right\n","        self.split = split\n","\n","    def __repr__(self):\n","        return f'{self.str} ({self.freq})'\n","\n","    def walk(self) -> list[Token]:\n","        if self.atomic or self.present:\n","            return [self]\n","        return self.left.walk() + self.right.walk()\n","\n","    def remove(self) -> None:\n","        if self.atomic:\n","            raise ValueError(f'Cannot remove an atomic token {self.str}.')\n","        self.present = False\n","        self.freq = 0\n","        self.words = set()\n","\n","    def restore(self) -> None:\n","        if self.present:\n","            raise ValueError(f'Cannot revoke already present token {self.str}.')\n","        self.present = True\n","\n","    def split_if_possible(self) -> Optional[list[Token]]:\n","        if self.atomic:\n","            return None\n","        self.present = False\n","        return self.walk()\n","\n","    def to_dict(self) -> dict:\n","        return {\n","            'id': self.id,\n","            'str': self.str,\n","            'freq': self.freq,\n","            'special': self.special,\n","            'present': self.present,\n","            'left': self.left.id if self.left is not None else None,\n","            'right': self.right.id if self.right is not None else None,\n","            'split': [t.id for t in self.walk()]\n","        }\n","\n","class Word:\n","    def __init__(self, id: int, word: str, freq: int = 0):\n","        self.id = id\n","        self.str = word\n","        self.freq = freq\n","        self.tokens = None\n","        self.pairs = None\n","\n","    def __repr__(self) -> str:\n","        return f'{self.str} ({self.freq})'\n","\n","    def encode(self, str2token: dict[str, Token]) -> None:\n","        self.tokens = [str2token[c] for c in self.str]\n","        self._recalculate()\n","\n","    def _recalculate(self, update_tokens: bool = True) -> None:\n","        self.pairs = MCounter(zip(self.tokens[:-1], self.tokens[1:])) * self.freq\n","        if update_tokens:\n","            for token in self.tokens:\n","                token.words.add(self)\n","\n","    def merge_pair(self, pair: tuple[Token, Token], new_token: Token, update_tokens: bool = True) -> int:\n","        new_tokens = []\n","        i = 0\n","        while i < len(self.tokens):\n","            if i < len(self.tokens) - 1 and (self.tokens[i], self.tokens[i+1]) == pair:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(self.tokens[i])\n","                i += 1\n","        new_token_frequency = len(self.tokens) - len(new_tokens)\n","        if update_tokens:\n","            pair[0].words.discard(self)\n","            pair[1].words.discard(self)\n","        self.tokens = new_tokens\n","        self._recalculate(update_tokens=update_tokens)\n","        return new_token_frequency * self.freq\n","\n","    def split_token(self, token: Token, split: list[Token], update_tokens: bool = True):\n","        new_tokens = []\n","        for t in self.tokens:\n","            if t == token:\n","                new_tokens.extend(split)\n","            else:\n","                new_tokens.append(t)\n","        self.tokens = new_tokens\n","        self._recalculate(update_tokens=update_tokens)\n","\n","class TamilPickyBPE:\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        pad_id: int = 3,\n","        unk_id: int = 0,\n","        bos_id: int = 1,\n","        eos_id: int = 2,\n","        coverage: float = 0.9999,\n","        threshold: float = 0.9999,\n","    ):\n","        self.desired_vocab_size = vocab_size\n","        self.pad_token = Token(pad_id, PAD, 0, special=True)\n","        self.unk_token = Token(unk_id, UNK, 0, special=True)\n","        self.bos_token = Token(bos_id, BOS, 0, special=True)\n","        self.eos_token = Token(eos_id, EOS, 0, special=True)\n","\n","        self.id2token = {\n","            token.id: token for token in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n","        }\n","        self.str2token = {\n","            token.str: token for token in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n","        }\n","        self.str2token = defaultdict(lambda: self.unk_token, self.str2token)\n","        self.max_special_token_id = max(self.id2token.keys())\n","        self.actual_vocab_size = len(self.id2token)\n","        self.new_id = self.max_special_token_id + 1\n","        self.coverage = coverage\n","        self.threshold = threshold\n","        self.events = list()\n","\n","    def _preprocess_tamil_text(self, text: str) -> str:\n","        \"\"\"preprocess Tamil text preserving Tamil script\"\"\"\n","        import unicodedata\n","\n","        text = unicodedata.normalize('NFC', text)\n","        text = text.replace(' ', f' {WHITESPACE}')\n","\n","        # Tamil Unicode range: U+0B80-U+0BFF\n","        text = re.sub(r'[^\\u0B80-\\u0BFF\\s\\w\\u0030-\\u0039\\u002E\\u002C\\u003F\\u0021\\u003A\\u003B\\u002D]', ' ', text)\n","        text = re.sub(r'([a-zA-Z]+)', r' \\1 ', text)\n","        text = re.sub(r'([௦-௯]+)', r' \\1 ', text)\n","        text = re.sub(r'\\s+', ' ', text)\n","\n","        return text.strip()\n","\n","    def _get_words(self, file: str) -> list[Word]:\n","        \"\"\"load and preprocess Tamil corpus from file\"\"\"\n","        logging.info(f'Loading Tamil corpus from {file}...')\n","        start_time = time.time()\n","\n","        counter = MCounter()\n","        with open(file, 'r', encoding='utf-8') as f:\n","            for i, line in enumerate(f):\n","                if not line.strip():\n","                    continue\n","\n","                processed_line = self._preprocess_tamil_text(line.strip())\n","                if not processed_line:\n","                    continue\n","\n","                words = processed_line.split()\n","                words = [WHITESPACE + word if not word.startswith(WHITESPACE) else word for word in words]\n","                counter.update(words)\n","\n","                if i > 0 and i % 50000 == 0:\n","                    logging.info(f'Processed {i} lines.')\n","\n","        num_words = len(counter)\n","        logging.info(f'Loaded {num_words} unique words in {time.time() - start_time:.2f}s.')\n","\n","        return [Word(i, word, freq) for i, (word, freq) in enumerate(counter.items())]\n","\n","    def _get_characters(self, words: list[Word]) -> MCounter:\n","        \"\"\"extract character frequencies from words\"\"\"\n","        counter = MCounter()\n","        for i, word in enumerate(words):\n","            counter.update(MCounter(word.str) * word.freq)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for character extraction.')\n","        return counter\n","\n","    def _filter_characters(self, characters: MCounter) -> MCounter:\n","        \"\"\"filter rare characters based on coverage threshold\"\"\"\n","        if self.coverage < 1:\n","            corpus_size = sum(characters.values())\n","            freq_to_remove = corpus_size - round(self.coverage * corpus_size)\n","            if freq_to_remove > 0:\n","                cum_sum = np.cumsum([freq for _, freq in reversed(characters.most_common())])\n","                num_to_remove = np.searchsorted(cum_sum, freq_to_remove)\n","                characters_to_remove = [c for c, _ in characters.most_common()[-num_to_remove:]]\n","                for c in characters_to_remove:\n","                    characters.pop(c)\n","                logging.info(f'Replaced {num_to_remove} rare characters with UNK.')\n","        return characters\n","\n","    def _initialize_vocab(self, words: list[Word]) -> None:\n","        \"\"\"initialize vocabulary with Tamil characters\"\"\"\n","        logging.info('Initializing vocabulary with Tamil characters...')\n","        characters = self._get_characters(words)\n","        filtered_characters = self._filter_characters(characters)\n","\n","        for i, character in enumerate(filtered_characters):\n","            token = Token(self.new_id + i, character, filtered_characters[character])\n","            self.id2token[token.id] = token\n","            self.str2token[token.str] = token\n","\n","        self.new_id += len(filtered_characters)\n","        self.actual_vocab_size += len(filtered_characters)\n","\n","        tamil_chars = sum(1 for char in filtered_characters if '\\u0B80' <= char <= '\\u0BFF')\n","        tamil_numerals = sum(1 for char in filtered_characters if '\\u0BE6' <= char <= '\\u0BEF')\n","\n","        logging.info(f'Initialized vocabulary with {len(filtered_characters)} unique characters.')\n","        logging.info(f'Found {tamil_chars} Tamil script characters.')\n","        logging.info(f'Found {tamil_numerals} Tamil numerals.')\n","\n","    @staticmethod\n","    def _validate_pair(pair) -> bool:\n","        \"\"\"check if pair contains only non-special tokens\"\"\"\n","        return not any(token.special for token in pair)\n","\n","    def _encode_words(self, words: list[Word]) -> None:\n","        \"\"\"encode words using current vocabulary\"\"\"\n","        logging.info('Encoding words with Tamil characters...')\n","        for i, word in enumerate(words):\n","            word.encode(self.str2token)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for encoding.')\n","\n","    def _initialize_pairs(self, words: list[Word]) -> MCounter:\n","        \"\"\"initialize pair frequencies\"\"\"\n","        pairs = MCounter()\n","        logging.info('Counting Tamil character pairs...')\n","        for i, word in enumerate(words):\n","            pairs.update(word.pairs)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for pair counting.')\n","\n","        # remove pairs containing special tokens\n","        to_remove = set()\n","        for pair in pairs:\n","            if not self._validate_pair(pair):\n","                to_remove.add(pair)\n","        for pair in to_remove:\n","            pairs.pop(pair)\n","\n","        return pairs\n","\n","    def _remove_if_possible(self, token: Token, merged_freq: int, pairs: MCounter) -> bool:\n","        \"\"\"remove token if it meets the threshold criteria\"\"\"\n","        if merged_freq / (token.freq + merged_freq) > self.threshold:\n","            split = token.split_if_possible()\n","            if split is not None:\n","                self.actual_vocab_size -= 1\n","                for t in split:\n","                    t.freq += token.freq\n","                for pair in zip(split[:-1], split[1:]):\n","                    pairs[pair] += token.freq\n","\n","                pairs_for_update = MCounter()\n","                for word in token.words:\n","                    if token not in word.tokens:\n","                        raise ValueError(f'Token {token} not found in word {word}.')\n","                    pairs_for_update.update({\n","                        pair: freq for pair, freq in word.pairs.items()\n","                        if self._validate_pair(pair) and token in pair\n","                    })\n","                    word.split_token(token, split)\n","\n","                self._update_pairs_on_remove(token, split, pairs_for_update, pairs)\n","                token.remove()\n","                return True\n","        return False\n","\n","    @staticmethod\n","    def _update_pairs_on_merge(new_token: Token, pair: tuple[Token, Token],\n","                              pairs_for_update: MCounter, pairs: MCounter):\n","        \"\"\"update pair frequencies after merge operation\"\"\"\n","        pairs.update(pairs_for_update)\n","        for p, freq in pairs_for_update.items():\n","            if new_token not in p:\n","                raise ValueError(f'Pair {p} does not contain the new token {new_token}.')\n","            if new_token is p[0]:\n","                if new_token is p[1]:\n","                    to_update = (pair[1], pair[0])\n","                else:\n","                    to_update = (pair[1], p[1])\n","            else:\n","                to_update = (p[0], pair[0])\n","            if to_update in pairs:\n","                pairs[to_update] -= freq\n","                if pairs[to_update] <= 0:\n","                    pairs.pop(to_update)\n","\n","    @staticmethod\n","    def _update_pairs_on_remove(token: Token, split: list[Token],\n","                               pairs_for_update: MCounter, pairs: MCounter):\n","        \"\"\"update pair frequencies after split operation\"\"\"\n","        for pair, freq in pairs_for_update.items():\n","            if token is pair[0]:\n","                if token is pair[1]:\n","                    to_update = (split[-1], split[0])\n","                else:\n","                    to_update = (split[-1], pair[1])\n","            else:\n","                to_update = (pair[0], split[0])\n","            pairs[to_update] += freq\n","            pairs.pop(pair)\n","\n","    def _merge_token_in_words(self, token_to_merge: Token, pair_to_merge: tuple[Token, Token],\n","                             pairs: MCounter) -> int:\n","        \"\"\"merge token in all relevant words\"\"\"\n","        actual_freq = 0\n","        pairs_for_update = MCounter()\n","\n","        for word in pair_to_merge[0].words & pair_to_merge[1].words:\n","            if pair_to_merge in word.pairs:\n","                word.pairs.pop(pair_to_merge)\n","                actual_freq += word.merge_pair(pair_to_merge, token_to_merge)\n","                pairs_for_update.update({\n","                    p: f for p, f in word.pairs.items()\n","                    if self._validate_pair(p) and token_to_merge in p\n","                })\n","\n","        self._update_pairs_on_merge(token_to_merge, pair_to_merge, pairs_for_update, pairs)\n","        token_to_merge.freq += actual_freq\n","\n","        if pair_to_merge[0] is pair_to_merge[1]:\n","            pair_to_merge[0].freq -= 2 * actual_freq\n","            removed = self._remove_if_possible(pair_to_merge[0], actual_freq, pairs)\n","            if removed:\n","                logging.info(f'Removed token {pair_to_merge[0].str} after merging into {token_to_merge.str}.')\n","                self.events.append(('SPLIT', pair_to_merge[0], pair_to_merge[0].walk()))\n","        else:\n","            for token in pair_to_merge:\n","                if not token.present:\n","                    raise ValueError(f'Token {token} is not present in vocabulary.')\n","                token.freq -= actual_freq\n","                token_freq = token.freq\n","                removed = self._remove_if_possible(token, actual_freq, pairs)\n","                if removed:\n","                    logging.info(f'Removed token {token.str} after merging into {token_to_merge.str}.')\n","                    self.events.append(('SPLIT', token, token.walk()))\n","\n","        return actual_freq\n","\n","    def _merge_pair(self, pair: tuple[Token, Token], pairs: MCounter) -> int:\n","        \"\"\"merge a token pair\"\"\"\n","        pairs.pop(pair)\n","        merged_str = pair[0].str + pair[1].str\n","\n","        if merged_str in self.str2token:\n","            new_token = self.str2token[merged_str]\n","            if not new_token.present:\n","                new_token.restore()\n","                logging.info(f'Restored previously removed token {new_token.str}.')\n","            else:\n","                logging.info(f'Additional merges for {new_token.str}.')\n","        else:\n","            new_token = Token(self.new_id, merged_str, 0, left=pair[0], right=pair[1])\n","            self.id2token[new_token.id] = new_token\n","            self.str2token[new_token.str] = new_token\n","            self.new_id += 1\n","\n","        self.events.append(('MERGE', pair, new_token))\n","        actual_freq = self._merge_token_in_words(new_token, pair, pairs)\n","        return actual_freq\n","\n","    def fit(self, input_file: str, output_dir: str, logging_step: int = 200) -> None:\n","        \"\"\"train Tamil PickyBPE tokenizer\"\"\"\n","        output_path = Path(output_dir)\n","        output_path.mkdir(parents=True, exist_ok=True)\n","\n","        logging.info(\"Starting Tamil PickyBPE training...\")\n","        logging.info(f\"Input: {input_file}\")\n","        logging.info(f\"Output: {output_dir}\")\n","        logging.info(f\"Target vocab size: {self.desired_vocab_size:,}\")\n","        logging.info(f\"Coverage: {self.coverage*100:.2f}%\")\n","        logging.info(f\"Threshold: {self.threshold*100:.2f}%\")\n","\n","        words = self._get_words(input_file)\n","        self._initialize_vocab(words)\n","        self._encode_words(words)\n","        pairs = self._initialize_pairs(words)\n","\n","        merge_time = []\n","        logging.info(f'Starting PickyBPE training for Tamil with {self.actual_vocab_size} initial tokens.')\n","\n","        while self.actual_vocab_size < self.desired_vocab_size:\n","            start_time = time.time()\n","            if not pairs:\n","                logging.info(f'No more pairs to merge. Stopping with vocab size of {self.actual_vocab_size}.')\n","                break\n","\n","            pair, count = pairs.most_common(1)[0]\n","            if count <= 0:\n","                logging.info(f'No more pairs to merge. Stopping with vocab size of {self.actual_vocab_size}.')\n","                break\n","\n","            freq = self._merge_pair(pair, pairs)\n","            self.actual_vocab_size += 1\n","            merge_time.append(time.time() - start_time)\n","\n","            if self.actual_vocab_size % logging_step == 0:\n","                avg_time = np.mean(merge_time) if merge_time else 0\n","                current_speed = 1.0 / avg_time if avg_time > 0 else 0\n","                logging.info(\n","                    f'VOCABULARY SIZE: {self.actual_vocab_size:,}/{self.desired_vocab_size:,}. '\n","                    f'Merged \"{pair[0].str}\" + \"{pair[1].str}\" with frequency {freq:,}. '\n","                    f'Speed: {current_speed:.1f} merges/sec'\n","                )\n","                merge_time = []\n","\n","        self._save_picky_model(output_path / 'picky_bpe_model.json')\n","        self._save_huggingface_files(output_path)\n","\n","        logging.info(f'Training completed. Files saved to {output_path}')\n","\n","    def _save_picky_model(self, file_path: Path) -> None:\n","        \"\"\"save PickyBPE model in original format\"\"\"\n","        logging.info(f'Saving Tamil PickyBPE model to {file_path}...')\n","\n","        assigned_ids = sorted(self.id2token.keys())\n","        id_mapping = {}\n","        id_counter = 0\n","\n","        for i in assigned_ids:\n","            if self.id2token[i].present:\n","                id_mapping[i] = id_counter\n","                id_counter += 1\n","\n","        model_data = {\n","            'language': 'tamil',\n","            'script': 'tamil',\n","            'tokens': [token.to_dict() for token in self.id2token.values()],\n","            'id2int': {str(k): v for k, v in id_mapping.items()},\n","            'int2id': {str(v): k for k, v in id_mapping.items()},\n","            'merges': [\n","                {'id': i, 'pair': [token.to_dict() for token in merge[1]], 'new_token': merge[2].to_dict()}\n","                for i, merge in enumerate(self.events) if merge[0] == 'MERGE'\n","            ],\n","            'splits': [\n","                {'id': i, 'token': merge[1].to_dict(), 'split': [token.to_dict() for token in merge[2]]}\n","                for i, merge in enumerate(self.events) if merge[0] == 'SPLIT'\n","            ],\n","            'training_config': {\n","                'coverage': self.coverage,\n","                'threshold': self.threshold,\n","                'vocab_size': self.desired_vocab_size\n","            }\n","        }\n","\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(model_data, f, indent=2, ensure_ascii=False)\n","\n","    def _save_huggingface_files(self, output_path: Path) -> None:\n","        \"\"\"convert and save HuggingFace compatible tokenizer files\"\"\"\n","\n","        # extract present tokens and create vocabulary\n","        vocab = {}\n","        present_tokens = []\n","        for token_id in sorted(self.id2token.keys()):\n","            token = self.id2token[token_id]\n","            if token.present:\n","                vocab[token.str] = len(present_tokens)\n","                present_tokens.append(token)\n","\n","        # create merge rules from events\n","        merges = []\n","        for event in self.events:\n","            if event[0] == 'MERGE':\n","                pair = event[1]\n","                left_str = pair[0].str\n","                right_str = pair[1].str\n","                if left_str in vocab and right_str in vocab:\n","                    merges.append(f\"{left_str} {right_str}\")\n","\n","        # tokenizer.json\n","        tokenizer_data = {\n","            \"version\": \"1.0\",\n","            \"truncation\": None,\n","            \"padding\": None,\n","            \"added_tokens\": [],\n","            \"normalizer\": {\n","                \"type\": \"NFC\"\n","            },\n","            \"pre_tokenizer\": {\n","                \"type\": \"Sequence\",\n","                \"pretokenizers\": [\n","                    {\n","                        \"type\": \"WhitespaceSplit\"\n","                    },\n","                    {\n","                        \"type\": \"Metaspace\",\n","                        \"replacement\": WHITESPACE,\n","                        \"add_prefix_space\": True\n","                    }\n","                ]\n","            },\n","            \"post_processor\": {\n","                \"type\": \"TemplateProcessing\",\n","                \"single\": f\"{BOS}:1 $A:0 {EOS}:1\",\n","                \"pair\": f\"{BOS}:1 $A:0 {EOS}:1 $B:0 {EOS}:1\",\n","                \"special_tokens\": {\n","                    BOS: {\"id\": 1, \"type_id\": 1},\n","                    EOS: {\"id\": 2, \"type_id\": 1}\n","                }\n","            },\n","            \"decoder\": {\n","                \"type\": \"Metaspace\",\n","                \"replacement\": WHITESPACE,\n","                \"add_prefix_space\": True\n","            },\n","            \"model\": {\n","                \"type\": \"BPE\",\n","                \"dropout\": None,\n","                \"unk_token\": UNK,\n","                \"continuing_subword_prefix\": None,\n","                \"end_of_word_suffix\": None,\n","                \"fuse_unk\": False,\n","                \"vocab\": vocab,\n","                \"merges\": merges\n","            }\n","        }\n","\n","        with open(output_path / 'tokenizer.json', 'w', encoding='utf-8') as f:\n","            json.dump(tokenizer_data, f, indent=2, ensure_ascii=False)\n","\n","        # tokenizer_config.json\n","        config_data = {\n","            \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","            \"auto_map\": {\n","                \"AutoTokenizer\": [\"tokenizer.json\", None]\n","            },\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD,\n","            \"model_max_length\": 2048,\n","            \"padding_side\": \"left\",\n","            \"truncation_side\": \"right\",\n","            \"chat_template\": None,\n","            \"clean_up_tokenization_spaces\": True,\n","            \"spaces_between_special_tokens\": False,\n","            \"language\": \"tamil\",\n","            \"script\": \"tamil\"\n","        }\n","\n","        with open(output_path / 'tokenizer_config.json', 'w', encoding='utf-8') as f:\n","            json.dump(config_data, f, indent=2, ensure_ascii=False)\n","\n","        # special_tokens_map.json\n","        special_tokens_data = {\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD\n","        }\n","\n","        with open(output_path / 'special_tokens_map.json', 'w', encoding='utf-8') as f:\n","            json.dump(special_tokens_data, f, indent=2, ensure_ascii=False)\n","\n","        # added_tokens.json\n","        with open(output_path / 'added_tokens.json', 'w', encoding='utf-8') as f:\n","            json.dump([], f, indent=2, ensure_ascii=False)\n","\n","        # vocab.json\n","        with open(output_path / 'vocab.json', 'w', encoding='utf-8') as f:\n","            json.dump(vocab, f, indent=2, ensure_ascii=False)\n","\n","        logging.info(f'Final vocabulary size: {len(vocab):,}')\n","        logging.info(f'Number of merge rules: {len(merges):,}')\n","        logging.info(f'Number of split events: {sum(1 for event in self.events if event[0] == \"SPLIT\")}')\n","\n","def train_tamil_picky_bpe(\n","    input_file: str,\n","    output_dir: str = \"./tamil_picky_bpe\",\n","    vocab_size: int = 10000,\n","    coverage: float = 0.9999,\n","    threshold: float = 0.9999,\n","    logging_step: int = 200\n","):\n","    \"\"\"train a PickyBPE tokenizer for Tamil language\"\"\"\n","    tokenizer = TamilPickyBPE(\n","        vocab_size=vocab_size,\n","        coverage=coverage,\n","        threshold=threshold\n","    )\n","\n","    print(\"Training PickyBPE tokenizer for Tamil...\")\n","    print(f\"Input file: {input_file}\")\n","    print(f\"Output directory: {output_dir}\")\n","    print(f\"Target vocabulary size: {vocab_size:,}\")\n","    print(f\"Special tokens: {UNK} (0), {BOS} (1), {EOS} (2), {PAD} (3)\")\n","    print(f\"Character coverage: {coverage*100:.2f}%\")\n","    print(f\"Removal threshold: {threshold*100:.2f}%\")\n","\n","    start_time = time.time()\n","    tokenizer.fit(input_file, output_dir, logging_step)\n","    training_time = time.time() - start_time\n","\n","    return tokenizer\n","\n","def test_tamil_picky_bpe(tokenizer_path: str):\n","    \"\"\"test the trained tokenizer\"\"\"\n","    from transformers import AutoTokenizer\n","\n","    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n","\n","    test_sentences = [\n","        \"வணக்கம், நீங்கள் எப்படி இருக்கிறீர்கள்?\",\n","        \"நான் தமிழ் மொழி கற்றுக்கொண்டிருக்கிறேன்.\",\n","        \"சென்னை தமிழ்நாட்டின் தலைநகரம்.\",\n","        \"புத்தகம் மேசையில் இருக்கிறது.\",\n","        \"இன்று வானிலை மிகவும் நல்லது.\",\n","    ]\n","\n","    print(\"\\nTesting Tamil PickyBPE tokenizer:\")\n","    print(\"=\" * 70)\n","\n","    for i, sentence in enumerate(test_sentences, 1):\n","        print(f\"\\n{i}. Original: {sentence}\")\n","\n","        tokens = tokenizer.tokenize(sentence)\n","        print(f\"   Tokens: {tokens}\")\n","        print(f\"   Count: {len(tokens)} tokens\")\n","\n","        token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n","        print(f\"   IDs: {token_ids}\")\n","\n","        decoded = tokenizer.decode(token_ids)\n","        print(f\"   Decoded: {decoded}\")\n","\n","        if decoded.strip() == sentence.strip():\n","            print(\"   Perfect reconstruction\")\n","        else:\n","            print(\"   Reconstruction differs\")\n","\n","def main():\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/dataset/ta_reduced_train.txt\"\n","    output_dir = \"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/tokenizers/picky_bpe\"\n","\n","    vocab_size = 10000\n","    coverage = 0.9999\n","    threshold = 0.9999\n","    logging_step = 500\n","\n","    tokenizer = train_tamil_picky_bpe(\n","        input_file=input_file,\n","        output_dir=output_dir,\n","        vocab_size=vocab_size,\n","        coverage=coverage,\n","        threshold=threshold,\n","        logging_step=logging_step\n","    )\n","\n","    test_tamil_picky_bpe(output_dir)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"6s3A-_pQXG3Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Grapheme Picky BPE"],"metadata":{"id":"OPB94SgwXINW"}},{"cell_type":"code","source":["from __future__ import annotations\n","import json\n","import logging\n","import time\n","import argparse\n","import re\n","import sys\n","import os\n","import unicodedata\n","from transformers import AutoTokenizer\n","from pathlib import Path\n","from collections import defaultdict, Counter\n","from typing import Union, Optional, Dict, List\n","import numpy as np\n","import grapheme\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    datefmt='%Y-%m-%d %H:%M:%S',\n","    handlers=[\n","        logging.StreamHandler(),\n","    ],\n","    force=True\n",")\n","logger = logging.getLogger(__name__)\n","\n","WHITESPACE = '▁'\n","PAD = '<pad>'\n","UNK = '<unk>'\n","BOS = '<s>'\n","EOS = '</s>'\n","\n","TAMIL_CONSONANTS = set('\\u0B95\\u0B99\\u0B9A\\u0B9E\\u0B9F\\u0BA3\\u0BA4\\u0BA8\\u0BAA\\u0BAE\\u0BAF\\u0BB0\\u0BB2\\u0BB5\\u0BB6\\u0BB7\\u0BB8\\u0BB9')\n","TAMIL_VOWELS = set('\\u0B85\\u0B86\\u0B87\\u0B88\\u0B89\\u0B8A\\u0B8E\\u0B8F\\u0B90\\u0B92\\u0B93\\u0B94')\n","TAMIL_VOWEL_SIGNS = set('\\u0BBE\\u0BBF\\u0BC0\\u0BC1\\u0BC2\\u0BC6\\u0BC7\\u0BC8\\u0BCA\\u0BCB\\u0BCC')\n","TAMIL_VIRAMA = '\\u0BCD'\n","TAMIL_ANUSVARA = '\\u0B82'\n","TAMIL_VISARGA = '\\u0B83'\n","\n","class MCounter(Counter):\n","    \"\"\"extended Counter class with multiplication support\"\"\"\n","    def __mul__(self, other):\n","        if not isinstance(other, int):\n","            raise TypeError(\"Non-int factor\")\n","        return MCounter({k: other * v for k, v in self.items()})\n","\n","    def __rmul__(self, other):\n","        return self * other\n","\n","    def __add__(self, other):\n","        return MCounter(super().__add__(other))\n","\n","class Token:\n","    def __init__(\n","        self,\n","        id: int,\n","        str: str,\n","        freq: int = 0,\n","        special: bool = False,\n","        present: bool = True,\n","        left: Optional[Token] = None,\n","        right: Optional[Token] = None,\n","        split: Optional[list[Token]] = None\n","    ):\n","        self.id = id\n","        self.str = str\n","        self.freq = freq\n","        self.special = special\n","        self.present = present\n","        self.atomic = len(str) == 1 or special\n","        self.words = set()\n","        self.left = left\n","        self.right = right\n","        self.split = split\n","\n","    def __repr__(self):\n","        return f'{self.str} ({self.freq})'\n","\n","    def walk(self) -> list[Token]:\n","        if self.atomic or self.present:\n","            return [self]\n","        result = []\n","        if self.left is not None:\n","            result.extend(self.left.walk())\n","        if self.right is not None:\n","            result.extend(self.right.walk())\n","        if not result:\n","            return [self]\n","        return result\n","\n","    def remove(self) -> None:\n","        if self.atomic:\n","            raise ValueError(f'Cannot remove an atomic token {self.str}.')\n","        self.present = False\n","        self.freq = 0\n","        self.words = set()\n","\n","    def restore(self) -> None:\n","        if self.present:\n","            raise ValueError(f'Cannot revoke already present token {self.str}.')\n","        self.present = True\n","\n","    def split_if_possible(self) -> Optional[list[Token]]:\n","        if self.atomic:\n","            return None\n","        self.present = False\n","        return self.walk()\n","\n","    def to_dict(self) -> dict:\n","        return {\n","            'id': self.id,\n","            'str': self.str,\n","            'freq': self.freq,\n","            'special': self.special,\n","            'present': self.present,\n","            'left': self.left.id if self.left is not None else None,\n","            'right': self.right.id if self.right is not None else None,\n","            'split': [t.id for t in self.walk()]\n","        }\n","\n","class Word:\n","    def __init__(self, id: int, word: str, freq: int = 0):\n","        self.id = id\n","        self.str = word\n","        self.freq = freq\n","        self.tokens = None\n","        self.pairs = None\n","\n","    def __repr__(self) -> str:\n","        return f'{self.str} ({self.freq})'\n","\n","    def encode(self, str2token: dict[str, Token]) -> None:\n","        graphemes_list = list(grapheme.graphemes(self.str))\n","        self.tokens = [str2token[g] for g in graphemes_list]\n","        self._recalculate()\n","\n","    def _recalculate(self, update_tokens: bool = True) -> None:\n","        self.pairs = MCounter(zip(self.tokens[:-1], self.tokens[1:])) * self.freq\n","        if update_tokens:\n","            for token in self.tokens:\n","                token.words.add(self)\n","\n","    def merge_pair(self, pair: tuple[Token, Token], new_token: Token, update_tokens: bool = True) -> int:\n","        new_tokens = []\n","        i = 0\n","        while i < len(self.tokens):\n","            if i < len(self.tokens) - 1 and (self.tokens[i], self.tokens[i+1]) == pair:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(self.tokens[i])\n","                i += 1\n","        new_token_frequency = len(self.tokens) - len(new_tokens)\n","        if update_tokens:\n","            pair[0].words.discard(self)\n","            pair[1].words.discard(self)\n","        self.tokens = new_tokens\n","        self._recalculate(update_tokens=update_tokens)\n","        return new_token_frequency * self.freq\n","\n","    def split_token(self, token: Token, split: list[Token], update_tokens: bool = True):\n","        new_tokens = []\n","        for t in self.tokens:\n","            if t == token:\n","                new_tokens.extend(split)\n","            else:\n","                new_tokens.append(t)\n","        self.tokens = new_tokens\n","        self._recalculate(update_tokens=update_tokens)\n","\n","class TamilGraphemePickyBPE:\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        pad_id: int = 3,\n","        unk_id: int = 0,\n","        bos_id: int = 1,\n","        eos_id: int = 2,\n","        coverage: float = 0.9999,\n","        threshold: float = 0.9999,\n","    ):\n","        self.desired_vocab_size = vocab_size\n","        self.pad_token = Token(pad_id, PAD, 0, special=True)\n","        self.unk_token = Token(unk_id, UNK, 0, special=True)\n","        self.bos_token = Token(bos_id, BOS, 0, special=True)\n","        self.eos_token = Token(eos_id, EOS, 0, special=True)\n","\n","        self.id2token = {\n","            token.id: token for token in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n","        }\n","        self.str2token = {\n","            token.str: token for token in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n","        }\n","        self.str2token = defaultdict(lambda: self.unk_token, self.str2token)\n","        self.max_special_token_id = max(self.id2token.keys())\n","        self.actual_vocab_size = len(self.id2token)\n","        self.new_id = self.max_special_token_id + 1\n","        self.coverage = coverage\n","        self.threshold = threshold\n","        self.events = list()\n","        self.grapheme_vocab = set()\n","\n","    @staticmethod\n","    def _validate_pair(pair) -> bool:\n","        \"\"\"check if pair contains only non-special tokens\"\"\"\n","        return not any(token.special for token in pair)\n","\n","    def _preprocess_tamil_text(self, text: str) -> str:\n","        \"\"\"preprocess Tamil text preserving grapheme clusters\"\"\"\n","        text = unicodedata.normalize('NFC', text)\n","        text = text.replace(' ', f' {WHITESPACE}')\n","        text = re.sub(r'[^\\u0B80-\\u0BFF\\s\\w\\u0030-\\u0039\\u002E\\u002C\\u003F\\u0021\\u003A\\u003B\\u002D]', ' ', text)\n","        text = re.sub(r'([a-zA-Z]+)', r' \\1 ', text)\n","        text = re.sub(r'([௦-௯]+)', r' \\1 ', text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text.strip()\n","\n","    def _get_words(self, file: str) -> list[Word]:\n","        logging.info(f'Loading corpus from {file}...')\n","        start_time = time.time()\n","\n","        counter = MCounter()\n","        with open(file, 'r', encoding='utf-8') as f:\n","            for i, line in enumerate(f):\n","                if not line.strip():\n","                    continue\n","\n","                processed_line = self._preprocess_tamil_text(line.strip())\n","                if not processed_line:\n","                    continue\n","\n","                words = processed_line.split()\n","                words = [WHITESPACE + word if not word.startswith(WHITESPACE) else word for word in words]\n","                counter.update(words)\n","\n","                if i > 0 and i % 50000 == 0:\n","                    logging.info(f'Processed {i} lines.')\n","\n","        num_words = len(counter)\n","        logging.info(f'Loaded {num_words} unique words in {time.time() - start_time:.2f}s.')\n","\n","        return [Word(i, word, freq) for i, (word, freq) in enumerate(counter.items())]\n","\n","    def _extract_graphemes(self, words: list[Word]) -> MCounter:\n","        \"\"\"extract graphemes with Tamil cluster awareness\"\"\"\n","        logging.info('Extracting Tamil grapheme clusters from corpus...')\n","        start_time = time.time()\n","\n","        grapheme_counter = MCounter()\n","\n","        for i, word in enumerate(words):\n","            clusters = list(grapheme.graphemes(word.str))\n","\n","            for cluster in clusters:\n","                if cluster.strip():\n","                    grapheme_counter[cluster] += word.freq\n","                    self.grapheme_vocab.add(cluster)\n","\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for Tamil grapheme extraction.')\n","\n","        processing_time = time.time() - start_time\n","        logging.info(f'Extracted {len(grapheme_counter)} Tamil grapheme clusters in {processing_time:.2f}s.')\n","\n","        tamil_clusters = sum(1 for g in grapheme_counter if any('\\u0B80' <= char <= '\\u0BFF' for char in g))\n","        logging.info(f'Found {tamil_clusters} Tamil script clusters out of {len(grapheme_counter)} total.')\n","\n","        return grapheme_counter\n","\n","    def _filter_graphemes(self, graphemes: MCounter) -> MCounter:\n","        \"\"\"filter rare graphemes based on coverage threshold\"\"\"\n","        if self.coverage < 1:\n","            corpus_size = sum(graphemes.values())\n","            freq_to_remove = corpus_size - round(self.coverage * corpus_size)\n","            if freq_to_remove > 0:\n","                cum_sum = np.cumsum([freq for _, freq in reversed(graphemes.most_common())])\n","                num_to_remove = np.searchsorted(cum_sum, freq_to_remove)\n","                graphemes_to_remove = [g for g, _ in graphemes.most_common()[-num_to_remove:]]\n","                for g in graphemes_to_remove:\n","                    graphemes.pop(g)\n","                    self.grapheme_vocab.discard(g)\n","                logging.info(f'Replaced {num_to_remove} rare graphemes with UNK.')\n","        return graphemes\n","\n","    def _initialize_vocab(self, graphemes: MCounter) -> None:\n","        \"\"\"initialize the BPE vocabulary from extracted graphemes\"\"\"\n","        next_id = self.new_id\n","\n","        for grapheme_str in sorted(graphemes.keys()):\n","            if grapheme_str not in self.str2token:\n","                token_obj = Token(str=grapheme_str, id=next_id, special=False)\n","                self.id2token[next_id] = token_obj\n","                self.str2token[grapheme_str] = token_obj\n","                next_id += 1\n","\n","        self.actual_vocab_size = len(self.id2token)\n","        self.new_id = next_id\n","\n","        special_count = len([t for t in self.id2token.values() if t.special])\n","        logging.info(f\"Initialized vocab: {self.actual_vocab_size} tokens ({special_count} special + {self.actual_vocab_size - special_count} learned)\")\n","\n","    def _validate_tamil_merge(self, pair: tuple) -> bool:\n","        \"\"\"validate that merging two tokens won't break Tamil grapheme rules\"\"\"\n","        if not self._validate_pair(pair):\n","            return False\n","\n","        left_str = pair[0].str\n","        right_str = pair[1].str\n","        merged_str = left_str + right_str\n","\n","        original_graphemes = list(grapheme.graphemes(left_str)) + list(grapheme.graphemes(right_str))\n","        merged_graphemes = list(grapheme.graphemes(merged_str))\n","\n","        return len(merged_graphemes) <= len(original_graphemes)\n","\n","    def _encode_words(self, words: list[Word]) -> None:\n","        logging.info('Encoding words with Tamil graphemes...')\n","\n","        for i, word in enumerate(words):\n","            word.encode(self.str2token)\n","\n","            if i < 5:\n","                tokens_str = [token.str for token in word.tokens]\n","                logging.info(f'Word \"{word.str}\" -> tokens: {tokens_str}')\n","                if word.pairs:\n","                    pair_strs = [(f\"{p[0].str}+{p[1].str}\", freq) for p, freq in word.pairs.items()]\n","                    logging.info(f'  Pairs: {pair_strs[:5]}')\n","\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for grapheme encoding.')\n","\n","        logging.info(f'Encoding complete.')\n","\n","    def _initialize_pairs(self, words: list[Word]) -> MCounter:\n","        \"\"\"initialize pair frequencies from grapheme-encoded words\"\"\"\n","        pairs = MCounter()\n","        logging.info('Counting Tamil grapheme pairs...')\n","\n","        for i, word in enumerate(words):\n","            pairs.update(word.pairs)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for pair counting.')\n","\n","        to_remove = set()\n","        for pair in pairs:\n","            if not self._validate_pair(pair):\n","                to_remove.add(pair)\n","\n","        for pair in to_remove:\n","            pairs.pop(pair)\n","\n","        logging.info(f'Final pair count: {len(pairs)} unique pairs, {sum(pairs.values())} total instances.')\n","\n","        if pairs:\n","            top_pairs = pairs.most_common(10)\n","            logging.info(f'Top 10 pairs: {[(f\"{p[0].str}+{p[1].str}\", freq) for p, freq in top_pairs]}')\n","\n","        return pairs\n","\n","    def _remove_if_possible(self, token: Token, merged_freq: int, pairs: MCounter) -> bool:\n","        \"\"\"remove token if it meets the threshold criteria\"\"\"\n","        if token.freq + merged_freq == 0:\n","            return False\n","\n","        if merged_freq / (token.freq + merged_freq) > self.threshold:\n","            split = token.split_if_possible()\n","            if split is not None:\n","                self.actual_vocab_size -= 1\n","                for t in split:\n","                    t.freq += token.freq\n","                for pair in zip(split[:-1], split[1:]):\n","                    pairs[pair] += token.freq\n","\n","                pairs_for_update = MCounter()\n","                for word in token.words:\n","                    if token not in word.tokens:\n","                        raise ValueError(f'Token {token} not found in word {word}.')\n","                    pairs_for_update.update({\n","                        pair: freq for pair, freq in word.pairs.items()\n","                        if self._validate_pair(pair) and token in pair\n","                    })\n","                    word.split_token(token, split)\n","\n","                self._update_pairs_on_remove(token, split, pairs_for_update, pairs)\n","                token.remove()\n","                return True\n","        return False\n","\n","    @staticmethod\n","    def _update_pairs_on_merge(new_token: Token, pair: tuple[Token, Token],\n","                              pairs_for_update: MCounter, pairs: MCounter):\n","        \"\"\"update pair frequencies after merge operation\"\"\"\n","        pairs.update(pairs_for_update)\n","        for p, freq in pairs_for_update.items():\n","            if new_token not in p:\n","                raise ValueError(f'Pair {p} does not contain the new token {new_token}.')\n","            if new_token is p[0]:\n","                if new_token is p[1]:\n","                    to_update = (pair[1], pair[0])\n","                else:\n","                    to_update = (pair[1], p[1])\n","            else:\n","                to_update = (p[0], pair[0])\n","            if to_update in pairs:\n","                pairs[to_update] -= freq\n","                if pairs[to_update] <= 0:\n","                    pairs.pop(to_update)\n","\n","    @staticmethod\n","    def _update_pairs_on_remove(token: Token, split: list[Token],\n","                               pairs_for_update: MCounter, pairs: MCounter):\n","        \"\"\"update pair frequencies after split operation\"\"\"\n","        for pair, freq in pairs_for_update.items():\n","            if token is pair[0]:\n","                if token is pair[1]:\n","                    to_update = (split[-1], split[0])\n","                else:\n","                    to_update = (split[-1], pair[1])\n","            else:\n","                to_update = (pair[0], split[0])\n","            pairs[to_update] += freq\n","            pairs.pop(pair)\n","\n","    def _merge_token_in_words(self, token_to_merge: Token, pair_to_merge: tuple[Token, Token],\n","                             pairs: MCounter) -> int:\n","        \"\"\"merge token in all relevant words\"\"\"\n","        actual_freq = 0\n","        pairs_for_update = MCounter()\n","\n","        for word in pair_to_merge[0].words & pair_to_merge[1].words:\n","            if pair_to_merge in word.pairs:\n","                word.pairs.pop(pair_to_merge)\n","                actual_freq += word.merge_pair(pair_to_merge, token_to_merge)\n","                pairs_for_update.update({\n","                    p: f for p, f in word.pairs.items()\n","                    if self._validate_pair(p) and token_to_merge in p\n","                })\n","\n","        self._update_pairs_on_merge(token_to_merge, pair_to_merge, pairs_for_update, pairs)\n","        token_to_merge.freq += actual_freq\n","\n","        if pair_to_merge[0] is pair_to_merge[1]:\n","            pair_to_merge[0].freq -= 2 * actual_freq\n","            removed = self._remove_if_possible(pair_to_merge[0], actual_freq, pairs)\n","            if removed:\n","                logging.info(f'Removed token {pair_to_merge[0].str} after merging into {token_to_merge.str}.')\n","                self.events.append(('SPLIT', pair_to_merge[0], pair_to_merge[0].walk()))\n","        else:\n","            for token in pair_to_merge:\n","                if not token.present:\n","                    raise ValueError(f'Token {token} is not present in vocabulary.')\n","                token.freq -= actual_freq\n","                removed = self._remove_if_possible(token, actual_freq, pairs)\n","                if removed:\n","                    logging.info(f'Removed token {token.str} after merging into {token_to_merge.str}.')\n","                    self.events.append(('SPLIT', token, token.walk()))\n","\n","        return actual_freq\n","\n","    def _merge_pair(self, pair: tuple[Token, Token], pairs: MCounter) -> int:\n","        \"\"\"merge a token pair with Tamil validation\"\"\"\n","        if not self._validate_tamil_merge(pair):\n","            return 0\n","\n","        pairs.pop(pair)\n","        merged_str = pair[0].str + pair[1].str\n","\n","        if merged_str in self.str2token:\n","            new_token = self.str2token[merged_str]\n","            if not new_token.present:\n","                new_token.restore()\n","                logging.info(f'Restored previously removed token {new_token.str}.')\n","            else:\n","                logging.info(f'Additional merges for {new_token.str}.')\n","        else:\n","            new_token = Token(self.new_id, merged_str, 0, left=pair[0], right=pair[1])\n","            self.id2token[new_token.id] = new_token\n","            self.str2token[new_token.str] = new_token\n","            self.new_id += 1\n","\n","        self.events.append(('MERGE', pair, new_token))\n","        actual_freq = self._merge_token_in_words(new_token, pair, pairs)\n","        return actual_freq\n","\n","    def fit(self, input_file: str, output_dir: str, logging_step: int = 200) -> None:\n","        \"\"\"train Grapheme PickyBPE tokenizer\"\"\"\n","        output_path = Path(output_dir)\n","        output_path.mkdir(parents=True, exist_ok=True)\n","\n","        logging.info(\"Starting Tamil Grapheme-Aware PickyBPE training...\")\n","        logging.info(f\"Input: {input_file}\")\n","        logging.info(f\"Output: {output_dir}\")\n","        logging.info(f\"Target vocab size: {self.desired_vocab_size:,}\")\n","        logging.info(f\"Coverage: {self.coverage*100:.2f}%\")\n","        logging.info(f\"Threshold: {self.threshold*100:.2f}%\")\n","\n","        words = self._get_words(input_file)\n","        graphemes = self._extract_graphemes(words)\n","        filtered_graphemes = self._filter_graphemes(graphemes)\n","        self._initialize_vocab(filtered_graphemes)\n","        self._encode_words(words)\n","        pairs = self._initialize_pairs(words)\n","\n","        merge_time = []\n","        logging.info(f'Starting BPE training with {self.actual_vocab_size} initial tokens.')\n","\n","        while self.actual_vocab_size < self.desired_vocab_size:\n","            start_time = time.time()\n","            if not pairs:\n","                logging.info(f'No more pairs to merge. Stopping with vocab size of {self.actual_vocab_size}.')\n","                break\n","\n","            pair, count = pairs.most_common(1)[0]\n","            if count <= 0:\n","                logging.info(f'No more pairs to merge. Stopping with vocab size of {self.actual_vocab_size}.')\n","                break\n","\n","            freq = self._merge_pair(pair, pairs)\n","            self.actual_vocab_size += 1\n","            merge_time.append(time.time() - start_time)\n","\n","            if self.actual_vocab_size % logging_step == 0:\n","                avg_time = np.mean(merge_time) if merge_time else 0\n","                current_speed = 1.0 / avg_time if avg_time > 0 else 0\n","                logging.info(\n","                    f'VOCABULARY SIZE: {self.actual_vocab_size:,}/{self.desired_vocab_size:,}. '\n","                    f'Merged \"{pair[0].str}\" + \"{pair[1].str}\" with frequency {freq:,}. '\n","                    f'Speed: {current_speed:.1f} merges/sec'\n","                )\n","                merge_time = []\n","\n","        self._save_picky_model(output_path / 'grapheme_picky_bpe_model.json')\n","        self._save_huggingface_files(output_path)\n","\n","        logging.info(f'Training completed. Files saved to {output_path}')\n","\n","    def _save_picky_model(self, file_path: Path) -> None:\n","        logging.info(f'Saving Tamil Grapheme PickyBPE model to {file_path}...')\n","\n","        assigned_ids = sorted(self.id2token.keys())\n","        id_mapping = {}\n","        id_counter = 0\n","\n","        for i in assigned_ids:\n","            if self.id2token[i].present:\n","                id_mapping[i] = id_counter\n","                id_counter += 1\n","\n","        model_data = {\n","            'language': 'tamil',\n","            'script': 'tamil',\n","            'algorithm': 'Grapheme-Aware PickyBPE',\n","            'tokens': [token.to_dict() for token in self.id2token.values()],\n","            'id2int': {str(k): v for k, v in id_mapping.items()},\n","            'int2id': {str(v): k for k, v in id_mapping.items()},\n","            'merges': [\n","                {'id': i, 'pair': [token.to_dict() for token in merge[1]], 'new_token': merge[2].to_dict()}\n","                for i, merge in enumerate(self.events) if merge[0] == 'MERGE'\n","            ],\n","            'splits': [\n","                {'id': i, 'token': merge[1].to_dict(), 'split': [token.to_dict() for token in merge[2]]}\n","                for i, merge in enumerate(self.events) if merge[0] == 'SPLIT'\n","            ],\n","            'training_config': {\n","                'coverage': self.coverage,\n","                'threshold': self.threshold,\n","                'vocab_size': self.desired_vocab_size\n","            },\n","            'grapheme_vocab': list(self.grapheme_vocab)\n","        }\n","\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(model_data, f, indent=2, ensure_ascii=False)\n","\n","    def _save_huggingface_files(self, output_path: Path) -> None:\n","\n","        vocab = {}\n","        added_tokens = []\n","        special_tokens = [UNK, BOS, EOS, PAD]\n","\n","        next_id = 0\n","        for tok in special_tokens:\n","            vocab[tok] = next_id\n","            added_tokens.append({\n","                \"id\": next_id,\n","                \"content\": tok,\n","                \"special\": True,\n","                \"single_word\": False,\n","                \"lstrip\": False,\n","                \"rstrip\": False,\n","                \"normalized\": False\n","            })\n","            next_id += 1\n","\n","        non_special_tokens = [\n","            token for token in self.id2token.values()\n","            if token.present and not token.special\n","        ]\n","        for token in non_special_tokens:\n","            if token.str not in vocab:\n","                vocab[token.str] = next_id\n","                next_id += 1\n","\n","        logging.info(f\"Final vocab size: {len(vocab)}\")\n","\n","        merges = []\n","        for event in self.events:\n","            if event[0] == 'MERGE':\n","                left, right = event[1]\n","                if left.str in vocab and right.str in vocab:\n","                    merges.append(f\"{left.str} {right.str}\")\n","\n","        logging.info(f\"Number of merge rules: {len(merges)}\")\n","\n","        tokenizer_data = {\n","            \"version\": \"1.0\",\n","            \"truncation\": None,\n","            \"padding\": None,\n","            \"added_tokens\": added_tokens,\n","            \"normalizer\": {\"type\": \"NFC\"},\n","            \"pre_tokenizer\": {\n","                \"type\": \"Metaspace\",\n","                \"replacement\": WHITESPACE,\n","                \"add_prefix_space\": True,\n","                \"prepend_scheme\": \"always\"\n","            },\n","            \"post_processor\": {\n","                \"type\": \"TemplateProcessing\",\n","                \"single\": f\"{BOS}:1 $A:0 {EOS}:2\",\n","                \"pair\": f\"{BOS}:1 $A:0 {EOS}:2 $B:0 {EOS}:2\",\n","                \"special_tokens\": {\n","                    BOS: {\"id\": vocab[BOS], \"type_id\": 1},\n","                    EOS: {\"id\": vocab[EOS], \"type_id\": 1}\n","                }\n","            },\n","            \"decoder\": {\n","                \"type\": \"Metaspace\",\n","                \"replacement\": WHITESPACE,\n","                \"add_prefix_space\": True,\n","                \"prepend_scheme\": \"always\"\n","            },\n","            \"model\": {\n","                \"type\": \"BPE\",\n","                \"unk_token\": UNK,\n","                \"vocab\": vocab,\n","                \"merges\": merges\n","            }\n","        }\n","\n","        (output_path / \"tokenizer.json\").write_text(\n","            json.dumps(tokenizer_data, indent=2, ensure_ascii=False), encoding=\"utf-8\"\n","        )\n","        (output_path / \"vocab.json\").write_text(\n","            json.dumps(vocab, indent=2, ensure_ascii=False), encoding=\"utf-8\"\n","        )\n","        (output_path / \"added_tokens.json\").write_text(\n","            json.dumps(added_tokens, indent=2, ensure_ascii=False), encoding=\"utf-8\"\n","        )\n","        (output_path / \"special_tokens_map.json\").write_text(\n","            json.dumps({\"bos_token\": BOS, \"eos_token\": EOS, \"unk_token\": UNK, \"pad_token\": PAD},\n","                      indent=2, ensure_ascii=False),\n","            encoding=\"utf-8\"\n","        )\n","        (output_path / \"tokenizer_config.json\").write_text(\n","            json.dumps({\n","                \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","                \"auto_map\": {\"AutoTokenizer\": [\"tokenizer.json\", None]},\n","                \"bos_token\": BOS,\n","                \"eos_token\": EOS,\n","                \"unk_token\": UNK,\n","                \"pad_token\": PAD,\n","                \"model_max_length\": 2048,\n","                \"padding_side\": \"left\",\n","                \"truncation_side\": \"right\",\n","                \"language\": \"tamil\",\n","                \"script\": \"tamil\",\n","                \"algorithm\": \"Grapheme-Aware PickyBPE\"\n","            }, indent=2, ensure_ascii=False),\n","            encoding=\"utf-8\"\n","        )\n","\n","        logging.info(\"HuggingFace-compatible files saved.\")\n","\n","def train_tamil_grapheme_tokenizer(\n","    input_file: str,\n","    output_dir: str = \"./tamil_grapheme_tokenizer\",\n","    vocab_size: int = 10000,\n","    coverage: float = 0.9999,\n","    threshold: float = 0.9999,\n","    logging_step: int = 200\n","):\n","    \"\"\"train a Tamil tokenizer with grapheme awareness\"\"\"\n","\n","    tokenizer = TamilGraphemePickyBPE(\n","        vocab_size=vocab_size,\n","        coverage=coverage,\n","        threshold=threshold\n","    )\n","\n","    start_time = time.time()\n","    tokenizer.fit(input_file, output_dir, logging_step)\n","    training_time = time.time() - start_time\n","\n","    print(f\"\\nTamil tokenizer training completed in {training_time:.2f} seconds\")\n","    print(f\"Files saved to: {output_dir}\")\n","\n","    return tokenizer\n","\n","def test_tamil_grapheme_tokenizer(tokenizer_path: str):\n","\n","    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n","\n","    test_phrases = [\n","        \"வணக்கம்\",\n","        \"தமிழ்நாடு\",\n","        \"கணினி அறிவியல்\",\n","        \"வணக்கம், நான் தமிழ் பேசுகிறேன்\"\n","    ]\n","\n","    print(\"\\nTesting Tamil tokenizer:\")\n","    print(\"=\" * 60)\n","\n","    all_perfect = True\n","\n","    for phrase in test_phrases:\n","        tokens = tokenizer.tokenize(phrase)\n","        decoded = tokenizer.decode(tokenizer.encode(phrase, add_special_tokens=False))\n","        perfect = phrase == decoded\n","        all_perfect = all_perfect and perfect\n","\n","        print(f\"Original: '{phrase}'\")\n","        print(f\"Tokens: {tokens}\")\n","        print(f\"Decoded: '{decoded}'\")\n","        print(f\"Perfect: {perfect}\")\n","        print(\"-\" * 40)\n","\n","    if all_perfect:\n","        print(\"All test phrases reconstructed perfectly\")\n","    else:\n","        print(\"Some reconstruction issues detected\")\n","\n","def main():\n","\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/dataset/ta_reduced_train.txt\"\n","    output_dir = \"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/tokenizers/grapheme_picky_bpe\"\n","\n","    vocab_size = 10000\n","    coverage = 0.9999\n","    threshold = 0.9999\n","    logging_step = 200\n","\n","    if not Path(input_file).exists():\n","        print(f\"Error: Input file not found: {input_file}\")\n","        return\n","\n","    tokenizer = train_tamil_grapheme_tokenizer(\n","        input_file=input_file,\n","        output_dir=output_dir,\n","        vocab_size=vocab_size,\n","        coverage=coverage,\n","        threshold=threshold,\n","        logging_step=logging_step\n","    )\n","\n","    test_tamil_grapheme_tokenizer(output_dir)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"aHLo_wcNXI5P"},"execution_count":null,"outputs":[]}]}