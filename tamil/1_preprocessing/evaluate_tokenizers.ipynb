{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMBNqyJuBuOFYqhb+Aqpp24"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GbhjqNzAkpmX"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install -q sentencepiece transformers datasets huggingface_hub"],"metadata":{"id":"18calgxrkvpG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import annotations\n","import os\n","import json\n","import time\n","import random\n","import logging\n","import unicodedata\n","import re\n","import getpass\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from collections import Counter, defaultdict\n","from typing import List, Dict, Union, Tuple, Optional\n","from transformers import AutoTokenizer\n","\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","class TokenizerEvaluator:\n","\n","    def __init__(self, test_texts: List[str]):\n","        self.test_texts = test_texts\n","\n","    def calculate_fertility(self, tokenizer, texts: List[str]) -> float:\n","        \"\"\"calculate average tokens per word\"\"\"\n","        total_tokens = 0\n","        total_words = 0\n","\n","        for text in texts:\n","            words = text.split()\n","            total_words += len(words)\n","\n","            if hasattr(tokenizer, 'encode_as_ids') and 'SentencePiece' in str(type(tokenizer)):\n","                tokens = tokenizer.encode_as_ids(text)\n","            elif hasattr(tokenizer, 'encode'):\n","                try:\n","                    tokens = tokenizer.encode(text, add_special_tokens=False)\n","                except TypeError:\n","                    tokens = tokenizer.encode(text)\n","            else:\n","                tokens = tokenizer.tokenize(text)\n","            total_tokens += len(tokens)\n","\n","        return total_tokens / total_words if total_words > 0 else 0\n","\n","    def calculate_compression_ratio(self, tokenizer, texts: List[str]) -> float:\n","        \"\"\"calculate compression ratio (chars per token)\"\"\"\n","        total_chars = 0\n","        total_tokens = 0\n","\n","        for text in texts:\n","            total_chars += len(text)\n","\n","            if hasattr(tokenizer, 'encode_as_ids') and 'SentencePiece' in str(type(tokenizer)):\n","                tokens = tokenizer.encode_as_ids(text)\n","            elif hasattr(tokenizer, 'encode'):\n","                try:\n","                    tokens = tokenizer.encode(text, add_special_tokens=False)\n","                except TypeError:\n","                    tokens = tokenizer.encode(text)\n","            else:\n","                tokens = tokenizer.tokenize(text)\n","            total_tokens += len(tokens)\n","\n","        return total_chars / total_tokens if total_tokens > 0 else 0\n","\n","    def calculate_coverage(self, tokenizer, texts: List[str]) -> float:\n","        \"\"\"calculate vocabulary coverage (1 - UNK ratio)\"\"\"\n","        total_tokens = 0\n","        unk_tokens = 0\n","\n","        for text in texts:\n","            if hasattr(tokenizer, 'encode_as_ids') and 'SentencePiece' in str(type(tokenizer)):\n","                tokens = tokenizer.encode_as_ids(text)\n","                unk_tokens += sum(1 for t in tokens if t == 1)\n","            elif hasattr(tokenizer, 'encode'):\n","                try:\n","                    tokens = tokenizer.encode(text, add_special_tokens=False)\n","                except TypeError:\n","                    tokens = tokenizer.encode(text)\n","                unk_ids = {0, 1, tokenizer.unk_token_id} if hasattr(tokenizer, 'unk_token_id') else {0, 1}\n","                unk_tokens += sum(1 for t in tokens if t in unk_ids)\n","            else:\n","                tokens = tokenizer.tokenize(text)\n","                unk_tokens += sum(1 for t in tokens if '<unk>' in str(t).lower() or '[unk]' in str(t).lower())\n","\n","            total_tokens += len(tokens)\n","\n","        coverage = 1 - (unk_tokens / total_tokens) if total_tokens > 0 else 0\n","        return max(0, coverage)\n","\n","    def calculate_token_length_distribution(self, tokenizer, texts: List[str]) -> Dict[str, float]:\n","        \"\"\"calculate token length statistics\"\"\"\n","        token_lengths = []\n","        sample_texts = texts[:100]\n","\n","        for text in sample_texts:\n","            if hasattr(tokenizer, 'encode_as_pieces') and 'SentencePiece' in str(type(tokenizer)):\n","                tokens = tokenizer.encode_as_pieces(text)\n","            elif hasattr(tokenizer, 'tokenize'):\n","                tokens = tokenizer.tokenize(text)\n","            else:\n","                continue\n","\n","            for token in tokens:\n","                token_str = str(token).replace('▁', '').replace('Ġ', '')\n","                token_lengths.append(len(token_str))\n","\n","        if not token_lengths:\n","            return {'mean_length': 0, 'std_length': 0}\n","\n","        return {\n","            'mean_length': np.mean(token_lengths),\n","            'std_length': np.std(token_lengths)\n","        }\n","\n","    def evaluate_tokenizer(self, tokenizer, name: str) -> Dict[str, float]:\n","\n","        logger.info(f\"Evaluating tokenizer: {name}\")\n","\n","        eval_texts = self.test_texts[:500] if len(self.test_texts) > 500 else self.test_texts\n","\n","        metrics = {\n","            'fertility': self.calculate_fertility(tokenizer, eval_texts),\n","            'compression_ratio': self.calculate_compression_ratio(tokenizer, eval_texts),\n","            'coverage': self.calculate_coverage(tokenizer, eval_texts)\n","        }\n","\n","        length_stats = self.calculate_token_length_distribution(tokenizer, eval_texts)\n","        metrics.update(length_stats)\n","\n","        metrics['composite_score'] = (\n","            metrics['compression_ratio'] * metrics['coverage'] / metrics['fertility']\n","            if metrics['fertility'] > 0 else 0\n","        )\n","\n","        logger.info(f\"Metrics for {name}: {metrics}\")\n","        return metrics\n","\n","class TamilCorpusProcessor:\n","    \"\"\"handles Tamil corpus loading and preprocessing\"\"\"\n","\n","    def __init__(self, max_sentences: int = 5000):\n","        self.max_sentences = max_sentences\n","        self.tamil_pattern = re.compile(r'[\\u0B80-\\u0BFF]')\n","        self.tamil_vowels = re.compile(r'[அஆஇஈஉஊஎஏஐஒஓஔ]')\n","        self.tamil_consonants = re.compile(r'[கஙசஞடணதநபமயரலவழளறன]')\n","\n","    def load_text_file(self, file_path: str, max_sentences: int = None) -> List[str]:\n","        \"\"\"load Tamil text file (one sentence per line)\"\"\"\n","        logger.info(f\"Loading text file from: {file_path}\")\n","\n","        if max_sentences is None:\n","            max_sentences = self.max_sentences\n","\n","        file_path = Path(file_path)\n","\n","        sentences = []\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for i, line in enumerate(f):\n","                if i >= max_sentences:\n","                    break\n","                line = line.strip()\n","                if line:\n","                    sentences.append(line)\n","\n","        valid_sentences = [s for s in sentences if self.is_valid_tamil_text(s)]\n","        logger.info(f\"Loaded {len(valid_sentences)} valid sentences from {len(sentences)} total\")\n","\n","        return valid_sentences[:max_sentences]\n","\n","    def split_into_sentences(self, text: str) -> List[str]:\n","        sentences = re.split(r'[.!?।]+', text)\n","        return [s.strip() for s in sentences if len(s.strip()) > 10]\n","\n","    def is_valid_tamil_text(self, text: str) -> bool:\n","        \"\"\"check if text is valid Tamil\"\"\"\n","        if len(text) < 10 or len(text) > 1000:\n","            return False\n","\n","        tamil_chars = len(self.tamil_pattern.findall(text))\n","        total_chars = len([c for c in text if c.isalpha()])\n","\n","        if total_chars == 0:\n","            return False\n","\n","        tamil_ratio = tamil_chars / total_chars\n","\n","        if tamil_ratio < 0.6:\n","            return False\n","\n","        common_tamil_words = ['அது', 'இது', 'நான்', 'நீ', 'அவன்', 'அவள்', 'நாம்', 'நீங்கள்', 'அவர்கள்', 'என்', 'உன்', 'அவன்', 'மற்றும்', 'ஆனால்']\n","        words = text.split()\n","        tamil_word_count = sum(1 for word in words if any(tw in word for tw in common_tamil_words))\n","\n","        return tamil_word_count > 0 or tamil_ratio > 0.7\n","\n","class TokenizerLoader:\n","\n","    def __init__(self, hf_token: str = None):\n","        self.hf_token = hf_token\n","\n","    def load_huggingface_tokenizer(self, model_name: str):\n","        tokenizer = AutoTokenizer.from_pretrained(\n","            model_name,\n","            token=self.hf_token,\n","            trust_remote_code=True\n","        )\n","        logger.info(f\"Loaded HuggingFace tokenizer: {model_name}\")\n","        return tokenizer\n","\n","def evaluate_tokenizers(test_texts: List[str], tokenizer_configs: List[Dict], hf_token: str = None) -> pd.DataFrame:\n","\n","    evaluator = TokenizerEvaluator(test_texts)\n","    loader = TokenizerLoader(hf_token=hf_token)\n","\n","    results = []\n","\n","    for config in tokenizer_configs:\n","        tokenizer_name = config['name']\n","        tokenizer_type = config['type']\n","        tokenizer_path = config['path']\n","\n","        logger.info(f\"Loading tokenizer: {tokenizer_name}\")\n","\n","        if tokenizer_type == 'huggingface':\n","            tokenizer = loader.load_huggingface_tokenizer(tokenizer_path)\n","        else:\n","            logger.error(f\"Unknown tokenizer type: {tokenizer_type}\")\n","            continue\n","\n","        if tokenizer is None:\n","            logger.warning(f\"Skipping {tokenizer_name} - failed to load\")\n","            continue\n","\n","        metrics = evaluator.evaluate_tokenizer(tokenizer, tokenizer_name)\n","        results.append({\"tokenizer\": tokenizer_name, \"type\": tokenizer_type, **metrics})\n","\n","    return pd.DataFrame(results)\n","\n","def main(dataset_path: str = None, max_sentences: int = 2000, hf_token: str = None):\n","\n","    logger.info(\"Starting Tamil tokenizer evaluation...\")\n","\n","    random.seed(42)\n","    np.random.seed(42)\n","\n","    processor = TamilCorpusProcessor(max_sentences=max_sentences)\n","\n","    if dataset_path:\n","        logger.info(f\"Using dataset: {dataset_path}\")\n","        test_texts = processor.load_text_file(dataset_path, max_sentences)\n","\n","    if len(test_texts) == 0:\n","        logger.error(\"No valid test texts loaded\")\n","        return\n","\n","    logger.info(f\"Successfully loaded {len(test_texts)} sentences for evaluation\")\n","\n","    tokenizer_configs = [\n","        {\n","            'name': 'Llama-2-7B',\n","            'type': 'huggingface',\n","            'path': 'meta-llama/Llama-2-7b-hf'\n","        },\n","        {\n","            'name': 'Gemma-7B',\n","            'type': 'huggingface',\n","            'path': 'google/gemma-7b'\n","        }\n","    ]\n","\n","    logger.info(f\"Configured {len(tokenizer_configs)} tokenizers for evaluation\")\n","\n","    print(\"Evaluating tokenizers...\")\n","    results_df = evaluate_tokenizers(test_texts, tokenizer_configs, hf_token=hf_token)\n","\n","    if len(results_df) == 0:\n","        print(\"ERROR: No tokenizers were successfully evaluated\")\n","        return\n","\n","    print(\"\\n\" + \"=\"*100)\n","    print(\"TAMIL TOKENIZER EVALUATION RESULTS\")\n","    print(\"=\"*100)\n","\n","    display_df = results_df.copy()\n","    numeric_cols = ['fertility', 'compression_ratio', 'coverage', 'composite_score', 'mean_length']\n","    for col in numeric_cols:\n","        if col in display_df.columns:\n","            display_df[col] = display_df[col].round(4)\n","\n","    print(display_df[['tokenizer', 'type', 'fertility', 'compression_ratio', 'coverage', 'composite_score']].to_string(index=False))\n","    print(\"=\"*100)\n","\n","    best_tokenizer = results_df.loc[results_df['composite_score'].idxmax()]\n","    print(f\"\\nBest Tokenizer: {best_tokenizer['tokenizer']}\")\n","    print(f\"  - Composite Score: {best_tokenizer['composite_score']:.4f}\")\n","    print(f\"  - Fertility: {best_tokenizer['fertility']:.4f} (lower is better)\")\n","    print(f\"  - Compression Ratio: {best_tokenizer['compression_ratio']:.4f} (higher is better)\")\n","    print(f\"  - Coverage: {best_tokenizer['coverage']:.4f} (higher is better)\")\n","\n","def run_evaluation():\n","\n","    print(\"Tamil Tokenizer Evaluation Script\")\n","    print(\"=\" * 50)\n","\n","    hf_token = getpass.getpass(\"EHugging Face token : \")\n","\n","    dataset_path = '/content/drive/My Drive/Colab Notebooks/LRLs/tamil/dataset/ta_reduced_eval.txt'\n","    max_sentences = 5000\n","\n","    print(f\"\\nDataset Configuration:\")\n","    print(f\"  Path: {dataset_path}\")\n","    print(f\"  Max Sentences: {max_sentences}\")\n","\n","    main(dataset_path=dataset_path, max_sentences=max_sentences, hf_token=hf_token)\n","\n","if __name__ == \"__main__\":\n","    run_evaluation()"],"metadata":{"id":"p-_y5FHblZed"},"execution_count":null,"outputs":[]}]}