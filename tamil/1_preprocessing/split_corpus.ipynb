{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXP2iVhW/ow7DSlPuAET52"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NquZ10t3ip0D"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Filter corpus."],"metadata":{"id":"TnM1SV33i89o"}},{"cell_type":"code","source":["\"\"\"\n","Filter a Tamil text corpus.\n","\"\"\"\n","import argparse\n","import random\n","import os\n","import hashlib\n","import re\n","import string\n","import time\n","from collections import defaultdict\n","from tqdm import tqdm  # For progress bar (optional, can be removed if not available)\n","\n","def get_file_size_mb(file_path):\n","    return os.path.getsize(file_path) / (1024 * 1024)\n","\n","def sample_avg_line_size(input_file, sample_size=10000):\n","    \"\"\"sample a number of lines to determine average line size\"\"\"\n","    total_size = 0\n","    count = 0\n","    with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:\n","        for _ in range(sample_size):\n","            line = f.readline()\n","            if not line:\n","                break\n","            total_size += len(line.encode('utf-8'))\n","            count += 1\n","    return total_size / count\n","\n","def get_default_filter_patterns():\n","    \"\"\"get default regex patterns to filter out common metadata and non-linguistic content\"\"\"\n","    return [\n","        # common metadata prefixes\n","        r'^Labels:',\n","        r'^Tags:',\n","        r'^Categories:',\n","        r'^Posted by:',\n","        r'^Author:',\n","        r'^Date:',\n","        r'^குறிச்சொற்கள்:',\n","\n","        # date patterns\n","        r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b',\n","        r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{4}\\b',\n","        r'\\d{4}\\s+(ஜனவரி|பிப்ரவரி|மார்ச்|ஏப்ரல்|மே|ஜூன்|ஜூலை|ஆகஸ்ட்|செப்டம்பர்|அக்டோபர்|நவம்பர்|டிசம்பர்)',\n","        r'(திங்கள்|செவ்வாய்|புதன்|வியாழன்|வெள்ளி|சனி|ஞாயிறு)க்கிழமை',\n","\n","        r'Admin\\s+\\w+day',\n","\n","        # common blog/website formatting\n","        r'^\\d+\\s+comments$',\n","        r'^Share this:',\n","        r'^பகிர்:',\n","        r'Comments\\s+-\\s+\\d+',\n","        r'Views\\s+-\\s+\\{\\{[\\w\\.]+\\}\\}',\n","\n","        # social media markers\n","        r'^வணக்கம்\\s+\\w+\\s*!',\n","        r'^நன்றி\\s+\\w+',\n","        r'அருமை\\.{2,}',\n","        r'தேனம்மை வருகைக்கு நன்றி',\n","\n","        # emotional expressions\n","        r'^ஹ{2,}',\n","        r'^\\){1,}$',\n","\n","        # cpyright\n","        r'^©',\n","        r'Copyright\\s+\\d{4}',\n","        r'All rights reserved',\n","\n","        # URL patterns\n","        r'^https?://',\n","        r'^www\\.',\n","        r'இருந்து மீள்விக்கப்பட்டது',\n","\n","        # email patterns\n","        r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+',\n","\n","        # question-answer indicators\n","        r'^ANSWER\\s*:',\n","        r'^\\d+\\)\\s+',\n","        r'^\\d+\\.\\s+',\n","\n","        # blog/forum patterns\n","        r'\\w+\\s+wrote:',\n","        r'^//.*//\\s*$',\n","        r'^என்னங்க',\n","        r'சார்!',\n","        r'மாதிரி$',\n","\n","        # time indicators\n","        r'நேரம்\\s+காலை|பகல்|மாலை',\n","\n","        r'ரூபாய்\\s+மட்டுமே$',\n","        r'சர்வீஸ்\\s+சார்ஜ்',\n","\n","        # news headline indicators\n","        r'!! -\\s+\\w+\\s+News$',\n","        r'^மோடிக்கு|^இந்நிலையில்',\n","\n","        r'Edit$',\n","        r'படங்கள்:',\n","        r'இதுல என்ன அரசியல்',\n","        r'சிரிச்சு முடியலை',\n","    ]\n","\n","def is_filtered_line(line, min_line_length=10, filter_patterns=None, tamil_ratio_threshold=0.3):\n","    \"\"\"check if a line should be filtered out based on various criteria\"\"\"\n","    line = line.strip()\n","\n","    if len(line) < min_line_length:\n","        return True\n","\n","    # apply regex pattern filtering\n","    if filter_patterns:\n","        for pattern in filter_patterns:\n","            if re.search(pattern, line):\n","                return True\n","\n","    # check Tamil to non-Tamil ratio\n","    if tamil_ratio_threshold > 0:\n","        tamil_pattern = re.compile(r'[\\u0B80-\\u0BFF]')\n","        tamil_chars = len(re.findall(tamil_pattern, line))\n","        # skip lines with too few Tamil characters relative to length\n","        if tamil_chars / max(1, len(line)) < tamil_ratio_threshold:\n","            # keep lines that are pure English/Latin and very long\n","            latin_pattern = re.compile(r'[a-zA-Z0-9\\s.,;:!?()\\[\\]{}\\'\"\"`~@#$%^&*+=_\\\\|<>/\\-]')\n","            latin_chars = len(re.findall(latin_pattern, line))\n","            if latin_chars / max(1, len(line)) > 0.8 and len(line) > 50:\n","                return False\n","            return True\n","\n","    return False\n","\n","def process_in_chunks(input_file, output_file, target_size_mb, chunk_size=500000, min_line_length=20,\n","                     tamil_ratio=0.6, use_filtering=True, resume_from_chunk=0):\n","    \"\"\"\n","    Args:\n","        input_file: Path to input file\n","        output_file: Path to output file\n","        target_size_mb: Target size in MB\n","        chunk_size: Number of lines to process in each chunk\n","        min_line_length: Minimum line length\n","        tamil_ratio: Minimum Tamil character ratio\n","        use_filtering: Whether to use pattern filtering\n","        resume_from_chunk: Resume from a specific chunk\n","    \"\"\"\n","    start_time = time.time()\n","\n","    original_size_mb = get_file_size_mb(input_file)\n","\n","    filter_patterns = get_default_filter_patterns() if use_filtering else None\n","\n","    # get target line count based on sampled average line size\n","    avg_line_size_bytes = sample_avg_line_size(input_file)\n","    target_lines = int((target_size_mb * 1024 * 1024) / avg_line_size_bytes)\n","\n","    print(f\"Original file size: {original_size_mb:.2f} MB\")\n","    print(f\"Target file size: {target_size_mb:.2f} MB\")\n","    print(f\"Average line size: {avg_line_size_bytes:.2f} bytes\")\n","    print(f\"Target number of lines: ~{target_lines:,}\")\n","\n","    estimated_total_lines = int((original_size_mb * 1024 * 1024) / avg_line_size_bytes)\n","    print(f\"Estimated total lines: ~{estimated_total_lines:,}\")\n","\n","    reservoir = []\n","    seen_hashes = set()  # for deduplication\n","    total_processed = 0\n","    total_filtered = 0\n","    unique_count = 0\n","\n","    pattern_match_counts = {i: 0 for i in range(len(filter_patterns))} if filter_patterns else {}\n","\n","    # load existing reservoir if resuming\n","    if resume_from_chunk > 0:\n","        resume_file = f\"{output_file}.part{resume_from_chunk}\"\n","        if os.path.exists(resume_file):\n","            print(f\"Resuming from chunk {resume_from_chunk}, loading existing reservoir...\")\n","            with open(resume_file, 'r', encoding='utf-8', errors='ignore') as f:\n","                for line in f:\n","                    reservoir.append(line.strip())\n","                    # rebuild hash set for deduplication\n","                    line_hash = hashlib.md5(line.strip().encode('utf-8')).hexdigest()\n","                    seen_hashes.add(line_hash)\n","            print(f\"Loaded {len(reservoir):,} lines from previous chunks\")\n","\n","    # process each chunk\n","    with open(input_file, 'r', encoding='utf-8', errors='ignore') as infile:\n","        if resume_from_chunk > 0:\n","            lines_to_skip = resume_from_chunk * chunk_size\n","            print(f\"Skipping {lines_to_skip:,} lines to resume...\")\n","            for _ in tqdm(range(lines_to_skip), desc=\"Skipping lines\"):\n","                infile.readline()\n","            total_processed = lines_to_skip\n","\n","        chunk_num = resume_from_chunk\n","        while True:\n","            chunk_num += 1\n","            chunk_start_time = time.time()\n","\n","            chunk_lines = []\n","            for _ in range(chunk_size):\n","                line = infile.readline()\n","                if not line:\n","                    break\n","                chunk_lines.append(line)\n","\n","            if not chunk_lines:\n","                break\n","\n","            print(f\"\\nProcessing chunk {chunk_num} ({len(chunk_lines):,} lines)...\")\n","\n","            chunk_filtered = 0\n","            chunk_unique = 0\n","\n","            for line in tqdm(chunk_lines, desc=\"Filtering and deduplicating\"):\n","                total_processed += 1\n","                line = line.strip()\n","\n","                if is_filtered_line(line, min_line_length, filter_patterns, tamil_ratio):\n","                    total_filtered += 1\n","                    chunk_filtered += 1\n","\n","                    # debug: count which patterns are matching\n","                    if filter_patterns:\n","                        for j, pattern in enumerate(filter_patterns):\n","                            if re.search(pattern, line):\n","                                pattern_match_counts[j] += 1\n","                    continue\n","\n","                # apply deduplication\n","                line_hash = hashlib.md5(line.encode('utf-8')).hexdigest()\n","                if line_hash in seen_hashes:\n","                    continue\n","\n","                seen_hashes.add(line_hash)\n","                unique_count += 1\n","                chunk_unique += 1\n","\n","                # reservoir sampling\n","                if len(reservoir) < target_lines:\n","                    reservoir.append(line)\n","                else:\n","                    j = random.randint(0, total_processed)\n","                    if j < target_lines:\n","                        reservoir[j % len(reservoir)] = line\n","\n","            chunk_time = time.time() - chunk_start_time\n","            elapsed_time = time.time() - start_time\n","            print(f\"Chunk {chunk_num} statistics:\")\n","            print(f\"  Processed: {len(chunk_lines):,} lines\")\n","            print(f\"  Filtered: {chunk_filtered:,} lines ({chunk_filtered/len(chunk_lines)*100:.2f}%)\")\n","            print(f\"  Unique: {chunk_unique:,} lines\")\n","            print(f\"  Time: {chunk_time:.2f} seconds ({len(chunk_lines)/chunk_time:.2f} lines/sec)\")\n","            print(f\"  Processed: {total_processed:,} lines (~{total_processed/estimated_total_lines*100:.2f}% of file)\")\n","            print(f\"  Filtered: {total_filtered:,} lines ({total_filtered/total_processed*100:.2f}%)\")\n","            print(f\"  Unique: {unique_count:,} lines\")\n","            print(f\"  Current reservoir size: {len(reservoir):,} lines\")\n","            print(f\"  Elapsed time: {elapsed_time:.2f} seconds\")\n","\n","            if total_processed > 0:\n","                lines_per_second = total_processed / elapsed_time\n","                remaining_lines = estimated_total_lines - total_processed\n","                estimated_remaining_time = remaining_lines / lines_per_second\n","                print(f\"  Estimated remaining time: {estimated_remaining_time/60:.2f} minutes\")\n","\n","            with open(f\"{output_file}.part{chunk_num}\", \"w\", encoding=\"utf-8\") as outfile:\n","                for line in reservoir:\n","                    outfile.write(line + \"\\n\")\n","            print(f\"Saved intermediate results to {output_file}.part{chunk_num}\")\n","\n","            with open(f\"{output_file}.stats{chunk_num}\", \"w\", encoding=\"utf-8\") as outfile:\n","                outfile.write(f\"Processed: {total_processed:,} lines\\n\")\n","                outfile.write(f\"Filtered: {total_filtered:,} lines\\n\")\n","                outfile.write(f\"Unique: {unique_count:,} lines\\n\")\n","                outfile.write(f\"Reservoir size: {len(reservoir):,} lines\\n\")\n","\n","            if total_processed >= estimated_total_lines:\n","                print(\"Reached or exceeded estimated total lines, finishing...\")\n","                break\n","\n","    if filter_patterns and total_filtered > 0:\n","        print(\"\\nFilter pattern statistics:\")\n","        for j, pattern in enumerate(filter_patterns):\n","            if pattern_match_counts[j] > 0:\n","                print(f\"Pattern '{pattern}' matched {pattern_match_counts[j]:,} lines\")\n","        print(f\"Total filtered: {total_filtered:,} lines ({total_filtered/total_processed*100:.2f}%)\")\n","\n","    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n","        for line in reservoir:\n","            outfile.write(line + \"\\n\")\n","\n","    final_size_mb = get_file_size_mb(output_file)\n","    total_time = time.time() - start_time\n","\n","    print(f\"\\nFinal results:\")\n","    print(f\"Final file size: {final_size_mb:.2f} MB\")\n","    print(f\"Reduction ratio: {final_size_mb/original_size_mb*100:.4f}%\")\n","    print(f\"Lines saved: {len(reservoir):,}\")\n","    print(f\"Total processing time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n","\n","def main():\n","    args = argparse.Namespace(\n","        input=\"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/dataset/ta.txt\",\n","        output=\"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/dataset/ta_reduced.txt\",\n","        target_size=6,  # target file size in MB\n","        chunk_size=500000,  # lines per chunk\n","        min_line_length=20,  # minimum line length\n","        tamil_ratio=0.6,  # minimum ratio of Tamil characters required\n","        use_filtering=True,  # enable pattern-based filtering\n","        resume_from_chunk=0  # start from beginning\n","    )\n","\n","    print(f\"Reducing {args.input} to approximately {args.target_size} MB using chunk processing...\")\n","\n","    process_in_chunks(\n","        args.input,\n","        args.output,\n","        args.target_size,\n","        chunk_size=args.chunk_size,\n","        min_line_length=args.min_line_length,\n","        tamil_ratio=args.tamil_ratio,\n","        use_filtering=args.use_filtering,\n","        resume_from_chunk=args.resume_from_chunk\n","    )\n","\n","    print(f\"Reduced file saved to {args.output}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"cHjCpooSi4FA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split corpus in train and eval sets."],"metadata":{"id":"MKQ3GkoQqPWI"}},{"cell_type":"code","source":["\"\"\"\n","Split a text corpus into training and evaluation sets.\n","\"\"\"\n","import os\n","import re\n","import argparse\n","import random\n","from pathlib import Path\n","from typing import List, Tuple\n","\n","def split_into_sentences(text: str) -> List[str]:\n","    # match sentence boundaries followed by spaces and capital letters\n","    sentence_pattern = r'(?<=[.!?])\\s+(?=[A-Z])'\n","    # split on the pattern, keep the separators\n","    sentences = re.split(sentence_pattern, text)\n","    # split by newlines for paragraphs and lists\n","    result = []\n","    for sentence in sentences:\n","        for line in sentence.split('\\n'):\n","            if line.strip():\n","                result.append(line.strip())\n","    return result\n","\n","def split_corpus(input_file: str, eval_size: float = 0.05, random_seed: int = 42, sentence_level: bool = True) -> Tuple[List[str], List[str]]:\n","    random.seed(random_seed)\n","    with open(input_file, 'r', encoding='utf-8') as f:\n","        content = f.read()\n","    # split into units (sentences or lines)\n","    if sentence_level:\n","        units = split_into_sentences(content)\n","        print(f\"Split corpus into {len(units)} sentences\")\n","    else:\n","        units = [line.strip() for line in content.split('\\n') if line.strip()]\n","        print(f\"Split corpus into {len(units)} lines\")\n","    random.shuffle(units)\n","    # calculate split point\n","    eval_count = max(1, int(len(units) * eval_size))\n","    # split corpus\n","    eval_units = units[:eval_count]\n","    train_units = units[eval_count:]\n","    print(f\"Training set: {len(train_units)} units ({100 - eval_size*100:.1f}%)\")\n","    print(f\"Evaluation set: {len(eval_units)} units ({eval_size*100:.1f}%)\")\n","    return train_units, eval_units\n","\n","def write_output_files(train_units: List[str], eval_units: List[str], output_dir: str, base_filename: str) -> Tuple[str, str]:\n","    os.makedirs(output_dir, exist_ok=True)\n","    base_name = Path(base_filename).stem\n","    train_path = os.path.join(output_dir, f\"{base_name}_train.txt\")\n","    eval_path = os.path.join(output_dir, f\"{base_name}_eval.txt\")\n","    with open(train_path, 'w', encoding='utf-8') as f:\n","        f.write('\\n'.join(train_units))\n","    with open(eval_path, 'w', encoding='utf-8') as f:\n","        f.write('\\n'.join(eval_units))\n","    train_size = os.path.getsize(train_path) / (1024 * 1024)\n","    eval_size = os.path.getsize(eval_path) / (1024 * 1024)\n","    print(f\"Training file size: {train_size:.2f} MB\")\n","    print(f\"Evaluation file size: {eval_size:.2f} MB\")\n","    return train_path, eval_path\n","\n","def validate_data_distribution(train_units: List[str], eval_units: List[str]) -> dict:\n","    train_chars = ''.join(train_units)\n","    eval_chars = ''.join(eval_units)\n","    train_unique_chars = set(train_chars)\n","    eval_unique_chars = set(eval_chars)\n","    eval_only_chars = eval_unique_chars - train_unique_chars\n","\n","    train_avg_len = sum(len(unit) for unit in train_units) / len(train_units) if train_units else 0\n","    eval_avg_len = sum(len(unit) for unit in eval_units) / len(eval_units) if eval_units else 0\n","\n","    train_words = ' '.join(train_units).split()\n","    eval_words = ' '.join(eval_units).split()\n","\n","    train_unique_words = set(train_words)\n","    eval_unique_words = set(eval_words)\n","\n","    eval_only_words = eval_unique_words - train_unique_words\n","    eval_only_words_pct = len(eval_only_words) / len(eval_unique_words) * 100 if eval_unique_words else 0\n","\n","    validation = {\n","        \"train_units\": len(train_units),\n","        \"eval_units\": len(eval_units),\n","        \"train_chars\": len(train_chars),\n","        \"eval_chars\": len(eval_chars),\n","        \"train_unique_chars\": len(train_unique_chars),\n","        \"eval_unique_chars\": len(eval_unique_chars),\n","        \"eval_only_chars\": len(eval_only_chars),\n","        \"eval_only_chars_list\": ''.join(sorted(eval_only_chars))[:100] if len(eval_only_chars) > 0 else \"\",\n","        \"train_avg_unit_length\": train_avg_len,\n","        \"eval_avg_unit_length\": eval_avg_len,\n","        \"train_unique_words\": len(train_unique_words),\n","        \"eval_unique_words\": len(eval_unique_words),\n","        \"eval_only_words\": len(eval_only_words),\n","        \"eval_only_words_pct\": eval_only_words_pct,\n","    }\n","\n","    print(\"\\nDistribution Validation:\")\n","    print(f\"Average unit length - Train: {train_avg_len:.1f}, Eval: {eval_avg_len:.1f} chars\")\n","    print(f\"Unique characters - Train: {len(train_unique_chars)}, Eval: {len(eval_unique_chars)}\")\n","\n","    if eval_only_chars:\n","        print(f\"Warning: {len(eval_only_chars)} characters appear in evaluation but not in training\")\n","        print(f\"First few eval-only chars: {validation['eval_only_chars_list'][:20]}...\")\n","\n","    print(f\"Unique words - Train: {len(train_unique_words)}, Eval: {len(eval_unique_words)}\")\n","    print(f\"Words only in evaluation set: {len(eval_only_words)} ({eval_only_words_pct:.1f}%)\")\n","\n","    return validation\n","\n","def main():\n","    parser = argparse.ArgumentParser(description=\"Split a corpus into training and evaluation sets\")\n","    parser.add_argument(\"--input_file\", type=str, required=True,\n","                        help=\"Path to the input corpus file\")\n","    parser.add_argument(\"--output_dir\", type=str, default=\"./\",\n","                        help=\"Directory to write output files\")\n","    parser.add_argument(\"--eval_size\", type=float, default=0.05,\n","                        help=\"Proportion of corpus to use for evaluation (0.0 to 1.0)\")\n","    parser.add_argument(\"--random_seed\", type=int, default=42,\n","                        help=\"Random seed for reproducible splits\")\n","    parser.add_argument(\"--sentence_level\", action=\"store_true\",\n","                        help=\"Split at sentence boundaries instead of line boundaries\")\n","\n","    args = argparse.Namespace(\n","        input_file=\"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/dataset/ta_reduced.txt\",\n","        output_dir=\"/content/drive/My Drive/Colab Notebooks/LRLs/tamil/dataset\",\n","        eval_size=0.05,\n","        random_seed=42,\n","        sentence_level=True\n","    )\n","\n","    print(f\"Splitting corpus file: {args.input_file}\")\n","    print(f\"Evaluation set size: {args.eval_size * 100:.1f}%\")\n","    print(f\"Using {'sentence' if args.sentence_level else 'line'} level splitting\")\n","\n","    train_units, eval_units = split_corpus(\n","        args.input_file,\n","        args.eval_size,\n","        args.random_seed,\n","        args.sentence_level\n","    )\n","\n","    base_filename = os.path.basename(args.input_file)\n","    train_path, eval_path = write_output_files(\n","        train_units,\n","        eval_units,\n","        args.output_dir,\n","        base_filename\n","    )\n","\n","    validation = validate_data_distribution(train_units, eval_units)\n","\n","    print(f\"  Training corpus: {train_path}\")\n","    print(f\"  Fertility evaluation corpus: {eval_path}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"T0xSxTY3qO-t"},"execution_count":null,"outputs":[]}]}