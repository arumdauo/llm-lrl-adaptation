{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["qEn7XsovoZXP"],"authorship_tag":"ABX9TyNJPM2pY/dIpfbe/gkiOGkz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JYXirzP2oFr9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# Standard BPE"],"metadata":{"id":"qEn7XsovoZXP"}},{"cell_type":"code","source":["from __future__ import annotations\n","import json\n","import logging\n","import time\n","import re\n","import sys\n","import unicodedata\n","from pathlib import Path\n","from collections import defaultdict, Counter\n","from typing import Union, Optional, Dict, List\n","import numpy as np\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    datefmt='%Y-%m-%d %H:%M:%S',\n","    handlers=[logging.StreamHandler(sys.stdout)],\n","    force=True\n",")\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","PAD = '<pad>'\n","UNK = '<unk>'\n","BOS = '<s>'\n","EOS = '</s>'\n","\n","class MCounter(Counter):\n","    \"\"\"extended Counter class with multiplication support\"\"\"\n","    def __mul__(self, other):\n","        if not isinstance(other, int):\n","            raise TypeError(\"Non-int factor\")\n","        return MCounter({k: other * v for k, v in self.items()})\n","\n","    def __rmul__(self, other):\n","        return self * other\n","\n","    def __add__(self, other):\n","        return MCounter(super().__add__(other))\n","\n","class SimpleBPE:\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        pad_id: int = 3,\n","        unk_id: int = 0,\n","        bos_id: int = 1,\n","        eos_id: int = 2,\n","        coverage: float = 0.9999,\n","    ):\n","        self.desired_vocab_size = vocab_size\n","        self.coverage = coverage\n","\n","        self.special_tokens = {\n","            PAD: pad_id,\n","            UNK: unk_id,\n","            BOS: bos_id,\n","            EOS: eos_id\n","        }\n","\n","        self.vocab = {}\n","        self.id2token = {}\n","        self.merges = []\n","\n","        for token_str, token_id in self.special_tokens.items():\n","            self.vocab[token_str] = token_id\n","            self.id2token[token_id] = token_str\n","\n","        self.next_id = max(self.special_tokens.values()) + 1\n","\n","    def _preprocess_nepali_text(self, text: str) -> str:\n","        \"\"\"preprocess Nepali text preserving word boundaries\"\"\"\n","        text = unicodedata.normalize('NFC', text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n","        text = re.sub(r'[^\\u0900-\\u097F\\u0980-\\u09FF\\sa-zA-Z0-9.,!?;:(){}[\\]\"\\'-]', ' ', text)\n","        text = re.sub(r'\\s*([.,!?;:])\\s*', r'\\1 ', text)\n","        text = re.sub(r'\\s*([(){}[\\]\"\\'])\\s*', r' \\1 ', text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text.strip()\n","\n","    def _get_word_frequencies(self, file: str) -> Dict[str, int]:\n","        \"\"\"load corpus and create word frequencies\"\"\"\n","        logging.info(f'Loading Nepali corpus from {file}...')\n","        start_time = time.time()\n","\n","        word_freqs = MCounter()\n","        char_freqs = MCounter()\n","        line_count = 0\n","\n","        with open(file, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                line_count += 1\n","                line = line.strip()\n","                if not line:\n","                    continue\n","\n","                processed_line = self._preprocess_nepali_text(line)\n","                if not processed_line:\n","                    continue\n","\n","                words = processed_line.split()\n","                for word in words:\n","                    if word:\n","                        word_freqs[word] += 1\n","                        for char in word:\n","                            char_freqs[char] += 1\n","\n","                if len(words) > 1:\n","                    char_freqs[' '] += len(words) - 1\n","\n","                if line_count % 50000 == 0:\n","                    logging.info(f'Processed {line_count:,} lines.')\n","\n","        logging.info(f'Loaded {len(word_freqs):,} unique words from {line_count:,} lines in {time.time() - start_time:.2f}s.')\n","        logging.info(f'Character types: {len(char_freqs):,}')\n","\n","        return dict(word_freqs), dict(char_freqs)\n","\n","    def _create_initial_splits(self, word_freqs: Dict[str, int], char_freqs: Dict[str, int]) -> Dict[str, List[str]]:\n","        \"\"\"create initial character-level splits for all words\"\"\"\n","        logging.info('Creating initial character splits...')\n","\n","        if self.coverage < 1:\n","            total_chars = sum(char_freqs.values())\n","            target_chars = round(self.coverage * total_chars)\n","\n","            sorted_chars = sorted(char_freqs.items(), key=lambda x: x[1], reverse=True)\n","            kept_chars = set()\n","            char_count = 0\n","\n","            for char, freq in sorted_chars:\n","                kept_chars.add(char)\n","                char_count += freq\n","                if char_count >= target_chars:\n","                    break\n","\n","            logging.info(f'Kept {len(kept_chars)} characters with {self.coverage*100:.2f}% coverage')\n","        else:\n","            kept_chars = set(char_freqs.keys())\n","\n","        if ' ' not in kept_chars:\n","            kept_chars.add(' ')\n","            logging.info('Added space character to vocabulary')\n","\n","        for char in kept_chars:\n","            if char not in self.vocab:\n","                self.vocab[char] = self.next_id\n","                self.id2token[self.next_id] = char\n","                self.next_id += 1\n","\n","        word_splits = {}\n","        unk_count = 0\n","\n","        for word in word_freqs:\n","            splits = []\n","            for char in word:\n","                if char in self.vocab:\n","                    splits.append(char)\n","                else:\n","                    splits.append(UNK)\n","                    unk_count += 1\n","            word_splits[word] = splits\n","\n","        devanagari_chars = sum(1 for char in kept_chars if '\\u0900' <= char <= '\\u097F')\n","        logging.info(f'Initialized vocabulary with {len(kept_chars)} characters')\n","        logging.info(f'Found {devanagari_chars} Devanagari characters')\n","\n","        if unk_count > 0:\n","            logging.warning(f'{unk_count} unknown characters found and replaced with {UNK}')\n","\n","        return word_splits\n","\n","    def _get_pairs(self, word_splits: Dict[str, List[str]], word_freqs: Dict[str, int]) -> MCounter:\n","        \"\"\"count all adjacent pairs in the vocabulary\"\"\"\n","        pairs = MCounter()\n","        for word, splits in word_splits.items():\n","            freq = word_freqs[word]\n","            for i in range(len(splits) - 1):\n","                pair = (splits[i], splits[i + 1])\n","                pairs[pair] += freq\n","        return pairs\n","\n","    def _merge_pair(self, pair: tuple[str, str], word_splits: Dict[str, List[str]],\n","                    word_freqs: Dict[str, int]) -> Dict[str, List[str]]:\n","        \"\"\"merge pair in all word splits\"\"\"\n","        left, right = pair\n","        merged = left + right\n","\n","        if merged not in self.vocab:\n","            self.vocab[merged] = self.next_id\n","            self.id2token[self.next_id] = merged\n","            self.next_id += 1\n","\n","        self.merges.append(pair)\n","\n","        new_word_splits = {}\n","        for word, splits in word_splits.items():\n","            new_splits = []\n","            i = 0\n","            while i < len(splits):\n","                if i < len(splits) - 1 and splits[i] == left and splits[i + 1] == right:\n","                    new_splits.append(merged)\n","                    i += 2\n","                else:\n","                    new_splits.append(splits[i])\n","                    i += 1\n","            new_word_splits[word] = new_splits\n","\n","        return new_word_splits\n","\n","    def fit(self, input_file: str, output_dir: str, logging_step: int = 100) -> None:\n","\n","        output_path = Path(output_dir)\n","        output_path.mkdir(parents=True, exist_ok=True)\n","\n","        word_freqs, char_freqs = self._get_word_frequencies(input_file)\n","        word_splits = self._create_initial_splits(word_freqs, char_freqs)\n","\n","        current_vocab_size = len(self.vocab)\n","        logging.info(f'Starting BPE training with {current_vocab_size} initial tokens')\n","        logging.info(f'Target vocabulary size: {self.desired_vocab_size}')\n","\n","        merge_times = []\n","        while current_vocab_size < self.desired_vocab_size:\n","            start_time = time.time()\n","\n","            pairs = self._get_pairs(word_splits, word_freqs)\n","\n","            if not pairs:\n","                logging.info('No more pairs to merge. Stopping training.')\n","                break\n","\n","            most_frequent_pair, freq = pairs.most_common(1)[0]\n","\n","            if freq <= 1:\n","                logging.info('No pairs with frequency > 1. Stopping training.')\n","                break\n","\n","            word_splits = self._merge_pair(most_frequent_pair, word_splits, word_freqs)\n","            current_vocab_size += 1\n","\n","            merge_times.append(time.time() - start_time)\n","\n","            if current_vocab_size % logging_step == 0:\n","                left, right = most_frequent_pair\n","                avg_time = np.mean(merge_times) if merge_times else 0\n","                space_info = \" [SPACE]\" if ' ' in (left, right) else \"\"\n","                logging.info(\n","                    f'Vocab size: {current_vocab_size:,}/{self.desired_vocab_size:,}. '\n","                    f'Merged \"{left}\" + \"{right}\" (freq: {freq:,}){space_info}. '\n","                    f'Avg merge time: {avg_time:.3f}s'\n","                )\n","                merge_times = []\n","\n","        logging.info(f'Training completed with final vocabulary size: {len(self.vocab):,}')\n","\n","        space_tokens = [token for token in self.vocab.keys() if ' ' in token]\n","        logging.info(f'Created {len(space_tokens)} tokens containing spaces')\n","\n","        self._save_model_files(output_path)\n","        logging.info(f'Files saved to {output_path}')\n","\n","    def _save_model_files(self, output_path: Path) -> None:\n","        \"\"\"save model files compatible with Llama integration\"\"\"\n","        self._save_simple_bpe_model(output_path / 'simple_bpe_model.json')\n","        self._save_files(output_path)\n","\n","    def _save_simple_bpe_model(self, file_path: Path) -> None:\n","        logging.info(f'Saving BPE model to {file_path}...')\n","\n","        model_data = {\n","            'vocab': self.vocab,\n","            'id2token': self.id2token,\n","            'merges': [{'left': left, 'right': right} for left, right in self.merges],\n","            'vocab_size': len(self.vocab),\n","            'special_tokens': self.special_tokens,\n","            'approach': 'bpe',\n","            'language': 'nepali',\n","            'script': 'devanagari'\n","        }\n","\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(model_data, f, indent=2, ensure_ascii=False)\n","\n","    def _save_files(self, output_path: Path) -> None:\n","\n","        with open(output_path / 'vocab.json', 'w', encoding='utf-8') as f:\n","            json.dump(self.vocab, f, indent=2, ensure_ascii=False)\n","\n","        with open(output_path / 'merges.txt', 'w', encoding='utf-8') as f:\n","            f.write('#version: 0.2\\n')\n","            for left, right in self.merges:\n","                f.write(f'{left} {right}\\n')\n","\n","        metadata = {\n","            \"tokenizer_type\": \"bpe\",\n","            \"vocab_size\": len(self.vocab),\n","            \"merge_count\": len(self.merges),\n","            \"special_tokens\": self.special_tokens,\n","            \"approach\": \"space_aware_without_metaspace\",\n","            \"language\": \"nepali\",\n","            \"script\": \"devanagari\"\n","        }\n","\n","        with open(output_path / 'tokenizer_metadata.json', 'w', encoding='utf-8') as f:\n","            json.dump(metadata, f, indent=2, ensure_ascii=False)\n","\n","        logging.info(f'Final vocabulary size: {len(self.vocab):,}')\n","        logging.info(f'Number of merge rules: {len(self.merges):,}')\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        text = self._preprocess_nepali_text(text)\n","\n","        tokens = []\n","        for char in text:\n","            if char in self.vocab:\n","                tokens.append(char)\n","            else:\n","                tokens.append(UNK)\n","\n","        for left, right in self.merges:\n","            new_tokens = []\n","            i = 0\n","            while i < len(tokens):\n","                if i < len(tokens) - 1 and tokens[i] == left and tokens[i + 1] == right:\n","                    merged = left + right\n","                    new_tokens.append(merged)\n","                    i += 2\n","                else:\n","                    new_tokens.append(tokens[i])\n","                    i += 1\n","            tokens = new_tokens\n","\n","        return tokens\n","\n","    def encode(self, text: str) -> List[int]:\n","        tokens = self.tokenize(text)\n","        return [self.vocab.get(token, self.special_tokens[UNK]) for token in tokens]\n","\n","    def decode(self, token_ids: List[int]) -> str:\n","        tokens = [self.id2token.get(token_id, UNK) for token_id in token_ids]\n","        text = ''.join(tokens)\n","        return text\n","\n","def train_bpe(\n","    input_file: str,\n","    output_dir: str = \"./bpe_tokenizer\",\n","    vocab_size: int = 10000,\n","    coverage: float = 0.9999,\n","    logging_step: int = 100\n","):\n","\n","    tokenizer = SimpleBPE(vocab_size=vocab_size, coverage=coverage)\n","\n","    print(\"Training BPE tokenizer...\")\n","    print(f\"Input file: {input_file}\")\n","    print(f\"Output directory: {output_dir}\")\n","    print(f\"Target vocabulary size: {vocab_size:,}\")\n","\n","    start_time = time.time()\n","    tokenizer.fit(input_file, output_dir, logging_step)\n","    training_time = time.time() - start_time\n","\n","    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n","    print(f\"Files saved to: {output_dir}\")\n","\n","    return tokenizer\n","\n","def test_tokenizer(tokenizer, test_sentences=None):\n","    if test_sentences is None:\n","        test_sentences = [\n","            \"नमस्कार, तपाईं कसो हुनुहुन्छ?\",\n","            \"म नेपाली भाषा सिक्दै छु।\",\n","            \"काठमाडौं नेपालको राजधानी हो।\",\n","            \"Hello world\",\n","            \"Mix of English and नेपाली text together\",\n","        ]\n","\n","    print(\"\\nTesting tokenizer:\")\n","    print(\"=\" * 60)\n","\n","    for i, sentence in enumerate(test_sentences, 1):\n","        print(f\"\\n{i}. Original: {sentence}\")\n","\n","        tokens = tokenizer.tokenize(sentence)\n","        print(f\"   Tokens: {tokens}\")\n","        print(f\"   Count: {len(tokens)} tokens\")\n","\n","        token_ids = tokenizer.encode(sentence)\n","        print(f\"   IDs: {token_ids}\")\n","\n","        decoded = tokenizer.decode(token_ids)\n","        print(f\"   Decoded: {decoded}\")\n","\n","        if decoded.strip() == sentence.strip():\n","            print(\"   Perfect reconstruction\")\n","        else:\n","            print(\"   Reconstruction differs\")\n","\n","def main():\n","    \"\"\"main function to train Llama-compatible BPE tokenizer\"\"\"\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/dataset/ne_reduced_train.txt\"\n","    output_dir = \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/tokenizers/bpe\"\n","\n","    vocab_size = 10000\n","    coverage = 0.9999\n","    logging_step = 500\n","\n","    tokenizer = train_bpe(\n","        input_file=input_file,\n","        output_dir=output_dir,\n","        vocab_size=vocab_size,\n","        coverage=coverage,\n","        logging_step=logging_step\n","    )\n","\n","    test_tokenizer(tokenizer)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"depbOk6WoYx-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Picky BPE"],"metadata":{"id":"yJ8yEQeqoc_i"}},{"cell_type":"code","source":["from __future__ import annotations\n","import json\n","import logging\n","import time\n","import argparse\n","import re\n","import unicodedata\n","from transformers import AutoTokenizer\n","from pathlib import Path\n","from collections import defaultdict, Counter\n","from typing import Union, Optional, Dict, List\n","import numpy as np\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    datefmt='%Y-%m-%d %H:%M:%S',\n","    handlers=[\n","        logging.StreamHandler(),\n","    ],\n","    force=True\n",")\n","\n","WHITESPACE = '▁'\n","PAD = '<pad>'\n","UNK = '<unk>'\n","BOS = '<s>'\n","EOS = '</s>'\n","\n","class MCounter(Counter):\n","    \"\"\"extended Counter class with multiplication support\"\"\"\n","    def __mul__(self, other):\n","        if not isinstance(other, int):\n","            raise TypeError(\"Non-int factor\")\n","        return MCounter({k: other * v for k, v in self.items()})\n","\n","    def __rmul__(self, other):\n","        return self * other\n","\n","    def __add__(self, other):\n","        return MCounter(super().__add__(other))\n","\n","class Token:\n","    def __init__(\n","        self,\n","        id: int,\n","        str: str,\n","        freq: int = 0,\n","        special: bool = False,\n","        present: bool = True,\n","        left: Optional[Token] = None,\n","        right: Optional[Token] = None,\n","        split: Optional[list[Token]] = None\n","    ):\n","        self.id = id\n","        self.str = str\n","        self.freq = freq\n","        self.special = special\n","        self.present = present\n","        self.atomic = len(str) == 1 or special\n","        self.words = set()\n","        self.left = left\n","        self.right = right\n","        self.split = split\n","\n","    def __repr__(self):\n","        return f'{self.str} ({self.freq})'\n","\n","    def walk(self) -> list[Token]:\n","        if self.atomic or self.present:\n","            return [self]\n","        return self.left.walk() + self.right.walk()\n","\n","    def remove(self) -> None:\n","        if self.atomic:\n","            raise ValueError(f'Cannot remove an atomic token {self.str}.')\n","        self.present = False\n","        self.freq = 0\n","        self.words = set()\n","\n","    def restore(self) -> None:\n","        if self.present:\n","            raise ValueError(f'Cannot revoke already present token {self.str}.')\n","        self.present = True\n","\n","    def split_if_possible(self) -> Optional[list[Token]]:\n","        if self.atomic:\n","            return None\n","        self.present = False\n","        return self.walk()\n","\n","    def to_dict(self) -> dict:\n","        return {\n","            'id': self.id,\n","            'str': self.str,\n","            'freq': self.freq,\n","            'special': self.special,\n","            'present': self.present,\n","            'left': self.left.id if self.left is not None else None,\n","            'right': self.right.id if self.right is not None else None,\n","            'split': [t.id for t in self.walk()]\n","        }\n","\n","class Word:\n","    def __init__(self, id: int, word: str, freq: int = 0):\n","        self.id = id\n","        self.str = word\n","        self.freq = freq\n","        self.tokens = None\n","        self.pairs = None\n","\n","    def __repr__(self) -> str:\n","        return f'{self.str} ({self.freq})'\n","\n","    def encode(self, str2token: dict[str, Token]) -> None:\n","        self.tokens = [str2token[c] for c in self.str]\n","        self._recalculate()\n","\n","    def _recalculate(self, update_tokens: bool = True) -> None:\n","        self.pairs = MCounter(zip(self.tokens[:-1], self.tokens[1:])) * self.freq\n","        if update_tokens:\n","            for token in self.tokens:\n","                token.words.add(self)\n","\n","    def merge_pair(self, pair: tuple[Token, Token], new_token: Token, update_tokens: bool = True) -> int:\n","        new_tokens = []\n","        i = 0\n","        while i < len(self.tokens):\n","            if i < len(self.tokens) - 1 and (self.tokens[i], self.tokens[i+1]) == pair:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(self.tokens[i])\n","                i += 1\n","        new_token_frequency = len(self.tokens) - len(new_tokens)\n","        if update_tokens:\n","            pair[0].words.discard(self)\n","            pair[1].words.discard(self)\n","        self.tokens = new_tokens\n","        self._recalculate(update_tokens=update_tokens)\n","        return new_token_frequency * self.freq\n","\n","    def split_token(self, token: Token, split: list[Token], update_tokens: bool = True):\n","        new_tokens = []\n","        for t in self.tokens:\n","            if t == token:\n","                new_tokens.extend(split)\n","            else:\n","                new_tokens.append(t)\n","        self.tokens = new_tokens\n","        self._recalculate(update_tokens=update_tokens)\n","\n","class NepaliPickyBPE:\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        pad_id: int = 3,\n","        unk_id: int = 0,\n","        bos_id: int = 1,\n","        eos_id: int = 2,\n","        coverage: float = 0.9999,\n","        threshold: float = 0.9999,\n","    ):\n","        self.desired_vocab_size = vocab_size\n","        self.pad_token = Token(pad_id, PAD, 0, special=True)\n","        self.unk_token = Token(unk_id, UNK, 0, special=True)\n","        self.bos_token = Token(bos_id, BOS, 0, special=True)\n","        self.eos_token = Token(eos_id, EOS, 0, special=True)\n","\n","        self.id2token = {\n","            token.id: token for token in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n","        }\n","        self.str2token = {\n","            token.str: token for token in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n","        }\n","        self.str2token = defaultdict(lambda: self.unk_token, self.str2token)\n","        self.max_special_token_id = max(self.id2token.keys())\n","        self.actual_vocab_size = len(self.id2token)\n","        self.new_id = self.max_special_token_id + 1\n","        self.coverage = coverage\n","        self.threshold = threshold\n","        self.events = list()\n","\n","    def _preprocess_nepali_text(self, text: str) -> str:\n","        \"\"\"preprocess Nepali text preserving Devanagari script\"\"\"\n","        text = unicodedata.normalize('NFKC', text)\n","        text = text.replace(' ', f' {WHITESPACE}')\n","        text = re.sub(r'[^\\u0900-\\u097F\\s\\w\\u0030-\\u0039\\u002E\\u002C\\u003F\\u0021\\u003A\\u003B\\u002D।॥]', ' ', text)\n","        text = re.sub(r'([a-zA-Z]+)', r' \\1 ', text)\n","        text = re.sub(r'([०-९]+)', r' \\1 ', text)\n","        text = re.sub(r'([।॥])', r' \\1 ', text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text.strip()\n","\n","    def _get_words(self, file: str) -> list[Word]:\n","        \"\"\"load and preprocess Nepali corpus from file\"\"\"\n","        logging.info(f'Loading Nepali corpus from {file}...')\n","        start_time = time.time()\n","\n","        counter = MCounter()\n","        with open(file, 'r', encoding='utf-8') as f:\n","            for i, line in enumerate(f):\n","                if not line.strip():\n","                    continue\n","\n","                processed_line = self._preprocess_nepali_text(line.strip())\n","                if not processed_line:\n","                    continue\n","\n","                words = processed_line.split()\n","                words = [WHITESPACE + word if not word.startswith(WHITESPACE) else word for word in words]\n","                counter.update(words)\n","\n","                if i > 0 and i % 50000 == 0:\n","                    logging.info(f'Processed {i} lines.')\n","\n","        num_words = len(counter)\n","        logging.info(f'Loaded {num_words} unique words in {time.time() - start_time:.2f}s.')\n","\n","        return [Word(i, word, freq) for i, (word, freq) in enumerate(counter.items())]\n","\n","    def _get_characters(self, words: list[Word]) -> MCounter:\n","        \"\"\"extract character frequencies from words\"\"\"\n","        counter = MCounter()\n","        for i, word in enumerate(words):\n","            counter.update(MCounter(word.str) * word.freq)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for character extraction.')\n","        return counter\n","\n","    def _filter_characters(self, characters: MCounter) -> MCounter:\n","        \"\"\"filter rare characters based on coverage threshold\"\"\"\n","        if self.coverage < 1:\n","            corpus_size = sum(characters.values())\n","            freq_to_remove = corpus_size - round(self.coverage * corpus_size)\n","            if freq_to_remove > 0:\n","                cum_sum = np.cumsum([freq for _, freq in reversed(characters.most_common())])\n","                num_to_remove = np.searchsorted(cum_sum, freq_to_remove)\n","                characters_to_remove = [c for c, _ in characters.most_common()[-num_to_remove:]]\n","                for c in characters_to_remove:\n","                    characters.pop(c)\n","                logging.info(f'Replaced {num_to_remove} rare characters with UNK.')\n","        return characters\n","\n","    def _initialize_vocab(self, words: list[Word]) -> None:\n","        \"\"\"initialize vocabulary\"\"\"\n","        logging.info('Initializing vocabulary...')\n","        characters = self._get_characters(words)\n","        filtered_characters = self._filter_characters(characters)\n","\n","        for i, character in enumerate(filtered_characters):\n","            token = Token(self.new_id + i, character, filtered_characters[character])\n","            self.id2token[token.id] = token\n","            self.str2token[token.str] = token\n","\n","        self.new_id += len(filtered_characters)\n","        self.actual_vocab_size += len(filtered_characters)\n","\n","        devanagari_chars = sum(1 for char in filtered_characters if '\\u0900' <= char <= '\\u097F')\n","        devanagari_numerals = sum(1 for char in filtered_characters if '\\u0966' <= char <= '\\u096F')\n","        virama_count = sum(1 for char in filtered_characters if char == '\\u094D')\n","\n","        logging.info(f'Initialized vocabulary with {len(filtered_characters)} unique characters.')\n","        logging.info(f'Found {devanagari_chars} Devanagari script characters.')\n","        logging.info(f'Found {devanagari_numerals} Devanagari numerals.')\n","        logging.info(f'Found {virama_count} virama characters.')\n","\n","    @staticmethod\n","    def _validate_pair(pair) -> bool:\n","        \"\"\"check if pair contains only non-special tokens\"\"\"\n","        return not any(token.special for token in pair)\n","\n","    def _encode_words(self, words: list[Word]) -> None:\n","        \"\"\"encode words using current vocabulary\"\"\"\n","        logging.info('Encoding words with Devanagari characters...')\n","        for i, word in enumerate(words):\n","            word.encode(self.str2token)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for encoding.')\n","\n","    def _initialize_pairs(self, words: list[Word]) -> MCounter:\n","        \"\"\"initialize pair frequencies\"\"\"\n","        pairs = MCounter()\n","        logging.info('Counting Devanagari character pairs...')\n","        for i, word in enumerate(words):\n","            pairs.update(word.pairs)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for pair counting.')\n","\n","        to_remove = set()\n","        for pair in pairs:\n","            if not self._validate_pair(pair):\n","                to_remove.add(pair)\n","        for pair in to_remove:\n","            pairs.pop(pair)\n","\n","        return pairs\n","\n","    def _remove_if_possible(self, token: Token, merged_freq: int, pairs: MCounter) -> bool:\n","        \"\"\"remove token if it meets the threshold criteria\"\"\"\n","        if merged_freq / (token.freq + merged_freq) > self.threshold:\n","            split = token.split_if_possible()\n","            if split is not None:\n","                self.actual_vocab_size -= 1\n","                for t in split:\n","                    t.freq += token.freq\n","                for pair in zip(split[:-1], split[1:]):\n","                    pairs[pair] += token.freq\n","\n","                pairs_for_update = MCounter()\n","                for word in token.words:\n","                    if token not in word.tokens:\n","                        raise ValueError(f'Token {token} not found in word {word}.')\n","                    pairs_for_update.update({\n","                        pair: freq for pair, freq in word.pairs.items()\n","                        if self._validate_pair(pair) and token in pair\n","                    })\n","                    word.split_token(token, split)\n","\n","                self._update_pairs_on_remove(token, split, pairs_for_update, pairs)\n","                token.remove()\n","                return True\n","        return False\n","\n","    @staticmethod\n","    def _update_pairs_on_merge(new_token: Token, pair: tuple[Token, Token],\n","                              pairs_for_update: MCounter, pairs: MCounter):\n","        \"\"\"update pair frequencies after merge operation\"\"\"\n","        pairs.update(pairs_for_update)\n","        for p, freq in pairs_for_update.items():\n","            if new_token not in p:\n","                raise ValueError(f'Pair {p} does not contain the new token {new_token}.')\n","            if new_token is p[0]:\n","                if new_token is p[1]:\n","                    to_update = (pair[1], pair[0])\n","                else:\n","                    to_update = (pair[1], p[1])\n","            else:\n","                to_update = (p[0], pair[0])\n","            if to_update in pairs:\n","                pairs[to_update] -= freq\n","                if pairs[to_update] <= 0:\n","                    pairs.pop(to_update)\n","\n","    @staticmethod\n","    def _update_pairs_on_remove(token: Token, split: list[Token],\n","                               pairs_for_update: MCounter, pairs: MCounter):\n","        \"\"\"update pair frequencies after split operation\"\"\"\n","        for pair, freq in pairs_for_update.items():\n","            if token is pair[0]:\n","                if token is pair[1]:\n","                    to_update = (split[-1], split[0])\n","                else:\n","                    to_update = (split[-1], pair[1])\n","            else:\n","                to_update = (pair[0], split[0])\n","            pairs[to_update] += freq\n","            pairs.pop(pair)\n","\n","    def _merge_token_in_words(self, token_to_merge: Token, pair_to_merge: tuple[Token, Token],\n","                             pairs: MCounter) -> int:\n","        \"\"\"merge token in all relevant words\"\"\"\n","        actual_freq = 0\n","        pairs_for_update = MCounter()\n","\n","        for word in pair_to_merge[0].words & pair_to_merge[1].words:\n","            if pair_to_merge in word.pairs:\n","                word.pairs.pop(pair_to_merge)\n","                actual_freq += word.merge_pair(pair_to_merge, token_to_merge)\n","                pairs_for_update.update({\n","                    p: f for p, f in word.pairs.items()\n","                    if self._validate_pair(p) and token_to_merge in p\n","                })\n","\n","        self._update_pairs_on_merge(token_to_merge, pair_to_merge, pairs_for_update, pairs)\n","        token_to_merge.freq += actual_freq\n","\n","        if pair_to_merge[0] is pair_to_merge[1]:\n","            pair_to_merge[0].freq -= 2 * actual_freq\n","            removed = self._remove_if_possible(pair_to_merge[0], actual_freq, pairs)\n","            if removed:\n","                logging.info(f'Removed token {pair_to_merge[0].str} after merging into {token_to_merge.str}.')\n","                self.events.append(('SPLIT', pair_to_merge[0], pair_to_merge[0].walk()))\n","        else:\n","            for token in pair_to_merge:\n","                if not token.present:\n","                    raise ValueError(f'Token {token} is not present in vocabulary.')\n","                token.freq -= actual_freq\n","                removed = self._remove_if_possible(token, actual_freq, pairs)\n","                if removed:\n","                    logging.info(f'Removed token {token.str} after merging into {token_to_merge.str}.')\n","                    self.events.append(('SPLIT', token, token.walk()))\n","\n","        return actual_freq\n","\n","    def _merge_pair(self, pair: tuple[Token, Token], pairs: MCounter) -> int:\n","        \"\"\"merge a token pair\"\"\"\n","        pairs.pop(pair)\n","        merged_str = pair[0].str + pair[1].str\n","\n","        if merged_str in self.str2token:\n","            new_token = self.str2token[merged_str]\n","            if not new_token.present:\n","                new_token.restore()\n","                logging.info(f'Restored previously removed token {new_token.str}.')\n","            else:\n","                logging.info(f'Additional merges for {new_token.str}.')\n","        else:\n","            new_token = Token(self.new_id, merged_str, 0, left=pair[0], right=pair[1])\n","            self.id2token[new_token.id] = new_token\n","            self.str2token[new_token.str] = new_token\n","            self.new_id += 1\n","\n","        self.events.append(('MERGE', pair, new_token))\n","        actual_freq = self._merge_token_in_words(new_token, pair, pairs)\n","        return actual_freq\n","\n","    def fit(self, input_file: str, output_dir: str, logging_step: int = 200) -> None:\n","\n","        output_path = Path(output_dir)\n","        output_path.mkdir(parents=True, exist_ok=True)\n","\n","        logging.info(\"Starting Nepali PickyBPE training...\")\n","        logging.info(f\"Input: {input_file}\")\n","        logging.info(f\"Output: {output_dir}\")\n","        logging.info(f\"Target vocab size: {self.desired_vocab_size:,}\")\n","        logging.info(f\"Coverage: {self.coverage*100:.2f}%\")\n","        logging.info(f\"Threshold: {self.threshold*100:.2f}%\")\n","\n","        words = self._get_words(input_file)\n","        self._initialize_vocab(words)\n","        self._encode_words(words)\n","        pairs = self._initialize_pairs(words)\n","\n","        merge_time = []\n","        logging.info(f'Starting PickyBPE training with {self.actual_vocab_size} initial tokens.')\n","\n","        while self.actual_vocab_size < self.desired_vocab_size:\n","            start_time = time.time()\n","            if not pairs:\n","                logging.info(f'No more pairs to merge. Stopping with vocab size of {self.actual_vocab_size}.')\n","                break\n","\n","            pair, count = pairs.most_common(1)[0]\n","            if count <= 0:\n","                logging.info(f'No more pairs to merge. Stopping with vocab size of {self.actual_vocab_size}.')\n","                break\n","\n","            freq = self._merge_pair(pair, pairs)\n","            self.actual_vocab_size += 1\n","            merge_time.append(time.time() - start_time)\n","\n","            if self.actual_vocab_size % logging_step == 0:\n","                avg_time = np.mean(merge_time) if merge_time else 0\n","                current_speed = 1.0 / avg_time if avg_time > 0 else 0\n","                logging.info(\n","                    f'VOCABULARY SIZE: {self.actual_vocab_size:,}/{self.desired_vocab_size:,}. '\n","                    f'Merged \"{pair[0].str}\" + \"{pair[1].str}\" with frequency {freq:,}. '\n","                    f'Speed: {current_speed:.1f} merges/sec'\n","                )\n","                merge_time = []\n","\n","        self._save_picky_model(output_path / 'picky_bpe_model.json')\n","        self._save_huggingface_files(output_path)\n","\n","        logging.info(f'Training completed. Files saved to {output_path}')\n","\n","    def _save_picky_model(self, file_path: Path) -> None:\n","\n","        logging.info(f'Saving model to {file_path}...')\n","\n","        assigned_ids = sorted(self.id2token.keys())\n","        id_mapping = {}\n","        id_counter = 0\n","\n","        for i in assigned_ids:\n","            if self.id2token[i].present:\n","                id_mapping[i] = id_counter\n","                id_counter += 1\n","\n","        model_data = {\n","            'language': 'nepali',\n","            'script': 'devanagari',\n","            'tokens': [token.to_dict() for token in self.id2token.values()],\n","            'id2int': {str(k): v for k, v in id_mapping.items()},\n","            'int2id': {str(v): k for k, v in id_mapping.items()},\n","            'merges': [\n","                {'id': i, 'pair': [token.to_dict() for token in merge[1]], 'new_token': merge[2].to_dict()}\n","                for i, merge in enumerate(self.events) if merge[0] == 'MERGE'\n","            ],\n","            'splits': [\n","                {'id': i, 'token': merge[1].to_dict(), 'split': [token.to_dict() for token in merge[2]]}\n","                for i, merge in enumerate(self.events) if merge[0] == 'SPLIT'\n","            ],\n","            'training_config': {\n","                'coverage': self.coverage,\n","                'threshold': self.threshold,\n","                'vocab_size': self.desired_vocab_size\n","            }\n","        }\n","\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(model_data, f, indent=2, ensure_ascii=False)\n","\n","    def _save_huggingface_files(self, output_path: Path) -> None:\n","\n","        vocab = {}\n","        present_tokens = []\n","        for token_id in sorted(self.id2token.keys()):\n","            token = self.id2token[token_id]\n","            if token.present:\n","                vocab[token.str] = len(present_tokens)\n","                present_tokens.append(token)\n","\n","        merges = []\n","        for event in self.events:\n","            if event[0] == 'MERGE':\n","                pair = event[1]\n","                left_str = pair[0].str\n","                right_str = pair[1].str\n","                if left_str in vocab and right_str in vocab:\n","                    merges.append(f\"{left_str} {right_str}\")\n","\n","        tokenizer_data = {\n","            \"version\": \"1.0\",\n","            \"truncation\": None,\n","            \"padding\": None,\n","            \"added_tokens\": [],\n","            \"normalizer\": {\n","                \"type\": \"NFKC\"\n","            },\n","            \"pre_tokenizer\": {\n","                \"type\": \"Sequence\",\n","                \"pretokenizers\": [\n","                    {\n","                        \"type\": \"WhitespaceSplit\"\n","                    },\n","                    {\n","                        \"type\": \"Metaspace\",\n","                        \"replacement\": WHITESPACE,\n","                        \"add_prefix_space\": True\n","                    }\n","                ]\n","            },\n","            \"post_processor\": {\n","                \"type\": \"TemplateProcessing\",\n","                \"single\": f\"{BOS}:1 $A:0 {EOS}:1\",\n","                \"pair\": f\"{BOS}:1 $A:0 {EOS}:1 $B:0 {EOS}:1\",\n","                \"special_tokens\": {\n","                    BOS: {\"id\": 1, \"type_id\": 1},\n","                    EOS: {\"id\": 2, \"type_id\": 1}\n","                }\n","            },\n","            \"decoder\": {\n","                \"type\": \"Metaspace\",\n","                \"replacement\": WHITESPACE,\n","                \"add_prefix_space\": True\n","            },\n","            \"model\": {\n","                \"type\": \"BPE\",\n","                \"dropout\": None,\n","                \"unk_token\": UNK,\n","                \"continuing_subword_prefix\": None,\n","                \"end_of_word_suffix\": None,\n","                \"fuse_unk\": False,\n","                \"vocab\": vocab,\n","                \"merges\": merges\n","            }\n","        }\n","\n","        with open(output_path / 'tokenizer.json', 'w', encoding='utf-8') as f:\n","            json.dump(tokenizer_data, f, indent=2, ensure_ascii=False)\n","\n","        config_data = {\n","            \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","            \"auto_map\": {\n","                \"AutoTokenizer\": [\"tokenizer.json\", None]\n","            },\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD,\n","            \"model_max_length\": 2048,\n","            \"padding_side\": \"left\",\n","            \"truncation_side\": \"right\",\n","            \"chat_template\": None,\n","            \"clean_up_tokenization_spaces\": True,\n","            \"spaces_between_special_tokens\": False,\n","            \"language\": \"nepali\",\n","            \"script\": \"devanagari\"\n","        }\n","\n","        with open(output_path / 'tokenizer_config.json', 'w', encoding='utf-8') as f:\n","            json.dump(config_data, f, indent=2, ensure_ascii=False)\n","\n","        special_tokens_data = {\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD\n","        }\n","\n","        with open(output_path / 'special_tokens_map.json', 'w', encoding='utf-8') as f:\n","            json.dump(special_tokens_data, f, indent=2, ensure_ascii=False)\n","\n","        with open(output_path / 'added_tokens.json', 'w', encoding='utf-8') as f:\n","            json.dump([], f, indent=2, ensure_ascii=False)\n","\n","        with open(output_path / 'vocab.json', 'w', encoding='utf-8') as f:\n","            json.dump(vocab, f, indent=2, ensure_ascii=False)\n","\n","        logging.info(f'Final vocabulary size: {len(vocab):,}')\n","        logging.info(f'Number of merge rules: {len(merges):,}')\n","        logging.info(f'Number of split events: {sum(1 for event in self.events if event[0] == \"SPLIT\")}')\n","\n","def train_nepali_picky_bpe(\n","    input_file: str,\n","    output_dir: str = \"./nepali_picky_bpe\",\n","    vocab_size: int = 10000,\n","    coverage: float = 0.9999,\n","    threshold: float = 0.9999,\n","    logging_step: int = 200\n","):\n","    \"\"\"train a PickyBPE tokenizer for Nepali language\"\"\"\n","    tokenizer = NepaliPickyBPE(\n","        vocab_size=vocab_size,\n","        coverage=coverage,\n","        threshold=threshold\n","    )\n","\n","    print(\"Training PickyBPE tokenizer for Nepali...\")\n","    print(f\"Input file: {input_file}\")\n","    print(f\"Output directory: {output_dir}\")\n","    print(f\"Target vocabulary size: {vocab_size:,}\")\n","    print(f\"Special tokens: {UNK} (0), {BOS} (1), {EOS} (2), {PAD} (3)\")\n","    print(f\"Character coverage: {coverage*100:.2f}%\")\n","    print(f\"Removal threshold: {threshold*100:.2f}%\")\n","    print(f\"Algorithm: PickyBPE with dynamic token management\")\n","    print(f\"Script: Devanagari (Unicode U+0900-U+097F)\")\n","    print(f\"Normalization: NFKC (optimized for conjunct consonants)\")\n","\n","    start_time = time.time()\n","    tokenizer.fit(input_file, output_dir, logging_step)\n","    training_time = time.time() - start_time\n","\n","    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n","    print(f\"Files saved to: {output_dir}\")\n","\n","    return tokenizer\n","\n","def test_nepali_picky_bpe(tokenizer_path: str):\n","\n","    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n","\n","    test_sentences = [\n","        \"नमस्कार, तपाईं कस्तो हुनुहुन्छ?\",\n","        \"म नेपाली भाषा सिक्दै छु।\",\n","        \"काठमाडौं नेपालको राजधानी हो।\",\n","        \"किताब टेबुलमा छ।\",\n","        \"आज मौसम धेरै राम्रो छ।\",\n","        \"हिमालपर्वत संसारकै अग्लो पर्वत हो।\",\n","    ]\n","\n","    print(\"\\nTesting tokenizer:\")\n","    print(\"=\" * 80)\n","\n","    for i, sentence in enumerate(test_sentences, 1):\n","        print(f\"\\n{i}. Original: {sentence}\")\n","\n","        tokens = tokenizer.tokenize(sentence)\n","        print(f\"   Tokens: {tokens}\")\n","        print(f\"   Count: {len(tokens)} tokens\")\n","\n","        token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n","        print(f\"   IDs: {token_ids}\")\n","\n","        decoded = tokenizer.decode(token_ids)\n","        print(f\"   Decoded: {decoded}\")\n","\n","        if decoded.strip() == sentence.strip():\n","            print(\"   Perfect reconstruction\")\n","        else:\n","            print(\"   Reconstruction differs\")\n","\n","def main():\n","    \"\"\"main function to train Nepali PickyBPE tokenizer\"\"\"\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/dataset/ne_reduced_train.txt\"\n","    output_dir = \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/tokenizers/picky_bpe\"\n","\n","    vocab_size = 10000\n","    coverage = 0.9999\n","    threshold = 0.9999\n","    logging_step = 500\n","\n","    if not Path(input_file).exists():\n","        print(f\"Error: Input file not found: {input_file}\")\n","        return\n","\n","    tokenizer = train_nepali_picky_bpe(\n","        input_file=input_file,\n","        output_dir=output_dir,\n","        vocab_size=vocab_size,\n","        coverage=coverage,\n","        threshold=threshold,\n","        logging_step=logging_step\n","    )\n","\n","    test_nepali_picky_bpe(output_dir)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"cam2MkmGodao"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Grapheme Picky BPE"],"metadata":{"id":"hSQs-LQvodmE"}},{"cell_type":"code","source":["from __future__ import annotations\n","import json\n","import logging\n","import time\n","import argparse\n","import re\n","import sys\n","import os\n","import unicodedata\n","from pathlib import Path\n","from collections import defaultdict, Counter\n","from typing import Union, Optional, Dict, List\n","import numpy as np\n","import grapheme\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    datefmt='%Y-%m-%d %H:%M:%S',\n","    handlers=[\n","        logging.StreamHandler(),\n","    ],\n","    force=True\n",")\n","logger = logging.getLogger(__name__)\n","\n","WHITESPACE = '▁'\n","PAD = '<pad>'\n","UNK = '<unk>'\n","BOS = '<s>'\n","EOS = '</s>'\n","\n","class MCounter(Counter):\n","    \"\"\"extended Counter class with multiplication support\"\"\"\n","    def __mul__(self, other):\n","        if not isinstance(other, int):\n","            raise TypeError(\"Non-int factor\")\n","        return MCounter({k: other * v for k, v in self.items()})\n","\n","    def __rmul__(self, other):\n","        return self * other\n","\n","    def __add__(self, other):\n","        return MCounter(super().__add__(other))\n","\n","class Token:\n","    def __init__(\n","        self,\n","        id: int,\n","        str: str,\n","        freq: int = 0,\n","        special: bool = False,\n","        present: bool = True,\n","        left: Optional[Token] = None,\n","        right: Optional[Token] = None,\n","        split: Optional[list[Token]] = None\n","    ):\n","        self.id = id\n","        self.str = str\n","        self.freq = freq\n","        self.special = special\n","        self.present = present\n","        self.atomic = len(str) == 1 or special\n","        self.words = set()\n","        self.left = left\n","        self.right = right\n","        self.split = split\n","\n","    def __repr__(self):\n","        return f'{self.str} ({self.freq})'\n","\n","    def walk(self) -> list[Token]:\n","        if self.atomic or self.present:\n","            return [self]\n","        result = []\n","        if self.left is not None:\n","            result.extend(self.left.walk())\n","        if self.right is not None:\n","            result.extend(self.right.walk())\n","        if not result:\n","            return [self]\n","        return result\n","\n","    def remove(self) -> None:\n","        if self.atomic:\n","            raise ValueError(f'Cannot remove an atomic token {self.str}.')\n","        self.present = False\n","        self.freq = 0\n","        self.words = set()\n","\n","    def restore(self) -> None:\n","        if self.present:\n","            raise ValueError(f'Cannot revoke already present token {self.str}.')\n","        self.present = True\n","\n","    def split_if_possible(self) -> Optional[list[Token]]:\n","        if self.atomic:\n","            return None\n","        self.present = False\n","        return self.walk()\n","\n","    def to_dict(self) -> dict:\n","        return {\n","            'id': self.id,\n","            'str': self.str,\n","            'freq': self.freq,\n","            'special': self.special,\n","            'present': self.present,\n","            'left': self.left.id if self.left is not None else None,\n","            'right': self.right.id if self.right is not None else None,\n","            'split': [t.id for t in self.walk()]\n","        }\n","\n","class Word:\n","    def __init__(self, id: int, word: str, freq: int = 0):\n","        self.id = id\n","        self.str = word\n","        self.freq = freq\n","        self.tokens = None\n","        self.pairs = None\n","\n","    def __repr__(self) -> str:\n","        return f'{self.str} ({self.freq})'\n","\n","    def encode(self, str2token: dict[str, Token]) -> None:\n","        \"\"\"encode word using graphemes\"\"\"\n","        graphemes_list = list(grapheme.graphemes(self.str))\n","        self.tokens = [str2token[g] for g in graphemes_list]\n","        self._recalculate()\n","\n","    def _recalculate(self, update_tokens: bool = True) -> None:\n","        self.pairs = MCounter(zip(self.tokens[:-1], self.tokens[1:])) * self.freq\n","        if update_tokens:\n","            for token in self.tokens:\n","                token.words.add(self)\n","\n","    def merge_pair(self, pair: tuple[Token, Token], new_token: Token, update_tokens: bool = True) -> int:\n","        new_tokens = []\n","        i = 0\n","        while i < len(self.tokens):\n","            if i < len(self.tokens) - 1 and (self.tokens[i], self.tokens[i+1]) == pair:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(self.tokens[i])\n","                i += 1\n","        new_token_frequency = len(self.tokens) - len(new_tokens)\n","        if update_tokens:\n","            pair[0].words.discard(self)\n","            pair[1].words.discard(self)\n","        self.tokens = new_tokens\n","        self._recalculate(update_tokens=update_tokens)\n","        return new_token_frequency * self.freq\n","\n","    def split_token(self, token: Token, split: list[Token], update_tokens: bool = True):\n","        new_tokens = []\n","        for t in self.tokens:\n","            if t == token:\n","                new_tokens.extend(split)\n","            else:\n","                new_tokens.append(t)\n","        self.tokens = new_tokens\n","        self._recalculate(update_tokens=update_tokens)\n","\n","class NepaliGraphemePickyBPE:\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        pad_id: int = 3,\n","        unk_id: int = 0,\n","        bos_id: int = 1,\n","        eos_id: int = 2,\n","        coverage: float = 0.9995,\n","        threshold: float = 0.999,\n","    ):\n","        self.desired_vocab_size = vocab_size\n","        self.pad_token = Token(pad_id, PAD, 0, special=True)\n","        self.unk_token = Token(unk_id, UNK, 0, special=True)\n","        self.bos_token = Token(bos_id, BOS, 0, special=True)\n","        self.eos_token = Token(eos_id, EOS, 0, special=True)\n","\n","        self.id2token = {\n","            token.id: token for token in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n","        }\n","        self.str2token = {\n","            token.str: token for token in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n","        }\n","        self.str2token = defaultdict(lambda: self.unk_token, self.str2token)\n","        self.max_special_token_id = max(self.id2token.keys())\n","        self.actual_vocab_size = len(self.id2token)\n","        self.new_id = self.max_special_token_id + 1\n","        self.coverage = coverage\n","        self.threshold = threshold\n","        self.events = list()\n","        self.grapheme_vocab = set()\n","\n","    def _preprocess_nepali_text(self, text: str) -> str:\n","        text = unicodedata.normalize('NFC', text)\n","        text = text.replace(' ', f' {WHITESPACE}')\n","        text = re.sub(r'[^\\u0900-\\u097F\\s\\u0964\\u0965।॥,.\\-!?;:]', ' ', text)\n","        text = re.sub(r'([a-zA-Z]+)', r' \\1 ', text)\n","        text = re.sub(r'([०-९]+)', r' \\1 ', text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text.strip()\n","\n","    def _get_words(self, file: str) -> list[Word]:\n","        logging.info(f'Loading Nepali corpus from {file}...')\n","        start_time = time.time()\n","\n","        counter = MCounter()\n","        with open(file, 'r', encoding='utf-8') as f:\n","            for i, line in enumerate(f):\n","                if not line.strip():\n","                    continue\n","\n","                processed_line = self._preprocess_nepali_text(line.strip())\n","                if not processed_line:\n","                    continue\n","\n","                words = processed_line.split()\n","                words = [WHITESPACE + word if not word.startswith(WHITESPACE) else word for word in words]\n","                counter.update(words)\n","\n","                if i > 0 and i % 50000 == 0:\n","                    logging.info(f'Processed {i} lines.')\n","\n","        num_words = len(counter)\n","        logging.info(f'Loaded {num_words} unique words in {time.time() - start_time:.2f}s.')\n","\n","        return [Word(i, word, freq) for i, (word, freq) in enumerate(counter.items())]\n","\n","    def _extract_graphemes(self, words: list[Word]) -> MCounter:\n","        \"\"\"extract graphemes from words\"\"\"\n","        logging.info('Extracting unique graphemes from corpus...')\n","        start_time = time.time()\n","\n","        grapheme_counter = MCounter()\n","\n","        for i, word in enumerate(words):\n","            graphemes_in_word = list(grapheme.graphemes(word.str))\n","\n","            for g in graphemes_in_word:\n","                if g not in self.grapheme_vocab:\n","                    self.grapheme_vocab.add(g)\n","\n","            for g in graphemes_in_word:\n","                grapheme_counter[g] += word.freq\n","\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for grapheme extraction.')\n","\n","        processing_time = time.time() - start_time\n","        logging.info(f'Extracted {len(grapheme_counter)} unique graphemes in {processing_time:.2f}s.')\n","\n","        return grapheme_counter\n","\n","    def _filter_graphemes(self, graphemes: MCounter) -> MCounter:\n","        \"\"\"filter rare graphemes based on coverage threshold\"\"\"\n","        if self.coverage < 1:\n","            corpus_size = sum(graphemes.values())\n","            freq_to_remove = corpus_size - round(self.coverage * corpus_size)\n","            if freq_to_remove > 0:\n","                cum_sum = np.cumsum([freq for _, freq in reversed(graphemes.most_common())])\n","                num_to_remove = np.searchsorted(cum_sum, freq_to_remove)\n","                graphemes_to_remove = [g for g, _ in graphemes.most_common()[-num_to_remove:]]\n","                for g in graphemes_to_remove:\n","                    graphemes.pop(g)\n","                    self.grapheme_vocab.discard(g)\n","                logging.info(f'Replaced {num_to_remove} rare graphemes with UNK.')\n","        return graphemes\n","\n","    def _initialize_vocab(self, words: list[Word]) -> None:\n","        \"\"\"initialize vocabulary\"\"\"\n","        logging.info('Initializing vocabulary...')\n","\n","        graphemes = self._extract_graphemes(words)\n","        filtered_graphemes = self._filter_graphemes(graphemes)\n","\n","        for i, grapheme_str in enumerate(filtered_graphemes):\n","            token = Token(self.new_id + i, grapheme_str, filtered_graphemes[grapheme_str])\n","            self.id2token[token.id] = token\n","            self.str2token[token.str] = token\n","\n","        self.new_id += len(filtered_graphemes)\n","        self.actual_vocab_size += len(filtered_graphemes)\n","\n","        devanagari_graphemes = sum(1 for g in filtered_graphemes if any('\\u0900' <= char <= '\\u097F' for char in g))\n","        devanagari_numerals = sum(1 for g in filtered_graphemes if any('\\u0966' <= char <= '\\u096F' for char in g))\n","        traditional_punct = sum(1 for g in filtered_graphemes if any(char in '।॥' for char in g))\n","\n","        logging.info(f'Initialized vocabulary with {len(filtered_graphemes)} unique graphemes.')\n","        logging.info(f'Found {devanagari_graphemes} Devanagari script graphemes.')\n","        logging.info(f'Found {devanagari_numerals} Devanagari numeral graphemes.')\n","        logging.info(f'Found {traditional_punct} traditional punctuation graphemes.')\n","\n","    @staticmethod\n","    def _validate_pair(pair) -> bool:\n","        \"\"\"check if pair contains only non-special tokens\"\"\"\n","        return not any(token.special for token in pair)\n","\n","    def _encode_words(self, words: list[Word]) -> None:\n","        \"\"\"encode words using grapheme-aware encoding\"\"\"\n","        logging.info('Encoding words with Devanagari graphemes...')\n","\n","        for i, word in enumerate(words):\n","            word.encode(self.str2token)\n","\n","            if i < 5:\n","                tokens_str = [token.str for token in word.tokens]\n","                logging.info(f'Word \"{word.str}\" -> tokens: {tokens_str}')\n","                if word.pairs:\n","                    pair_strs = [(f\"{p[0].str}+{p[1].str}\", freq) for p, freq in word.pairs.items()]\n","                    logging.info(f'  Pairs: {pair_strs[:5]}')\n","\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for grapheme encoding.')\n","\n","        logging.info(f'Encoding complete.')\n","\n","    def _initialize_pairs(self, words: list[Word]) -> MCounter:\n","        \"\"\"initialize pair frequencies from grapheme-encoded words\"\"\"\n","        pairs = MCounter()\n","        logging.info('Counting Devanagari grapheme pairs...')\n","\n","        for i, word in enumerate(words):\n","            pairs.update(word.pairs)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for pair counting.')\n","\n","        to_remove = set()\n","        for pair in pairs:\n","            if not self._validate_pair(pair):\n","                to_remove.add(pair)\n","\n","        for pair in to_remove:\n","            pairs.pop(pair)\n","\n","        logging.info(f'Final pair count: {len(pairs)} unique pairs, {sum(pairs.values())} total instances.')\n","\n","        if pairs:\n","            top_pairs = pairs.most_common(10)\n","            logging.info(f'Top 10 pairs: {[(f\"{p[0].str}+{p[1].str}\", freq) for p, freq in top_pairs]}')\n","\n","        return pairs\n","\n","    def _remove_if_possible(self, token: Token, merged_freq: int, pairs: MCounter) -> bool:\n","        \"\"\"remove token if it meets the threshold criteria\"\"\"\n","        if token.freq + merged_freq == 0:\n","            return False\n","\n","        if merged_freq / (token.freq + merged_freq) > self.threshold:\n","            split = token.split_if_possible()\n","            if split is not None:\n","                self.actual_vocab_size -= 1\n","                for t in split:\n","                    t.freq += token.freq\n","                for pair in zip(split[:-1], split[1:]):\n","                    pairs[pair] += token.freq\n","\n","                pairs_for_update = MCounter()\n","                for word in token.words:\n","                    if token not in word.tokens:\n","                        raise ValueError(f'Token {token} not found in word {word}.')\n","                    pairs_for_update.update({\n","                        pair: freq for pair, freq in word.pairs.items()\n","                        if self._validate_pair(pair) and token in pair\n","                    })\n","                    word.split_token(token, split)\n","\n","                self._update_pairs_on_remove(token, split, pairs_for_update, pairs)\n","                token.remove()\n","                return True\n","        return False\n","\n","    @staticmethod\n","    def _update_pairs_on_merge(new_token: Token, pair: tuple[Token, Token],\n","                              pairs_for_update: MCounter, pairs: MCounter):\n","        \"\"\"update pair frequencies after merge operation\"\"\"\n","        pairs.update(pairs_for_update)\n","        for p, freq in pairs_for_update.items():\n","            if new_token not in p:\n","                raise ValueError(f'Pair {p} does not contain the new token {new_token}.')\n","            if new_token is p[0]:\n","                if new_token is p[1]:\n","                    to_update = (pair[1], pair[0])\n","                else:\n","                    to_update = (pair[1], p[1])\n","            else:\n","                to_update = (p[0], pair[0])\n","            if to_update in pairs:\n","                pairs[to_update] -= freq\n","                if pairs[to_update] <= 0:\n","                    pairs.pop(to_update)\n","\n","    @staticmethod\n","    def _update_pairs_on_remove(token: Token, split: list[Token],\n","                               pairs_for_update: MCounter, pairs: MCounter):\n","        \"\"\"update pair frequencies after split operation\"\"\"\n","        for pair, freq in pairs_for_update.items():\n","            if token is pair[0]:\n","                if token is pair[1]:\n","                    to_update = (split[-1], split[0])\n","                else:\n","                    to_update = (split[-1], pair[1])\n","            else:\n","                to_update = (pair[0], split[0])\n","            pairs[to_update] += freq\n","            pairs.pop(pair)\n","\n","    def _merge_token_in_words(self, token_to_merge: Token, pair_to_merge: tuple[Token, Token],\n","                             pairs: MCounter) -> int:\n","        \"\"\"merge token in all relevant words\"\"\"\n","        actual_freq = 0\n","        pairs_for_update = MCounter()\n","\n","        for word in pair_to_merge[0].words & pair_to_merge[1].words:\n","            if pair_to_merge in word.pairs:\n","                word.pairs.pop(pair_to_merge)\n","                actual_freq += word.merge_pair(pair_to_merge, token_to_merge)\n","                pairs_for_update.update({\n","                    p: f for p, f in word.pairs.items()\n","                    if self._validate_pair(p) and token_to_merge in p\n","                })\n","\n","        self._update_pairs_on_merge(token_to_merge, pair_to_merge, pairs_for_update, pairs)\n","        token_to_merge.freq += actual_freq\n","\n","        if pair_to_merge[0] is pair_to_merge[1]:\n","            pair_to_merge[0].freq -= 2 * actual_freq\n","            removed = self._remove_if_possible(pair_to_merge[0], actual_freq, pairs)\n","            if removed:\n","                logging.info(f'Removed token {pair_to_merge[0].str} after merging into {token_to_merge.str}.')\n","                self.events.append(('SPLIT', pair_to_merge[0], pair_to_merge[0].walk()))\n","        else:\n","            for token in pair_to_merge:\n","                if not token.present:\n","                    raise ValueError(f'Token {token} is not present in vocabulary.')\n","                token.freq -= actual_freq\n","                removed = self._remove_if_possible(token, actual_freq, pairs)\n","                if removed:\n","                    logging.info(f'Removed token {token.str} after merging into {token_to_merge.str}.')\n","                    self.events.append(('SPLIT', token, token.walk()))\n","\n","        return actual_freq\n","\n","    def _merge_pair(self, pair: tuple[Token, Token], pairs: MCounter) -> int:\n","        \"\"\"merge a token pair\"\"\"\n","        pairs.pop(pair)\n","        merged_str = pair[0].str + pair[1].str\n","\n","        if merged_str in self.str2token:\n","            new_token = self.str2token[merged_str]\n","            if not new_token.present:\n","                new_token.restore()\n","                logging.info(f'Restored previously removed token {new_token.str}.')\n","            else:\n","                logging.info(f'Additional merges for {new_token.str}.')\n","        else:\n","            new_token = Token(self.new_id, merged_str, 0, left=pair[0], right=pair[1])\n","            self.id2token[new_token.id] = new_token\n","            self.str2token[new_token.str] = new_token\n","            self.new_id += 1\n","\n","        self.events.append(('MERGE', pair, new_token))\n","        actual_freq = self._merge_token_in_words(new_token, pair, pairs)\n","        return actual_freq\n","\n","    def fit(self, input_file: str, output_dir: str, logging_step: int = 200) -> None:\n","        \"\"\"train tokenizer\"\"\"\n","        output_path = Path(output_dir)\n","        output_path.mkdir(parents=True, exist_ok=True)\n","\n","        logging.info(\"Starting training...\")\n","        logging.info(f\"Input: {input_file}\")\n","        logging.info(f\"Output: {output_dir}\")\n","        logging.info(f\"Target vocab size: {self.desired_vocab_size:,}\")\n","        logging.info(f\"Coverage: {self.coverage*100:.2f}%\")\n","        logging.info(f\"Threshold: {self.threshold*100:.2f}%\")\n","\n","        words = self._get_words(input_file)\n","        self._initialize_vocab(words)\n","        self._encode_words(words)\n","        pairs = self._initialize_pairs(words)\n","\n","        merge_time = []\n","        logging.info(f'Starting training with {self.actual_vocab_size} initial tokens.')\n","\n","        while self.actual_vocab_size < self.desired_vocab_size:\n","            start_time = time.time()\n","            if not pairs:\n","                logging.info(f'No more pairs to merge. Stopping with vocab size of {self.actual_vocab_size}.')\n","                break\n","\n","            pair, count = pairs.most_common(1)[0]\n","            if count <= 0:\n","                logging.info(f'No more pairs to merge. Stopping with vocab size of {self.actual_vocab_size}.')\n","                break\n","\n","            freq = self._merge_pair(pair, pairs)\n","            self.actual_vocab_size += 1\n","            merge_time.append(time.time() - start_time)\n","\n","            if self.actual_vocab_size % logging_step == 0:\n","                avg_time = np.mean(merge_time) if merge_time else 0\n","                current_speed = 1.0 / avg_time if avg_time > 0 else 0\n","                logging.info(\n","                    f'VOCABULARY SIZE: {self.actual_vocab_size:,}/{self.desired_vocab_size:,}. '\n","                    f'Merged \"{pair[0].str}\" + \"{pair[1].str}\" with frequency {freq:,}. '\n","                    f'Speed: {current_speed:.1f} merges/sec'\n","                )\n","                merge_time = []\n","\n","        self._save_picky_model(output_path / 'grapheme_picky_bpe_model.json')\n","        self._save_huggingface_files(output_path)\n","\n","        logging.info(f'Training completed. Files saved to {output_path}')\n","\n","    def _save_picky_model(self, file_path: Path) -> None:\n","        logging.info(f'Saving model to {file_path}...')\n","\n","        assigned_ids = sorted(self.id2token.keys())\n","        id_mapping = {}\n","        id_counter = 0\n","\n","        for i in assigned_ids:\n","            if self.id2token[i].present:\n","                id_mapping[i] = id_counter\n","                id_counter += 1\n","\n","        model_data = {\n","            'language': 'nepali',\n","            'script': 'devanagari',\n","            'algorithm': 'Grapheme-Aware PickyBPE',\n","            'tokens': [token.to_dict() for token in self.id2token.values()],\n","            'id2int': {str(k): v for k, v in id_mapping.items()},\n","            'int2id': {str(v): k for k, v in id_mapping.items()},\n","            'merges': [\n","                {'id': i, 'pair': [token.to_dict() for token in merge[1]], 'new_token': merge[2].to_dict()}\n","                for i, merge in enumerate(self.events) if merge[0] == 'MERGE'\n","            ],\n","            'splits': [\n","                {'id': i, 'token': merge[1].to_dict(), 'split': [token.to_dict() for token in merge[2]]}\n","                for i, merge in enumerate(self.events) if merge[0] == 'SPLIT'\n","            ],\n","            'training_config': {\n","                'coverage': self.coverage,\n","                'threshold': self.threshold,\n","                'vocab_size': self.desired_vocab_size\n","            },\n","            'grapheme_vocab': list(self.grapheme_vocab)\n","        }\n","\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(model_data, f, indent=2, ensure_ascii=False)\n","\n","    def _save_huggingface_files(self, output_path: Path) -> None:\n","\n","        vocab = {}\n","        present_tokens = []\n","        for token_id in sorted(self.id2token.keys()):\n","            token = self.id2token[token_id]\n","            if token.present:\n","                vocab[token.str] = len(present_tokens)\n","                present_tokens.append(token)\n","\n","        merges = []\n","        for event in self.events:\n","            if event[0] == 'MERGE':\n","                pair = event[1]\n","                left_str = pair[0].str\n","                right_str = pair[1].str\n","                if left_str in vocab and right_str in vocab:\n","                    merges.append(f\"{left_str} {right_str}\")\n","\n","        tokenizer_data = {\n","            \"version\": \"1.0\",\n","            \"truncation\": None,\n","            \"padding\": None,\n","            \"added_tokens\": [],\n","            \"normalizer\": {\n","                \"type\": \"NFC\"\n","            },\n","            \"pre_tokenizer\": {\n","                \"type\": \"Sequence\",\n","                \"pretokenizers\": [\n","                    {\n","                        \"type\": \"WhitespaceSplit\"\n","                    },\n","                    {\n","                        \"type\": \"Metaspace\",\n","                        \"replacement\": WHITESPACE,\n","                        \"add_prefix_space\": True\n","                    }\n","                ]\n","            },\n","            \"post_processor\": {\n","                \"type\": \"TemplateProcessing\",\n","                \"single\": f\"{BOS}:1 $A:0 {EOS}:1\",\n","                \"pair\": f\"{BOS}:1 $A:0 {EOS}:1 $B:0 {EOS}:1\",\n","                \"special_tokens\": {\n","                    BOS: {\"id\": 1, \"type_id\": 1},\n","                    EOS: {\"id\": 2, \"type_id\": 1}\n","                }\n","            },\n","            \"decoder\": {\n","                \"type\": \"Metaspace\",\n","                \"replacement\": WHITESPACE,\n","                \"add_prefix_space\": True\n","            },\n","            \"model\": {\n","                \"type\": \"BPE\",\n","                \"dropout\": None,\n","                \"unk_token\": UNK,\n","                \"continuing_subword_prefix\": None,\n","                \"end_of_word_suffix\": None,\n","                \"fuse_unk\": False,\n","                \"vocab\": vocab,\n","                \"merges\": merges\n","            }\n","        }\n","\n","        with open(output_path / 'tokenizer.json', 'w', encoding='utf-8') as f:\n","            json.dump(tokenizer_data, f, indent=2, ensure_ascii=False)\n","\n","        config_data = {\n","            \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","            \"auto_map\": {\n","                \"AutoTokenizer\": [\"tokenizer.json\", None]\n","            },\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD,\n","            \"model_max_length\": 2048,\n","            \"padding_side\": \"left\",\n","            \"truncation_side\": \"right\",\n","            \"chat_template\": None,\n","            \"clean_up_tokenization_spaces\": True,\n","            \"spaces_between_special_tokens\": False,\n","            \"language\": \"nepali\",\n","            \"script\": \"devanagari\",\n","            \"algorithm\": \"Grapheme-Aware PickyBPE\"\n","        }\n","\n","        with open(output_path / 'tokenizer_config.json', 'w', encoding='utf-8') as f:\n","            json.dump(config_data, f, indent=2, ensure_ascii=False)\n","\n","        special_tokens_data = {\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD\n","        }\n","\n","        with open(output_path / 'special_tokens_map.json', 'w', encoding='utf-8') as f:\n","            json.dump(special_tokens_data, f, indent=2, ensure_ascii=False)\n","\n","        with open(output_path / 'added_tokens.json', 'w', encoding='utf-8') as f:\n","            json.dump([], f, indent=2, ensure_ascii=False)\n","\n","        with open(output_path / 'vocab.json', 'w', encoding='utf-8') as f:\n","            json.dump(vocab, f, indent=2, ensure_ascii=False)\n","\n","        logging.info(f'Final vocabulary size: {len(vocab):,}')\n","        logging.info(f'Number of merge rules: {len(merges):,}')\n","        logging.info(f'Number of split events: {sum(1 for event in self.events if event[0] == \"SPLIT\")}')\n","        logging.info(f'Number of unique graphemes processed: {len(self.grapheme_vocab):,}')\n","\n","def train_nepali_grapheme_picky_bpe(\n","    input_file: str,\n","    output_dir: str = \"./nepali_grapheme_picky_bpe\",\n","    vocab_size: int = 10000,\n","    coverage: float = 0.9995,\n","    threshold: float = 0.999,\n","    logging_step: int = 200\n","):\n","    tokenizer = NepaliGraphemePickyBPE(\n","        vocab_size=vocab_size,\n","        coverage=coverage,\n","        threshold=threshold\n","    )\n","\n","    print(f\"Input file: {input_file}\")\n","    print(f\"Output directory: {output_dir}\")\n","    print(f\"Target vocabulary size: {vocab_size:,}\")\n","\n","    start_time = time.time()\n","    tokenizer.fit(input_file, output_dir, logging_step)\n","    training_time = time.time() - start_time\n","\n","    print(f\"Files saved to: {output_dir}\")\n","\n","    return tokenizer\n","\n","def main():\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/dataset/ne_reduced_train.txt\"\n","    output_dir = \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/tokenizers/grapheme_picky_bpe\"\n","\n","    vocab_size = 10000\n","    coverage = 0.9995\n","    threshold = 0.999\n","    logging_step = 200\n","\n","    tokenizer = train_nepali_grapheme_picky_bpe(\n","        input_file=input_file,\n","        output_dir=output_dir,\n","        vocab_size=vocab_size,\n","        coverage=coverage,\n","        threshold=threshold,\n","        logging_step=logging_step\n","    )\n","\n","    print(\"\\nTraining complete\")\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"5K0MlKKsodyU"},"execution_count":null,"outputs":[]}]}