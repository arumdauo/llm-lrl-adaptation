{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNcvOuQg3Eu15D9IuMmru6r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ml3AOwuDyPgm"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"f4mlTmT8yWDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import numpy as np\n","from pathlib import Path\n","from collections import defaultdict, Counter\n","from typing import List, Dict, Tuple\n","import tempfile\n","import time\n","from scipy.special import expit\n","import logging\n","import sys\n","import shutil\n","import matplotlib.pyplot as plt\n","import sentencepiece as spm\n","import gensim.models\n","import random\n","import unicodedata\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    datefmt='%Y-%m-%d %H:%M:%S',\n","    handlers=[\n","        logging.StreamHandler(sys.stdout),\n","    ],\n","    force=True\n",")\n","logger = logging.getLogger(__name__)\n","\n","class SaGeTokenizer:\n","\n","    def __init__(self,\n","                 vocab_size: int = 10000,\n","                 initial_vocab_multiplier: float = 2,\n","                 max_token_length: int = 20,\n","                 embedding_dim: int = 50,\n","                 window_size: int = 5,\n","                 negative_samples: int = 10,\n","                 min_token_freq: int = 12,\n","                 pruning_batch_size: int = 3500,\n","                 embedding_update_frequency: int = 5,\n","                 gensim_workers: int = 4,\n","                 gensim_epochs: int = 5,\n","                 gensim_min_count: int = 1,\n","                 fertility_target: float = 1.4,\n","                 fertility_tolerance: float = 0.08,\n","                 patience: int = 3,\n","                 min_improvement: float = 0.01):\n","\n","        self.vocab_size = vocab_size\n","        self.initial_vocab_size = int(vocab_size * initial_vocab_multiplier)\n","        self.max_token_length = max_token_length\n","        self.embedding_dim = embedding_dim\n","        self.window_size = window_size\n","        self.negative_samples = negative_samples\n","        self.min_token_freq = min_token_freq\n","        self.pruning_batch_size = pruning_batch_size\n","        self.embedding_update_frequency = embedding_update_frequency\n","\n","        self.gensim_workers = gensim_workers\n","        self.gensim_epochs = gensim_epochs\n","        self.gensim_min_count = gensim_min_count\n","\n","        self.fertility_target = fertility_target\n","        self.fertility_tolerance = fertility_tolerance\n","        self.patience = patience\n","        self.min_improvement = min_improvement\n","\n","        self.vocabulary = {}\n","        self.inv_vocabulary = {}\n","        self.token_frequencies = Counter()\n","        self.embeddings = None\n","        self.context_embeddings = None\n","\n","        self.training_history = {\n","            'iteration': [],\n","            'vocab_size': [],\n","            'fertility': [],\n","            'skip_gram_loss': [],\n","            'ablation_loss': [],\n","            'tokens_per_char': [],\n","            'coverage': []\n","        }\n","        self.validation_lines = []\n","        self.total_lines = 0\n","\n","        self.devanagari_range = range(0x0900, 0x097F + 1)\n","\n","    def is_devanagari_char(self, char: str) -> bool:\n","        return ord(char) in self.devanagari_range\n","\n","    def normalize_nepali_text(self, text: str) -> str:\n","        return unicodedata.normalize('NFKC', text)\n","\n","    def preprocess_nepali_line(self, line: str) -> str:\n","        line = line.strip()\n","        if not line:\n","            return \"\"\n","\n","        line = self.normalize_nepali_text(line)\n","        line = '▁' + line.replace(' ', '▁')\n","\n","        return line\n","\n","    def count_corpus_lines(self, corpus_file: str) -> int:\n","        total = 0\n","        with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            for _ in f:\n","                total += 1\n","        return total\n","\n","    def load_validation_set(self, corpus_file: str, num_lines: int = 1000) -> List[str]:\n","        logger.info(f\"Loading validation lines...\")\n","\n","        validation_start = max(int(self.total_lines * 0.9), self.total_lines - num_lines)\n","\n","        lines = []\n","        with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            for i, line in enumerate(f):\n","                if i >= validation_start:\n","                    line = self.preprocess_nepali_line(line)\n","                    if line:\n","                        lines.append(line)\n","\n","        logger.info(f\"Loaded {len(lines)} validation lines (from line {validation_start})\")\n","        return lines\n","\n","    def tokenize_with_vocabulary(self, text: str, vocab: Dict[str, int]) -> List[int]:\n","        \"\"\"tokenize using longest match\"\"\"\n","        text = self.preprocess_nepali_line(text)\n","        tokens = []\n","        i = 0\n","\n","        while i < len(text):\n","            matched = False\n","            for length in range(min(self.max_token_length, len(text) - i), 0, -1):\n","                substr = text[i:i+length]\n","                if substr in vocab:\n","                    tokens.append(vocab[substr])\n","                    i += length\n","                    matched = True\n","                    break\n","\n","            if not matched:\n","                char = text[i]\n","                tokens.append(vocab.get(char, vocab.get('<unk>', 0)))\n","                i += 1\n","\n","        return tokens\n","\n","    def compute_fertility(self, vocab: Dict[str, int], validation_lines: List[str]) -> Dict[str, float]:\n","\n","        total_words = 0\n","        total_tokens = 0\n","        total_chars = 0\n","        covered_chars = 0\n","        devanagari_chars = 0\n","\n","        for line in validation_lines[:100]:\n","            original_line = line.replace('▁', ' ').strip()\n","            words = original_line.split()\n","            if not words:\n","                continue\n","            total_words += len(words)\n","\n","            total_chars += len(line)\n","\n","            for char in line:\n","                if self.is_devanagari_char(char):\n","                    devanagari_chars += 1\n","\n","            tokens = self.tokenize_with_vocabulary(original_line, vocab)\n","            total_tokens += len(tokens)\n","\n","            for token_id in tokens:\n","                token = self.inv_vocabulary.get(token_id, '<unk>')\n","                if token != '<unk>':\n","                    covered_chars += len(token)\n","\n","        metrics = {\n","            'fertility': total_tokens / max(total_words, 1),\n","            'tokens_per_char': total_tokens / max(total_chars, 1),\n","            'coverage': min(covered_chars / max(total_chars, 1), 1.0),\n","            'avg_tokens_per_line': total_tokens / max(min(len(validation_lines), 100), 1),\n","            'devanagari_ratio': devanagari_chars / max(total_chars, 1)\n","        }\n","\n","        return metrics\n","\n","    def compute_skip_gram_loss(self, token_ids: List[int], target_emb: np.ndarray, context_emb: np.ndarray) -> float:\n","        \"\"\"compute Skip-gram loss\"\"\"\n","        if len(token_ids) < 2:\n","            return 0.0\n","\n","        loss = 0.0\n","        count = 0\n","\n","        for i, target_id in enumerate(token_ids):\n","            if target_id >= len(target_emb):\n","                continue\n","\n","            context_start = max(0, i - self.window_size)\n","            context_end = min(len(token_ids), i + self.window_size + 1)\n","\n","            for j in range(context_start, context_end):\n","                if i == j:\n","                    continue\n","\n","                context_id = token_ids[j]\n","                if context_id >= len(context_emb):\n","                    continue\n","\n","                score = np.dot(target_emb[target_id], context_emb[context_id])\n","                loss -= np.log(expit(score) + 1e-10)\n","                count += 1\n","\n","                for _ in range(self.negative_samples):\n","                    neg_id = np.random.randint(0, len(context_emb))\n","                    score = np.dot(target_emb[target_id], context_emb[neg_id])\n","                    loss -= np.log(1 - expit(score) + 1e-10)\n","                    count += 1\n","\n","        return loss / max(count, 1)\n","\n","    def initialize_vocabulary(self, corpus_file: str) -> Dict[str, int]:\n","        \"\"\"initialize vocabulary\"\"\"\n","        logger.info(f\"Initializing vocabulary with target size {self.initial_vocab_size}\")\n","\n","        ngram_counts = Counter()\n","        max_lines = int(self.total_lines * 0.9)\n","\n","        with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            for line_num, line in enumerate(f):\n","                if line_num >= max_lines:\n","                    break\n","                if line_num % 10000 == 0 and line_num > 0:\n","                    logger.info(f\"Processing line {line_num}...\")\n","\n","                line = self.preprocess_nepali_line(line)\n","                if not line:\n","                    continue\n","\n","                for n in range(1, min(len(line) + 1, self.max_token_length + 1)):\n","                    for i in range(len(line) - n + 1):\n","                        ngram = line[i:i+n]\n","\n","                        weight = 1\n","                        if any(self.is_devanagari_char(c) for c in ngram):\n","                            weight = 2\n","\n","                        ngram_counts[ngram] += weight\n","\n","        filtered_ngrams = {\n","            ngram: count for ngram, count in ngram_counts.items()\n","            if count >= self.min_token_freq\n","        }\n","\n","        logger.info(f\"Found {len(filtered_ngrams)} n-grams with freq >= {self.min_token_freq}\")\n","\n","        essential_tokens = ['<unk>', '<s>', '</s>', '<pad>', '▁']\n","\n","        for i in range(256):\n","            essential_tokens.append(chr(i))\n","\n","        essential_tokens.extend(['।', '॥', '॰'])\n","\n","        vocabulary = {}\n","        token_id = 0\n","\n","        for token in essential_tokens:\n","            vocabulary[token] = token_id\n","            self.token_frequencies[token] = ngram_counts.get(token, 1)\n","            token_id += 1\n","\n","        sorted_ngrams = sorted(filtered_ngrams.items(), key=lambda x: x[1], reverse=True)\n","\n","        for ngram, freq in sorted_ngrams:\n","            if ngram not in vocabulary:\n","                vocabulary[ngram] = token_id\n","                self.token_frequencies[ngram] = freq\n","                token_id += 1\n","\n","                if len(vocabulary) >= self.initial_vocab_size:\n","                    break\n","\n","        self.vocabulary = vocabulary\n","        self.inv_vocabulary = {v: k for k, v in vocabulary.items()}\n","\n","        logger.info(f\"Initialized vocabulary with {len(vocabulary)} tokens\")\n","\n","        devanagari_tokens = sum(1 for token in vocabulary.keys()\n","                              if any(self.is_devanagari_char(c) for c in token))\n","        logger.info(f\"Vocabulary contains {devanagari_tokens} tokens with Devanagari characters\")\n","\n","        return vocabulary\n","\n","    def train_embeddings_with_gensim(self, corpus_file: str, vocab: Dict[str, int]) -> Tuple[np.ndarray, np.ndarray, float]:\n","        logger.info(f\"Training embeddings with vocabulary of size {len(vocab)}\")\n","\n","        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False,\n","                                                suffix='.txt', encoding='utf-8')\n","\n","        max_training_lines = int(self.total_lines * 0.9)\n","        tokenized_lines = []\n","\n","        with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            for line_num, line in enumerate(f):\n","                if line_num >= max_training_lines:\n","                    break\n","                if line_num % 10000 == 0 and line_num > 0:\n","                    logger.info(f\"Tokenizing line {line_num} for Gensim...\")\n","\n","                original_line = line.strip()\n","                if not original_line:\n","                    continue\n","\n","                token_ids = self.tokenize_with_vocabulary(original_line, vocab)\n","                token_strings = [self.inv_vocabulary.get(tid, '<unk>') for tid in token_ids]\n","\n","                if token_strings:\n","                    temp_file.write(' '.join(token_strings) + '\\n')\n","\n","                    if len(tokenized_lines) < 1000:\n","                        tokenized_lines.append(token_ids)\n","\n","        temp_file.close()\n","\n","        logger.info(\"Training Word2Vec model...\")\n","        start_time = time.time()\n","\n","        model = gensim.models.Word2Vec(\n","            corpus_file=temp_file.name,\n","            vector_size=self.embedding_dim,\n","            window=self.window_size,\n","            min_count=self.gensim_min_count,\n","            workers=self.gensim_workers,\n","            sg=1,\n","            negative=self.negative_samples,\n","            alpha=0.025,\n","            min_alpha=0.0001,\n","            epochs=self.gensim_epochs,\n","            seed=42\n","        )\n","\n","        logger.info(f\"Word2Vec training completed in {time.time() - start_time:.2f}s\")\n","\n","        vocab_size = len(vocab)\n","        target_embeddings = np.random.uniform(-0.5/self.embedding_dim, 0.5/self.embedding_dim,\n","                                             (vocab_size, self.embedding_dim))\n","        context_embeddings = np.random.uniform(-0.5/self.embedding_dim, 0.5/self.embedding_dim,\n","                                              (vocab_size, self.embedding_dim))\n","\n","        found_embeddings = 0\n","        for token, token_id in vocab.items():\n","            if token in model.wv:\n","                target_embeddings[token_id] = model.wv[token]\n","                if hasattr(model.wv, 'syn1neg') and model.wv.syn1neg is not None:\n","                    word_index = model.wv.key_to_index[token]\n","                    context_embeddings[token_id] = model.wv.syn1neg[word_index]\n","                else:\n","                    context_embeddings[token_id] = model.wv[token]\n","                found_embeddings += 1\n","\n","        logger.info(f\"Found embeddings for {found_embeddings}/{vocab_size} tokens\")\n","\n","        total_loss = 0.0\n","        for token_ids in tokenized_lines[:100]:\n","            loss = self.compute_skip_gram_loss(token_ids, target_embeddings, context_embeddings)\n","            total_loss += loss\n","\n","        avg_loss = total_loss / max(len(tokenized_lines[:100]), 1)\n","\n","        os.unlink(temp_file.name)\n","\n","        return target_embeddings, context_embeddings, avg_loss\n","\n","    def compute_ablation_scores(self, corpus_file: str, vocab: Dict[str, int],\n","                                target_emb: np.ndarray, context_emb: np.ndarray,\n","                                sample_size: int = 20000) -> Tuple[Dict[str, float], float]:\n","        logger.info(f\"Computing ablation scores (sample size: {sample_size})...\")\n","\n","        token_contexts = defaultdict(list)\n","        total_loss = 0.0\n","        total_pairs = 0\n","\n","        max_lines = min(sample_size, int(self.total_lines * 0.9))\n","\n","        with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            lines_processed = 0\n","\n","            for line in f:\n","                if lines_processed >= max_lines:\n","                    break\n","\n","                original_line = line.strip()\n","                if not original_line:\n","                    continue\n","\n","                token_ids = self.tokenize_with_vocabulary(original_line, vocab)\n","\n","                for i in range(len(token_ids)):\n","                    target_id = token_ids[i]\n","                    if target_id >= len(self.inv_vocabulary):\n","                        continue\n","\n","                    target_token = self.inv_vocabulary[target_id]\n","\n","                    for j in range(max(0, i - self.window_size),\n","                                  min(len(token_ids), i + self.window_size + 1)):\n","                        if i != j:\n","                            context_id = token_ids[j]\n","                            if context_id >= len(context_emb):\n","                                continue\n","\n","                            token_contexts[target_token].append((target_id, context_id))\n","\n","                            score = np.dot(target_emb[target_id], context_emb[context_id])\n","                            total_loss -= np.log(expit(score) + 1e-10)\n","                            total_pairs += 1\n","\n","                lines_processed += 1\n","\n","                if lines_processed % 5000 == 0:\n","                    logger.info(f\"  Processed {lines_processed}/{max_lines} lines for ablation scores\")\n","\n","        avg_loss = total_loss / max(total_pairs, 1)\n","\n","        ablation_scores = {}\n","\n","        for token, token_id in vocab.items():\n","            if (token in ['<unk>', '<s>', '</s>', '<pad>', '▁', '।', '॥', '॰']\n","                or len(token) == 1):\n","                ablation_scores[token] = float('-inf')\n","                continue\n","\n","            devanagari_ratio = sum(1 for c in token if self.is_devanagari_char(c)) / len(token)\n","            if devanagari_ratio > 0.8 and len(token) > 3:\n","                protection_factor = 0.5\n","            else:\n","                protection_factor = 1.0\n","\n","            contexts = token_contexts.get(token, [])\n","\n","            if not contexts:\n","                ablation_scores[token] = float('inf')\n","                continue\n","\n","            likelihood_with = 0.0\n","            for target_id, context_id in contexts:\n","                score = np.dot(target_emb[target_id], context_emb[context_id])\n","                likelihood_with += np.log(expit(score) + 1e-10)\n","\n","            likelihood_without = 0.0\n","            sample_contexts = random.sample(contexts, min(100, len(contexts)))\n","            for target_id, context_id in sample_contexts:\n","                similar_score = np.mean([\n","                    np.dot(target_emb[tid], context_emb[context_id])\n","                    for tid in range(min(5, len(target_emb)))\n","                    if tid != target_id\n","                ])\n","                likelihood_without += np.log(expit(similar_score) + 1e-10)\n","\n","            if sample_contexts:\n","                likelihood_without *= len(contexts) / len(sample_contexts)\n","\n","            base_score = likelihood_without - likelihood_with\n","            ablation_scores[token] = base_score * protection_factor\n","\n","        logger.info(f\"Computed ablation scores. Average loss: {avg_loss:.4f}\")\n","        return ablation_scores, avg_loss\n","\n","    def prune_vocabulary(self, vocab: Dict[str, int], ablation_scores: Dict[str, float],\n","                        num_to_remove: int) -> Dict[str, int]:\n","\n","        scored_tokens = [(score, token) for token, score in ablation_scores.items()\n","                        if score != float('-inf')]\n","        scored_tokens.sort(reverse=True)\n","\n","        tokens_to_remove = set()\n","        protected_skipped = 0\n","\n","        for score, token in scored_tokens:\n","            if len(tokens_to_remove) >= num_to_remove:\n","                break\n","\n","            is_devanagari_token = any(self.is_devanagari_char(c) for c in token)\n","\n","            if is_devanagari_token and len(token) > 2:\n","                devanagari_ratio = sum(1 for c in token if self.is_devanagari_char(c)) / len(token)\n","\n","                if devanagari_ratio > 0.8 and score < 1.0 and protected_skipped < num_to_remove * 0.2:\n","                    protected_skipped += 1\n","                    logger.debug(f\"Protected Devanagari token: {token} (score: {score:.4f})\")\n","                    continue\n","\n","            tokens_to_remove.add(token)\n","\n","        if len(tokens_to_remove) < num_to_remove * 0.8:\n","            logger.warning(f\"Only selected {len(tokens_to_remove)} tokens for removal, need {num_to_remove}\")\n","            logger.info(\"Reducing protection to meet pruning targets\")\n","\n","            additional_needed = num_to_remove - len(tokens_to_remove)\n","            for score, token in scored_tokens:\n","                if token in tokens_to_remove:\n","                    continue\n","                if len(tokens_to_remove) >= num_to_remove:\n","                    break\n","\n","                is_devanagari_token = any(self.is_devanagari_char(c) for c in token)\n","                if not (is_devanagari_token and len(token) > 4 and score < 0.5):\n","                    tokens_to_remove.add(token)\n","\n","        new_vocab = {}\n","        new_id = 0\n","\n","        for token in sorted(vocab.keys()):\n","            if token not in tokens_to_remove:\n","                new_vocab[token] = new_id\n","                new_id += 1\n","\n","        logger.info(f\"Pruned vocabulary from {len(vocab)} to {len(new_vocab)} tokens\")\n","        logger.info(f\"Actually removed {len(vocab) - len(new_vocab)} tokens (target: {num_to_remove})\")\n","\n","        devanagari_removed = sum(1 for token in tokens_to_remove\n","                               if any(self.is_devanagari_char(c) for c in token))\n","        total_removed = len(tokens_to_remove)\n","        logger.info(f\"Removed {devanagari_removed}/{total_removed} tokens containing Devanagari characters\")\n","\n","        return new_vocab\n","\n","    def should_stop_early(self) -> Tuple[bool, str]:\n","\n","        if len(self.training_history['iteration']) < 2:\n","            return False, \"\"\n","\n","        current_fertility = self.training_history['fertility'][-1]\n","        current_vocab_size = self.training_history['vocab_size'][-1]\n","\n","        if current_vocab_size <= self.vocab_size * 1.05:\n","            return True, f\"Close to target vocab size: {current_vocab_size} (target: {self.vocab_size})\"\n","\n","        if (abs(current_fertility - self.fertility_target) <= self.fertility_tolerance and\n","            current_vocab_size <= self.vocab_size * 1.3):\n","            return True, f\"Reached target fertility: {current_fertility:.3f} and reasonable vocab size: {current_vocab_size}\"\n","\n","        if len(self.training_history['fertility']) >= self.patience:\n","            recent_fertilities = self.training_history['fertility'][-self.patience:]\n","\n","            if all(recent_fertilities[i] > recent_fertilities[i-1] + 0.02 for i in range(1, len(recent_fertilities))):\n","                if current_fertility > self.fertility_target + 0.6:\n","                    return True, f\"Fertility increasing too much: {current_fertility:.3f}\"\n","\n","        if len(self.training_history['vocab_size']) >= 5:\n","            recent_vocab_sizes = self.training_history['vocab_size'][-5:]\n","            if (max(recent_vocab_sizes) - min(recent_vocab_sizes) < 100 and\n","                current_vocab_size > self.vocab_size * 1.5):\n","                return True, f\"Vocabulary size stagnant at {current_vocab_size} (target: {self.vocab_size})\"\n","\n","        return False, \"\"\n","\n","    def train(self, corpus_file: str, output_dir: str) -> str:\n","        logger.info(\"=\" * 70)\n","        logger.info(\"Starting SaGe tokenizer training for Nepali with Gensim embeddings\")\n","        logger.info(f\"Target vocabulary size: {self.vocab_size}\")\n","        logger.info(f\"Target fertility: {self.fertility_target} +/- {self.fertility_tolerance}\")\n","        logger.info(\"=\" * 70)\n","\n","        self.total_lines = self.count_corpus_lines(corpus_file)\n","        logger.info(f\"Corpus has {self.total_lines} lines\")\n","\n","        self.validation_lines = self.load_validation_set(corpus_file)\n","\n","        current_vocab = self.initialize_vocabulary(corpus_file)\n","\n","        initial_metrics = self.compute_fertility(current_vocab, self.validation_lines)\n","        logger.info(f\"Initial metrics: Fertility={initial_metrics['fertility']:.3f}, \"\n","                   f\"Coverage={initial_metrics['coverage']:.3f}, \"\n","                   f\"Devanagari ratio={initial_metrics['devanagari_ratio']:.3f}\")\n","\n","        iteration = 0\n","        embeddings_trained = False\n","        best_fertility = float('inf')\n","        best_vocab = current_vocab.copy()\n","\n","        while len(current_vocab) > self.vocab_size:\n","            iteration += 1\n","            logger.info(f\"\\nIteration {iteration}\")\n","            logger.info(f\"Current vocabulary size: {len(current_vocab)}\")\n","\n","            if not embeddings_trained or iteration % self.embedding_update_frequency == 0:\n","                self.embeddings, self.context_embeddings, skip_gram_loss = \\\n","                    self.train_embeddings_with_gensim(corpus_file, current_vocab)\n","                embeddings_trained = True\n","            else:\n","                skip_gram_loss = 0.0\n","                num_val_lines = min(100, len(self.validation_lines))\n","                for line in self.validation_lines[:num_val_lines]:\n","                    original_line = line.replace('▁', ' ').strip()\n","                    token_ids = self.tokenize_with_vocabulary(original_line, current_vocab)\n","                    skip_gram_loss += self.compute_skip_gram_loss(\n","                        token_ids, self.embeddings, self.context_embeddings\n","                    )\n","                skip_gram_loss /= max(num_val_lines, 1)\n","\n","            ablation_scores, ablation_loss = self.compute_ablation_scores(\n","                corpus_file, current_vocab,\n","                self.embeddings, self.context_embeddings,\n","                sample_size=min(20000, self.total_lines)\n","            )\n","\n","            metrics = self.compute_fertility(current_vocab, self.validation_lines)\n","\n","            self.training_history['iteration'].append(iteration)\n","            self.training_history['vocab_size'].append(len(current_vocab))\n","            self.training_history['fertility'].append(metrics['fertility'])\n","            self.training_history['skip_gram_loss'].append(skip_gram_loss)\n","            self.training_history['ablation_loss'].append(ablation_loss)\n","            self.training_history['tokens_per_char'].append(metrics['tokens_per_char'])\n","            self.training_history['coverage'].append(metrics['coverage'])\n","\n","            logger.info(f\"Metrics: Fertility={metrics['fertility']:.3f}, \"\n","                       f\"Coverage={metrics['coverage']:.3f}, \"\n","                       f\"Devanagari ratio={metrics['devanagari_ratio']:.3f}, \"\n","                       f\"Skip-gram Loss={skip_gram_loss:.4f}\")\n","\n","            if abs(metrics['fertility'] - self.fertility_target) < \\\n","               abs(best_fertility - self.fertility_target):\n","                best_fertility = metrics['fertility']\n","                best_vocab = current_vocab.copy()\n","                logger.info(f\"New best fertility for Nepali: {best_fertility:.3f}\")\n","\n","            should_stop, reason = self.should_stop_early()\n","            if should_stop:\n","                logger.info(f\"\\nEarly stopping: {reason}\")\n","                current_vocab = best_vocab\n","                break\n","\n","            if metrics['fertility'] > self.fertility_target + 0.3:\n","                prune_multiplier = 1.5\n","            elif metrics['fertility'] < self.fertility_target - 0.2:\n","                prune_multiplier = 0.5\n","            else:\n","                prune_multiplier = 1.0\n","\n","            tokens_to_remove = min(\n","                int(self.pruning_batch_size * prune_multiplier),\n","                len(current_vocab) - self.vocab_size\n","            )\n","\n","            if len(current_vocab) > self.vocab_size and tokens_to_remove == 0:\n","                tokens_to_remove = 1\n","\n","            current_vocab = self.prune_vocabulary(\n","                current_vocab, ablation_scores, tokens_to_remove\n","            )\n","\n","            self.vocabulary = current_vocab\n","            self.inv_vocabulary = {v: k for k, v in current_vocab.items()}\n","\n","            logger.info(f\"Pruned {tokens_to_remove} tokens. New size: {len(current_vocab)}\")\n","\n","        final_metrics = self.compute_fertility(current_vocab, self.validation_lines)\n","        logger.info(f\"\\nTraining completed\")\n","        logger.info(f\"Final vocabulary size: {len(current_vocab)}\")\n","        logger.info(f\"Final fertility: {final_metrics['fertility']:.3f}\")\n","        logger.info(f\"Final coverage: {final_metrics['coverage']:.3f}\")\n","        logger.info(f\"Final Devanagari ratio: {final_metrics['devanagari_ratio']:.3f}\")\n","\n","        final_devanagari_tokens = sum(1 for token in current_vocab.keys()\n","                                    if any(self.is_devanagari_char(c) for c in token))\n","        logger.info(f\"Final vocabulary contains {final_devanagari_tokens} Devanagari tokens\")\n","\n","        os.makedirs(output_dir, exist_ok=True)\n","        history_file = os.path.join(output_dir, 'nepali_training_history.json')\n","        with open(history_file, 'w') as f:\n","            json.dump(self.training_history, f, indent=2)\n","\n","        model_file = self.convert_to_sentencepiece(current_vocab, corpus_file, output_dir)\n","\n","        return model_file\n","\n","    def convert_to_sentencepiece(self, final_vocab: Dict[str, int],\n","                                corpus_file: str, output_dir: str) -> str:\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        vocab_file = os.path.join(output_dir, \"nepali_vocab.txt\")\n","        total_freq = sum(self.token_frequencies.values())\n","\n","        with open(vocab_file, 'w', encoding='utf-8') as f:\n","            for token in sorted(final_vocab.keys(),\n","                              key=lambda x: self.token_frequencies.get(x, 0),\n","                              reverse=True):\n","                freq = self.token_frequencies.get(token, 1)\n","                score = np.log(freq / max(total_freq, 1))\n","                f.write(f\"{token}\\t{score}\\n\")\n","\n","        try:\n","            with tempfile.TemporaryDirectory(prefix='sage_nepali_', dir='/tmp') as temp_dir:\n","                temp_corpus = os.path.join(temp_dir, 'nepali_corpus.txt')\n","\n","                with open(corpus_file, 'r', encoding='utf-8') as src, \\\n","                     open(temp_corpus, 'w', encoding='utf-8') as dst:\n","                    for line_num, line in enumerate(src):\n","                        if line_num % 10000 == 0 and line_num > 0:\n","                            logger.info(f\"Preprocessing line {line_num} for SentencePiece...\")\n","\n","                        processed_line = self.normalize_nepali_text(line.strip())\n","                        if processed_line:\n","                            dst.write(processed_line + '\\n')\n","\n","                temp_model_prefix = os.path.join(temp_dir, 'nepali_tokenizer')\n","\n","                spm.SentencePieceTrainer.train(\n","                    input=temp_corpus,\n","                    model_prefix=temp_model_prefix,\n","                    vocab_size=len(final_vocab),\n","                    model_type='unigram',\n","                    character_coverage=0.9999,\n","                    normalization_rule_name='nfkc',\n","                    add_dummy_prefix=False,\n","                    unk_id=0,\n","                    bos_id=1,\n","                    eos_id=2,\n","                    pad_id=3,\n","                    input_sentence_size=min(100000, self.total_lines),\n","                    shuffle_input_sentence=True,\n","                    num_threads=16,\n","                    required_chars='।॥॰',\n","                    byte_fallback=True,\n","                    split_digits=True\n","                )\n","\n","                for ext in ['.model', '.vocab']:\n","                    src = f\"{temp_model_prefix}{ext}\"\n","                    if os.path.exists(src):\n","                        dst = os.path.join(output_dir, f\"nepali_tokenizer{ext}\")\n","                        shutil.copy(src, dst)\n","        except Exception as e:\n","            logger.error(f\"Error converting Nepali tokenizer to SentencePiece: {e}\")\n","            return None\n","\n","        self.create_huggingface_configs(output_dir, len(final_vocab))\n","\n","        model_file = os.path.join(output_dir, \"nepali_tokenizer.model\")\n","        if os.path.exists(model_file):\n","            logger.info(f\"Created Nepali SentencePiece model: {model_file}\")\n","        else:\n","            logger.error(\"Failed to create Nepali SentencePiece model\")\n","\n","        return model_file\n","\n","    def create_huggingface_configs(self, output_dir: str, vocab_size: int):\n","\n","        configs = {\n","            \"tokenizer_config.json\": {\n","                \"tokenizer_class\": \"sage\",\n","                \"model_max_length\": 4096,\n","                \"padding_side\": \"left\",\n","                \"bos_token\": \"<s>\",\n","                \"eos_token\": \"</s>\",\n","                \"unk_token\": \"<unk>\",\n","                \"pad_token\": \"<pad>\",\n","                \"add_bos_token\": True,\n","                \"add_eos_token\": False,\n","                \"clean_up_tokenization_spaces\": False,\n","                \"legacy\": False,\n","                \"sp_model_kwargs\": {\n","                    \"normalization_rule_name\": \"nfkc\"\n","                }\n","            },\n","            \"special_tokens_map.json\": {\n","                \"bos_token\": \"<s>\",\n","                \"eos_token\": \"</s>\",\n","                \"unk_token\": \"<unk>\",\n","                \"pad_token\": \"<pad>\"\n","            }\n","        }\n","\n","        for filename, config in configs.items():\n","            with open(os.path.join(output_dir, filename), 'w') as f:\n","                json.dump(config, f, indent=2)\n","\n","        logger.info(\"Created HuggingFace configuration files\")\n","\n","\n","def run__sage(corpus_file: str, output_dir: str):\n","    logger.info(\"Starting SaGe Tokenizer\")\n","\n","    tokenizer = SaGeTokenizer(\n","        vocab_size=10000,\n","        initial_vocab_multiplier=2,\n","        max_token_length=20,\n","        embedding_dim=50,\n","        window_size=5,\n","        negative_samples=10,\n","        min_token_freq=12,\n","        pruning_batch_size=3500,\n","        embedding_update_frequency=5,\n","        gensim_workers=4,\n","        gensim_epochs=5,\n","        gensim_min_count=1,\n","        fertility_target=1.4,\n","        fertility_tolerance=0.08,\n","        patience=3,\n","        min_improvement=0.01\n","    )\n","\n","    model_file = tokenizer.train(corpus_file, output_dir)\n","\n","    if model_file and os.path.exists(model_file):\n","        sp = spm.SentencePieceProcessor()\n","        sp.load(model_file)\n","\n","        logger.info(f\"\\nFinal Statistics for Nepali:\")\n","        logger.info(f\"   Vocabulary size: {sp.vocab_size()}\")\n","\n","        if tokenizer.training_history['iteration']:\n","            logger.info(f\"   Training iterations: {len(tokenizer.training_history['iteration'])}\")\n","            logger.info(f\"   Final fertility: {tokenizer.training_history['fertility'][-1]:.3f}\")\n","            logger.info(f\"   Final coverage: {tokenizer.training_history['coverage'][-1]:.3f}\")\n","\n","        test_sentences = [\n","            \"नमस्कार संसार\",\n","            \"नेपाली भाषा धेरै राम्रो छ\",\n","            \"नेपालको काठमाडौं उपत्यका\",\n","            \"हिमालयको सुन्दरता अतुलनीय छ\",\n","            \"धन्यवाद र नमस्ते\"\n","        ]\n","\n","        logger.info(\"\\nNepali Tokenization Examples:\")\n","        for sentence in test_sentences:\n","            tokens = sp.encode_as_pieces(sentence)\n","            logger.info(f\"   '{sentence}' -> {tokens[:10]}...\" if len(tokens) > 10 else f\"   '{sentence}' -> {tokens}\")\n","\n","        devanagari_count = 0\n","        for i in range(sp.vocab_size()):\n","            piece = sp.id_to_piece(i)\n","            if any(ord(c) in range(0x0900, 0x097F + 1) for c in piece):\n","                devanagari_count += 1\n","\n","        logger.info(f\"\\nFinal vocabulary contains {devanagari_count} tokens with Devanagari characters\")\n","\n","    return model_file\n","\n","\n","if __name__ == \"__main__\":\n","    corpus_file = \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/dataset/ne_reduced_train.txt\"\n","    output_dir = \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/tokenizers/sage\"\n","\n","    model = run_sage(corpus_file, output_dir)\n","\n","    if model:\n","        logger.info(f\"\\nModel saved to: {output_dir}\")"],"metadata":{"id":"8p3K6UOF0hs-"},"execution_count":null,"outputs":[]}]}