{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMIimavpMUFOA+1RRVfJGGm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qFLgdXQk3vrU"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install -q sentencepiece transformers datasets huggingface_hub"],"metadata":{"id":"aaTK8Ugr4Ouq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import os\n","import sys\n","from pathlib import Path\n","from typing import List, Dict, Tuple\n","import pandas as pd\n","import numpy as np\n","import sentencepiece as spm\n","from transformers import AutoTokenizer\n","\n","class TokenizerEvaluator:\n","    def __init__(self):\n","        self.test_sentences = [\n","            \"नेपालमा राजनीतिकस्थिरता नहुँदा विकास रोकिन्छ।\",\n","            \"नमस्कार, तपाईं कसो हुनुहुन्छ?\",\n","            \"काठमाडौं नेपालको राजधानी हो।\"\n","        ]\n","\n","        self.results = []\n","\n","    def count_words(self, sentence: str) -> int:\n","        import re\n","        cleaned = re.sub(r'[।॥।,\\?\\!\\.]+', '', sentence)\n","        words = cleaned.strip().split()\n","        return len([w for w in words if w.strip()])\n","\n","    def load_simple_bpe(self, model_path: str):\n","        try:\n","            model_file = os.path.join(model_path, 'simple_bpe_model.json')\n","            if not os.path.exists(model_file):\n","                return None\n","\n","            with open(model_file, 'r', encoding='utf-8') as f:\n","                model_data = json.load(f)\n","\n","            class SimpleBPETokenizer:\n","                def __init__(self, vocab, merges):\n","                    self.vocab = vocab\n","                    self.merges = merges\n","                    self.inv_vocab = {v: k for k, v in vocab.items()}\n","\n","                def tokenize_word(self, word):\n","                    word = '▁' + word\n","                    tokens = list(word)\n","\n","                    for merge_rule in self.merges:\n","                        left = merge_rule['left']\n","                        right = merge_rule['right']\n","\n","                        new_tokens = []\n","                        i = 0\n","                        while i < len(tokens):\n","                            if i < len(tokens) - 1 and tokens[i] == left and tokens[i + 1] == right:\n","                                merged = left + right\n","                                new_tokens.append(merged)\n","                                i += 2\n","                            else:\n","                                new_tokens.append(tokens[i])\n","                                i += 1\n","                        tokens = new_tokens\n","\n","                    return tokens\n","\n","                def tokenize(self, text):\n","                    words = text.split()\n","                    all_tokens = []\n","                    for word in words:\n","                        word_tokens = self.tokenize_word(word)\n","                        all_tokens.extend(word_tokens)\n","                    return all_tokens\n","\n","            vocab = model_data['vocab']\n","            merges = model_data['merges']\n","\n","            return SimpleBPETokenizer(vocab, merges)\n","\n","        except Exception as e:\n","            print(f\"Error loading SimpleBPE: {e}\")\n","            return None\n","\n","    def load_picky_bpe(self, model_path: str):\n","        try:\n","            tokenizer_file = os.path.join(model_path, 'tokenizer.json')\n","            if os.path.exists(tokenizer_file):\n","                added_tokens_file = os.path.join(model_path, 'added_tokens.json')\n","                if os.path.exists(added_tokens_file):\n","                    with open(added_tokens_file, 'r', encoding='utf-8') as f:\n","                        added_tokens = json.load(f)\n","\n","                    if isinstance(added_tokens, list):\n","                        with open(added_tokens_file, 'w', encoding='utf-8') as f:\n","                            json.dump({}, f, indent=2, ensure_ascii=False)\n","                        print(f\"Fixed added_tokens.json format for PickyBPE\")\n","\n","                return AutoTokenizer.from_pretrained(model_path)\n","        except Exception as e:\n","            print(f\"HuggingFace loading failed: {e}\")\n","\n","        try:\n","            picky_model_file = os.path.join(model_path, 'picky_bpe_model.json')\n","            if not os.path.exists(picky_model_file):\n","                print(f\"PickyBPE native model file not found at {picky_model_file}\")\n","                return None\n","\n","            with open(picky_model_file, 'r', encoding='utf-8') as f:\n","                model_data = json.load(f)\n","\n","            class PickyBPETokenizer:\n","                def __init__(self, model_data):\n","                    self.vocab = {}\n","                    self.inv_vocab = {}\n","\n","                    token_id = 0\n","                    for token_data in model_data['tokens']:\n","                        if token_data.get('present', True):\n","                            token_str = token_data['str']\n","                            self.vocab[token_str] = token_id\n","                            self.inv_vocab[token_id] = token_str\n","                            token_id += 1\n","\n","                    self.merges = []\n","                    for merge_data in model_data.get('merges', []):\n","                        left_token = merge_data['pair'][0]['str']\n","                        right_token = merge_data['pair'][1]['str']\n","                        self.merges.append((left_token, right_token))\n","\n","                def preprocess_text(self, text):\n","                    import unicodedata\n","                    text = unicodedata.normalize('NFKC', text)\n","                    text = text.replace(' ', f' ▁')\n","                    return text.strip()\n","\n","                def tokenize_word(self, word):\n","                    if not word.startswith('▁'):\n","                        word = '▁' + word\n","\n","                    tokens = []\n","                    for char in word:\n","                        if char in self.vocab:\n","                            tokens.append(char)\n","                        else:\n","                            tokens.append('<unk>')\n","\n","                    for left, right in self.merges:\n","                        new_tokens = []\n","                        i = 0\n","                        while i < len(tokens):\n","                            if i < len(tokens) - 1 and tokens[i] == left and tokens[i + 1] == right:\n","                                merged = left + right\n","                                if merged in self.vocab:\n","                                    new_tokens.append(merged)\n","                                    i += 2\n","                                else:\n","                                    new_tokens.append(tokens[i])\n","                                    i += 1\n","                            else:\n","                                new_tokens.append(tokens[i])\n","                                i += 1\n","                        tokens = new_tokens\n","\n","                    return tokens\n","\n","                def tokenize(self, text):\n","                    processed_text = self.preprocess_text(text)\n","                    words = processed_text.split()\n","                    all_tokens = []\n","                    for word in words:\n","                        word_tokens = self.tokenize_word(word)\n","                        all_tokens.extend(word_tokens)\n","                    return all_tokens\n","\n","            return PickyBPETokenizer(model_data)\n","\n","        except Exception as e:\n","            print(f\"Error loading PickyBPE native format: {e}\")\n","            return None\n","\n","    def load_grapheme_picky_bpe(self, model_path: str):\n","        try:\n","            tokenizer_file = os.path.join(model_path, 'tokenizer.json')\n","            if os.path.exists(tokenizer_file):\n","                added_tokens_file = os.path.join(model_path, 'added_tokens.json')\n","                if os.path.exists(added_tokens_file):\n","                    with open(added_tokens_file, 'r', encoding='utf-8') as f:\n","                        content = f.read().strip()\n","                        if not content or content == '[]':\n","                            with open(added_tokens_file, 'w', encoding='utf-8') as fw:\n","                                json.dump([], fw, indent=2, ensure_ascii=False)\n","\n","                return AutoTokenizer.from_pretrained(model_path)\n","        except Exception as e:\n","            print(f\"HuggingFace loading failed: {e}\")\n","\n","        try:\n","            grapheme_model_file = os.path.join(model_path, 'grapheme_picky_bpe_model.json')\n","            if not os.path.exists(grapheme_model_file):\n","                print(f\"Grapheme PickyBPE model file not found at {grapheme_model_file}\")\n","                return None\n","\n","            with open(grapheme_model_file, 'r', encoding='utf-8') as f:\n","                model_data = json.load(f)\n","\n","            class GraphemePickyBPETokenizer:\n","                def __init__(self, model_data):\n","                    self.vocab = {}\n","                    self.inv_vocab = {}\n","\n","                    token_id = 0\n","                    for token_data in model_data['tokens']:\n","                        if token_data.get('present', True):\n","                            token_str = token_data['str']\n","                            self.vocab[token_str] = token_id\n","                            self.inv_vocab[token_id] = token_str\n","                            token_id += 1\n","\n","                    self.merges = []\n","                    for merge_data in model_data.get('merges', []):\n","                        left_token = merge_data['pair'][0]['str']\n","                        right_token = merge_data['pair'][1]['str']\n","                        self.merges.append((left_token, right_token))\n","\n","                def preprocess_text(self, text):\n","                    import unicodedata\n","                    text = unicodedata.normalize('NFC', text)\n","                    text = text.replace(' ', f' ▁')\n","                    return text.strip()\n","\n","                def tokenize_word(self, word):\n","                    if not word.startswith('▁'):\n","                        word = '▁' + word\n","\n","                    import grapheme\n","                    graphemes = list(grapheme.graphemes(word))\n","\n","                    tokens = []\n","                    for g in graphemes:\n","                        if g in self.vocab:\n","                            tokens.append(g)\n","                        else:\n","                            tokens.append('<unk>')\n","\n","                    for left, right in self.merges:\n","                        new_tokens = []\n","                        i = 0\n","                        while i < len(tokens):\n","                            if i < len(tokens) - 1 and tokens[i] == left and tokens[i + 1] == right:\n","                                merged = left + right\n","                                if merged in self.vocab:\n","                                    new_tokens.append(merged)\n","                                    i += 2\n","                                else:\n","                                    new_tokens.append(tokens[i])\n","                                    i += 1\n","                            else:\n","                                new_tokens.append(tokens[i])\n","                                i += 1\n","                        tokens = new_tokens\n","\n","                    return tokens\n","\n","                def tokenize(self, text):\n","                    processed_text = self.preprocess_text(text)\n","                    words = processed_text.split()\n","                    all_tokens = []\n","                    for word in words:\n","                        word_tokens = self.tokenize_word(word)\n","                        all_tokens.extend(word_tokens)\n","                    return all_tokens\n","\n","            return GraphemePickyBPETokenizer(model_data)\n","\n","        except Exception as e:\n","            print(f\"Error loading Grapheme PickyBPE native format: {e}\")\n","            return None\n","\n","    def load_sage_tokenizer(self, model_path: str):\n","        try:\n","            model_file = os.path.join(model_path, 'nepali_tokenizer.model')\n","            if not os.path.exists(model_file):\n","                print(f\"SaGe model file not found at {model_file}\")\n","                return None\n","\n","            sp = spm.SentencePieceProcessor()\n","            sp.load(model_file)\n","            return sp\n","\n","        except Exception as e:\n","            print(f\"Error loading SaGe tokenizer: {e}\")\n","            return None\n","\n","    def evaluate_tokenizer(self, tokenizer, tokenizer_name: str, tokenizer_type: str):\n","        print(f\"Evaluating {tokenizer_name}...\")\n","\n","        if tokenizer is None:\n","            print(f\"Skipping {tokenizer_name} - not available\")\n","            return\n","\n","        sentence_results = []\n","\n","        for i, sentence in enumerate(self.test_sentences, 1):\n","            try:\n","                word_count = self.count_words(sentence)\n","\n","                if tokenizer_type == 'simple_bpe':\n","                    tokens = tokenizer.tokenize(sentence)\n","                    token_count = len(tokens)\n","                elif tokenizer_type == 'picky_bpe':\n","                    tokens = tokenizer.tokenize(sentence)\n","                    token_count = len(tokens)\n","                elif tokenizer_type == 'grapheme_picky_bpe':\n","                    tokens = tokenizer.tokenize(sentence)\n","                    token_count = len(tokens)\n","                elif tokenizer_type == 'sage':\n","                    tokens = tokenizer.encode_as_pieces(sentence)\n","                    token_count = len(tokens)\n","                else:\n","                    tokens = []\n","                    token_count = 0\n","\n","                fertility = token_count / word_count if word_count > 0 else 0\n","\n","                sentence_results.append({\n","                    'sentence_id': i,\n","                    'sentence': sentence,\n","                    'tokenizer': tokenizer_name,\n","                    'word_count': word_count,\n","                    'token_count': token_count,\n","                    'fertility': fertility,\n","                    'tokens': tokens[:10]\n","                })\n","\n","            except Exception as e:\n","                print(f\"Error processing sentence {i} with {tokenizer_name}: {e}\")\n","                sentence_results.append({\n","                    'sentence_id': i,\n","                    'sentence': sentence,\n","                    'tokenizer': tokenizer_name,\n","                    'word_count': self.count_words(sentence),\n","                    'token_count': 0,\n","                    'fertility': 0,\n","                    'tokens': []\n","                })\n","\n","        self.results.extend(sentence_results)\n","\n","    def run_evaluation(self, tokenizer_paths: Dict[str, str]):\n","        print(\"Starting Nepali Tokenizer Evaluation\")\n","        print(\"=\" * 50)\n","\n","        if 'simple_bpe' in tokenizer_paths:\n","            simple_bpe = self.load_simple_bpe(tokenizer_paths['simple_bpe'])\n","            self.evaluate_tokenizer(simple_bpe, 'SimpleBPE', 'simple_bpe')\n","\n","        if 'picky_bpe' in tokenizer_paths:\n","            picky_bpe = self.load_picky_bpe(tokenizer_paths['picky_bpe'])\n","            self.evaluate_tokenizer(picky_bpe, 'PickyBPE', 'picky_bpe')\n","\n","        if 'grapheme_picky_bpe' in tokenizer_paths:\n","            grapheme_picky_bpe = self.load_grapheme_picky_bpe(tokenizer_paths['grapheme_picky_bpe'])\n","            self.evaluate_tokenizer(grapheme_picky_bpe, 'GraphemePickyBPE', 'grapheme_picky_bpe')\n","\n","        if 'sage' in tokenizer_paths:\n","            sage = self.load_sage_tokenizer(tokenizer_paths['sage'])\n","            self.evaluate_tokenizer(sage, 'SaGe', 'sage')\n","\n","    def display_results(self):\n","        if not self.results:\n","            print(\"No results to display\")\n","            return\n","\n","        df = pd.DataFrame(self.results)\n","\n","        print(\"\\nDetailed Results by Sentence:\")\n","        print(\"=\" * 80)\n","\n","        for sentence_id in sorted(df['sentence_id'].unique()):\n","            sentence_data = df[df['sentence_id'] == sentence_id]\n","            sentence = sentence_data.iloc[0]['sentence']\n","\n","            print(f\"\\nSentence {sentence_id}: {sentence}\")\n","            print(f\"Words: {sentence_data.iloc[0]['word_count']}\")\n","            print(\"-\" * 60)\n","\n","            for _, row in sentence_data.iterrows():\n","                print(f\"{row['tokenizer']:>18}: {row['token_count']:>3} tokens | \"\n","                      f\"Fertility: {row['fertility']:>5.2f} | \"\n","                      f\"Tokens: {row['tokens']}\")\n","\n","        print(f\"\\n\\nSummary Statistics:\")\n","        print(\"=\" * 50)\n","\n","        summary = df.groupby('tokenizer').agg({\n","            'token_count': ['mean', 'std', 'sum'],\n","            'fertility': ['mean', 'std'],\n","            'word_count': ['sum']\n","        }).round(3)\n","\n","        print(summary)\n","\n","        print(f\"\\n\\nFertility Comparison (Average across all sentences):\")\n","        print(\"-\" * 40)\n","        fertility_avg = df.groupby('tokenizer')['fertility'].mean().sort_values()\n","        for tokenizer, fertility in fertility_avg.items():\n","            print(f\"{tokenizer:>18}: {fertility:.3f}\")\n","\n","        print(f\"\\n\\nTotal Tokens (sum across all sentences):\")\n","        print(\"-\" * 35)\n","        total_tokens = df.groupby('tokenizer')['token_count'].sum().sort_values()\n","        for tokenizer, total in total_tokens.items():\n","            print(f\"{tokenizer:>18}: {total:>4} tokens\")\n","\n","        return df\n","\n","def main():\n","    tokenizer_paths = {\n","        'simple_bpe': \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/tokenizers/standard_bpe\",\n","        'picky_bpe': \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/tokenizers/picky_bpe\",\n","        'grapheme_picky_bpe': \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/tokenizers/grapheme_picky_bpe\",\n","        'sage': \"/content/drive/My Drive/Colab Notebooks/LRLs/nepali/tokenizers/sage_gensim\"\n","    }\n","\n","    print(\"Checking tokenizer paths...\")\n","    for name, path in tokenizer_paths.items():\n","        if os.path.exists(path):\n","            print(f\"{name:>18}: Found at {path}\")\n","        else:\n","            print(f\"{name:>18}: NOT FOUND at {path}\")\n","\n","    evaluator = TokenizerEvaluator()\n","    evaluator.run_evaluation(tokenizer_paths)\n","\n","    results_df = evaluator.display_results()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"uR11YVxn4yfM"},"execution_count":null,"outputs":[]}]}