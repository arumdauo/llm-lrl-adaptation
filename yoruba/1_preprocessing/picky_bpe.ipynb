{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHhWzQttVsXP4YG6UcDdgQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_9EDhl_SyBi6"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers datasets numpy\n","!pip install matplotlib seaborn"],"metadata":{"id":"fkZ4cVAqyF40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Standard BPE"],"metadata":{"id":"vNd4rFDDy4Yu"}},{"cell_type":"code","source":["from __future__ import annotations\n","import json\n","import logging\n","import time\n","import argparse\n","import re\n","import sys\n","import unicodedata\n","from pathlib import Path\n","from collections import defaultdict, Counter\n","from typing import Union, Optional, Dict, List\n","import numpy as np\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    datefmt='%Y-%m-%d %H:%M:%S',\n","    handlers=[\n","        logging.StreamHandler(sys.stdout),\n","    ],\n","    force=True\n",")\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","# constants\n","WHITESPACE = '▁'\n","PAD = '<pad>'\n","UNK = '<unk>'\n","BOS = '<s>'\n","EOS = '</s>'\n","\n","class MCounter(Counter):\n","    \"\"\"extended Counter class with multiplication support\"\"\"\n","    def __mul__(self, other):\n","        if not isinstance(other, int):\n","            raise TypeError(\"Non-int factor\")\n","        return MCounter({k: other * v for k, v in self.items()})\n","\n","    def __rmul__(self, other):\n","        return self * other\n","\n","    def __add__(self, other):\n","        return MCounter(super().__add__(other))\n","\n","class SimpleBPE:\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        pad_id: int = 3,\n","        unk_id: int = 0,\n","        bos_id: int = 1,\n","        eos_id: int = 2,\n","        coverage: float = 0.9999,\n","    ):\n","        self.desired_vocab_size = vocab_size\n","        self.coverage = coverage\n","\n","        self.special_tokens = {\n","            PAD: pad_id,\n","            UNK: unk_id,\n","            BOS: bos_id,\n","            EOS: eos_id\n","        }\n","\n","        # vocabulary mappings\n","        self.vocab = {}  # token_str -> token_id\n","        self.id2token = {}  # token_id -> token_str\n","        self.merges = []  # list of merge rules (left, right)\n","\n","        # initialize with special tokens\n","        for token_str, token_id in self.special_tokens.items():\n","            self.vocab[token_str] = token_id\n","            self.id2token[token_id] = token_str\n","\n","        self.next_id = max(self.special_tokens.values()) + 1\n","\n","    def _preprocess_yoruba_text(self, text: str) -> str:\n","        text = unicodedata.normalize('NFC', text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text.strip()\n","\n","    def _get_words(self, file: str) -> Dict[str, int]:\n","        \"\"\"load and preprocess corpus from file\"\"\"\n","        start_time = time.time()\n","\n","        word_freqs = MCounter()\n","        with open(file, 'r', encoding='utf-8') as f:\n","            for i, line in enumerate(f):\n","                if not line.strip():\n","                    continue\n","\n","                processed_line = self._preprocess_yoruba_text(line)\n","                words = processed_line.split()\n","\n","                words = [WHITESPACE + word for word in words]\n","                word_freqs.update(words)\n","\n","                if i > 0 and i % 50000 == 0:\n","                    logging.info(f'Processed {i} lines.')\n","\n","        num_words = len(word_freqs)\n","        logging.info(f'Loaded {num_words} unique words in {time.time() - start_time:.2f}s.')\n","\n","        return dict(word_freqs)\n","\n","    def _get_characters(self, word_freqs: Dict[str, int]) -> MCounter:\n","        \"\"\"extract character frequencies from words\"\"\"\n","        char_freqs = MCounter()\n","        for word, freq in word_freqs.items():\n","            for char in word:\n","                char_freqs[char] += freq\n","        return char_freqs\n","\n","    def _filter_characters(self, char_freqs: MCounter) -> MCounter:\n","        \"\"\"filter rare characters based on coverage threshold\"\"\"\n","        if self.coverage < 1:\n","            total_chars = sum(char_freqs.values())\n","            target_chars = round(self.coverage * total_chars)\n","\n","            # sort characters by frequency (descending)\n","            sorted_chars = char_freqs.most_common()\n","\n","            # Keep characters until target coverage is reached\n","            kept_chars = MCounter()\n","            char_count = 0\n","            for char, freq in sorted_chars:\n","                kept_chars[char] = freq\n","                char_count += freq\n","                if char_count >= target_chars:\n","                    break\n","\n","            removed_count = len(char_freqs) - len(kept_chars)\n","\n","            return kept_chars\n","        return char_freqs\n","\n","    def _initialize_vocab(self, word_freqs: Dict[str, int]) -> Dict[str, List[str]]:\n","        \"\"\"initialize vocabulary with characters and return word splits\"\"\"\n","        # get character frequencies\n","        char_freqs = self._get_characters(word_freqs)\n","        filtered_chars = self._filter_characters(char_freqs)\n","\n","        # add characters to vocabulary\n","        for char in filtered_chars:\n","            if char not in self.vocab:\n","                self.vocab[char] = self.next_id\n","                self.id2token[self.next_id] = char\n","                self.next_id += 1\n","\n","        # initialize word splits\n","        word_splits = {}\n","        for word in word_freqs:\n","            splits = []\n","            for char in word:\n","                if char in self.vocab:\n","                    splits.append(char)\n","                else:\n","                    splits.append(UNK)\n","            word_splits[word] = splits\n","\n","        return word_splits\n","\n","    def _get_pairs(self, word_splits: Dict[str, List[str]], word_freqs: Dict[str, int]) -> MCounter:\n","        \"\"\"count all adjacent pairs in the vocabulary\"\"\"\n","        pairs = MCounter()\n","\n","        for word, splits in word_splits.items():\n","            freq = word_freqs[word]\n","            for i in range(len(splits) - 1):\n","                pair = (splits[i], splits[i + 1])\n","                pairs[pair] += freq\n","\n","        return pairs\n","\n","    def _merge_pair(self, pair: tuple[str, str], word_splits: Dict[str, List[str]],\n","                    word_freqs: Dict[str, int]) -> Dict[str, List[str]]:\n","        \"\"\"merge a pair in all word splits\"\"\"\n","        left, right = pair\n","        merged = left + right\n","\n","        # add merged token to vocabulary\n","        if merged not in self.vocab:\n","            self.vocab[merged] = self.next_id\n","            self.id2token[self.next_id] = merged\n","            self.next_id += 1\n","\n","        # record merge rule\n","        self.merges.append(pair)\n","\n","        # update word splits\n","        new_word_splits = {}\n","        for word, splits in word_splits.items():\n","            new_splits = []\n","            i = 0\n","            while i < len(splits):\n","                if i < len(splits) - 1 and splits[i] == left and splits[i + 1] == right:\n","                    # merge pair\n","                    new_splits.append(merged)\n","                    i += 2\n","                else:\n","                    new_splits.append(splits[i])\n","                    i += 1\n","            new_word_splits[word] = new_splits\n","\n","        return new_word_splits\n","\n","    def fit(self, input_file: str, output_dir: str, logging_step: int = 200) -> None:\n","        \"\"\"Train BPE tokenizer\"\"\"\n","        output_path = Path(output_dir)\n","        output_path.mkdir(parents=True, exist_ok=True)\n","\n","        # load and preprocess data\n","        word_freqs = self._get_words(input_file)\n","        word_splits = self._initialize_vocab(word_freqs)\n","\n","        current_vocab_size = len(self.vocab)\n","        logging.info(f'Starting BPE training with {current_vocab_size} initial tokens.')\n","\n","        # training loop\n","        merge_times = []\n","        while current_vocab_size < self.desired_vocab_size:\n","            start_time = time.time()\n","\n","            # count all pairs\n","            pairs = self._get_pairs(word_splits, word_freqs)\n","\n","            if not pairs:\n","                logging.info('No more pairs to merge. Stopping training.')\n","                break\n","\n","            # find most frequent pair\n","            most_frequent_pair, freq = pairs.most_common(1)[0]\n","\n","            if freq <= 1:\n","                logging.info('No pairs with frequency > 1. Stopping training.')\n","                break\n","\n","            # merge pair\n","            word_splits = self._merge_pair(most_frequent_pair, word_splits, word_freqs)\n","            current_vocab_size += 1\n","\n","            merge_times.append(time.time() - start_time)\n","\n","            if current_vocab_size % logging_step == 0:\n","                left, right = most_frequent_pair\n","                avg_time = np.mean(merge_times) if merge_times else 0\n","                logging.info(\n","                    f'Vocab size: {current_vocab_size:,}/{self.desired_vocab_size:,}. '\n","                    f'Merged \"{left}\" + \"{right}\" (freq: {freq:,}). '\n","                    f'Avg merge time: {avg_time:.3f}s'\n","                )\n","                merge_times = []\n","\n","        logging.info(f'Training completed with final vocabulary size: {len(self.vocab):,}')\n","\n","        self._save_simple_bpe_model(output_path / 'simple_bpe_model.json')\n","        self._save_huggingface_files(output_path)\n","\n","        logging.info(f'Files saved to {output_path}')\n","\n","    def _save_simple_bpe_model(self, file_path: Path) -> None:\n","        \"\"\"Save BPE model\"\"\"\n","        logging.info(f'Saving BPE model to {file_path}...')\n","\n","        model_data = {\n","            'vocab': self.vocab,\n","            'merges': [{'left': left, 'right': right} for left, right in self.merges],\n","            'vocab_size': len(self.vocab),\n","            'special_tokens': self.special_tokens\n","        }\n","\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(model_data, f, indent=2, ensure_ascii=False)\n","\n","    def _save_huggingface_files(self, output_path: Path) -> None:\n","        \"\"\"Save HuggingFace compatible files\"\"\"\n","        logging.info('Saving HuggingFace compatible files...')\n","\n","        # format merges for HuggingFace\n","        hf_merges = [f\"{left} {right}\" for left, right in self.merges]\n","\n","        # tokenizer.json\n","        tokenizer_data = {\n","            \"version\": \"1.0\",\n","            \"truncation\": None,\n","            \"padding\": None,\n","            \"added_tokens\": [],\n","            \"normalizer\": {\n","                \"type\": \"NFC\"\n","            },\n","            \"pre_tokenizer\": {\n","                \"type\": \"Sequence\",\n","                \"pretokenizers\": [\n","                    {\n","                        \"type\": \"WhitespaceSplit\"\n","                    },\n","                    {\n","                        \"type\": \"Metaspace\",\n","                        \"replacement\": WHITESPACE,\n","                        \"add_prefix_space\": True\n","                    }\n","                ]\n","            },\n","            \"post_processor\": {\n","                \"type\": \"TemplateProcessing\",\n","                \"single\": f\"{BOS}:1 $A:0 {EOS}:1\",\n","                \"pair\": f\"{BOS}:1 $A:0 {EOS}:1 $B:0 {EOS}:1\",\n","                \"special_tokens\": {\n","                    BOS: {\"id\": self.special_tokens[BOS], \"type_id\": 1},\n","                    EOS: {\"id\": self.special_tokens[EOS], \"type_id\": 1}\n","                }\n","            },\n","            \"decoder\": {\n","                \"type\": \"Metaspace\",\n","                \"replacement\": WHITESPACE,\n","                \"add_prefix_space\": True\n","            },\n","            \"model\": {\n","                \"type\": \"BPE\",\n","                \"dropout\": None,\n","                \"unk_token\": UNK,\n","                \"continuing_subword_prefix\": None,\n","                \"end_of_word_suffix\": None,\n","                \"fuse_unk\": False,\n","                \"vocab\": self.vocab,\n","                \"merges\": hf_merges\n","            }\n","        }\n","\n","        with open(output_path / 'tokenizer.json', 'w', encoding='utf-8') as f:\n","            json.dump(tokenizer_data, f, indent=2, ensure_ascii=False)\n","\n","        # tokenizer_config.json\n","        config_data = {\n","            \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","            \"auto_map\": {\n","                \"AutoTokenizer\": [\"tokenizer.json\", None]\n","            },\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD,\n","            \"model_max_length\": 2048,\n","            \"padding_side\": \"left\",\n","            \"truncation_side\": \"right\",\n","            \"chat_template\": None,\n","            \"clean_up_tokenization_spaces\": True,\n","            \"spaces_between_special_tokens\": False\n","        }\n","\n","        with open(output_path / 'tokenizer_config.json', 'w', encoding='utf-8') as f:\n","            json.dump(config_data, f, indent=2, ensure_ascii=False)\n","\n","        # special_tokens_map.json\n","        special_tokens_data = {\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD\n","        }\n","\n","        with open(output_path / 'special_tokens_map.json', 'w', encoding='utf-8') as f:\n","            json.dump(special_tokens_data, f, indent=2, ensure_ascii=False)\n","\n","        # added_tokens.json\n","        with open(output_path / 'added_tokens.json', 'w', encoding='utf-8') as f:\n","            json.dump({}, f, indent=2, ensure_ascii=False)\n","\n","        # vocab.json\n","        with open(output_path / 'vocab.json', 'w', encoding='utf-8') as f:\n","            json.dump(self.vocab, f, indent=2, ensure_ascii=False)\n","\n","        logging.info(f'Final vocabulary size: {len(self.vocab):,}')\n","        logging.info(f'Number of merge rules: {len(self.merges):,}')\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        \"\"\"tokenize text using trained model\"\"\"\n","        # preprocess\n","        text = self._preprocess_yoruba_text(text)\n","        words = text.split()\n","\n","        tokens = []\n","        for word in words:\n","            word_with_marker = WHITESPACE + word\n","            word_tokens = self._tokenize_word(word_with_marker)\n","            tokens.extend(word_tokens)\n","\n","        return tokens\n","\n","    def _tokenize_word(self, word: str) -> List[str]:\n","        \"\"\"tokenize a single word using BPE merges\"\"\"\n","        tokens = []\n","        for char in word:\n","            if char in self.vocab:\n","                tokens.append(char)\n","            else:\n","                tokens.append(UNK)\n","\n","        # apply merge rules\n","        for left, right in self.merges:\n","            new_tokens = []\n","            i = 0\n","            while i < len(tokens):\n","                if i < len(tokens) - 1 and tokens[i] == left and tokens[i + 1] == right:\n","                    merged = left + right\n","                    new_tokens.append(merged)\n","                    i += 2\n","                else:\n","                    new_tokens.append(tokens[i])\n","                    i += 1\n","            tokens = new_tokens\n","\n","        return tokens\n","\n","def main():\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        format='%(asctime)s - %(levelname)s - %(message)s'\n","    )\n","\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/dataset/yo_train.txt\"\n","    output_dir = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/tokenizers/standard_bpe\"\n","    vocab_size = 10000\n","\n","    tokenizer = SimpleBPE(\n","        vocab_size=vocab_size,\n","        coverage=0.9999\n","    )\n","\n","    print(f\"Input file: {input_file}\")\n","    print(f\"Output directory: {output_dir}\")\n","    print(f\"Target vocabulary size: {vocab_size}\")\n","\n","    tokenizer.fit(input_file, output_dir)\n","\n","    print(\"Training completed.\")\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"A5Hr90UOy6f-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Picky BPE"],"metadata":{"id":"lKMMnRQ6yI39"}},{"cell_type":"code","source":["from __future__ import annotations\n","import json\n","import logging\n","import time\n","import argparse\n","import re\n","import unicodedata\n","from pathlib import Path\n","from collections import defaultdict, Counter\n","from typing import Union, Optional, Dict, List\n","import numpy as np\n","\n","# constants\n","WHITESPACE = '▁'\n","PAD = '<pad>'\n","UNK = '<unk>'\n","BOS = '<s>'\n","EOS = '</s>'\n","\n","class MCounter(Counter):\n","    \"\"\"extended Counter class with multiplication support\"\"\"\n","    def __mul__(self, other):\n","        if not isinstance(other, int):\n","            raise TypeError(\"Non-int factor\")\n","        return MCounter({k: other * v for k, v in self.items()})\n","\n","    def __rmul__(self, other):\n","        return self * other\n","\n","    def __add__(self, other):\n","        return MCounter(super().__add__(other))\n","\n","class Token:\n","    def __init__(\n","        self,\n","        id: int,\n","        str: str,\n","        freq: int = 0,\n","        special: bool = False,\n","        present: bool = True,\n","        left: Optional[Token] = None,\n","        right: Optional[Token] = None,\n","        split: Optional[list[Token]] = None\n","    ):\n","        self.id = id\n","        self.str = str\n","        self.freq = freq\n","        self.special = special\n","        self.present = present\n","        self.atomic = len(str) == 1 or special\n","        self.words = set()\n","        self.left = left\n","        self.right = right\n","        self.split = split\n","\n","    def __repr__(self):\n","        return f'{self.str} ({self.freq})'\n","\n","    def walk(self) -> list[Token]:\n","        if self.atomic or self.present:\n","            return [self]\n","        return self.left.walk() + self.right.walk()\n","\n","    def remove(self) -> None:\n","        if self.atomic:\n","            raise ValueError(f'Cannot remove an atomic token {self.str}.')\n","        self.present = False\n","        self.freq = 0\n","        self.words = set()\n","\n","    def restore(self) -> None:\n","        if self.present:\n","            raise ValueError(f'Cannot revoke already present token {self.str}.')\n","        self.present = True\n","\n","    def split_if_possible(self) -> Optional[list[Token]]:\n","        if self.atomic:\n","            return None\n","        self.present = False\n","        return self.walk()\n","\n","    def to_dict(self) -> dict:\n","        return {\n","            'id': self.id,\n","            'str': self.str,\n","            'freq': self.freq,\n","            'special': self.special,\n","            'present': self.present,\n","            'left': self.left.id if self.left is not None else None,\n","            'right': self.right.id if self.right is not None else None,\n","            'split': [t.id for t in self.walk()]\n","        }\n","\n","class Word:\n","    def __init__(self, id: int, word: str, freq: int = 0):\n","        self.id = id\n","        self.str = word\n","        self.freq = freq\n","        self.tokens = None\n","        self.pairs = None\n","\n","    def __repr__(self) -> str:\n","        return f'{self.str} ({self.freq})'\n","\n","    def encode(self, str2token: dict[str, Token]) -> None:\n","        self.tokens = [str2token[c] for c in self.str]\n","        self._recalculate()\n","\n","    def _recalculate(self, update_tokens: bool = True) -> None:\n","        self.pairs = MCounter(zip(self.tokens[:-1], self.tokens[1:])) * self.freq\n","        if update_tokens:\n","            for token in self.tokens:\n","                token.words.add(self)\n","\n","    def merge_pair(self, pair: tuple[Token, Token], new_token: Token, update_tokens: bool = True) -> int:\n","        new_tokens = []\n","        i = 0\n","        while i < len(self.tokens):\n","            if i < len(self.tokens) - 1 and (self.tokens[i], self.tokens[i+1]) == pair:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(self.tokens[i])\n","                i += 1\n","        new_token_frequency = len(self.tokens) - len(new_tokens)\n","        if update_tokens:\n","            pair[0].words.discard(self)\n","            pair[1].words.discard(self)\n","        self.tokens = new_tokens\n","        self._recalculate(update_tokens=update_tokens)\n","        return new_token_frequency * self.freq\n","\n","    def split_token(self, token: Token, split: list[Token], update_tokens: bool = True):\n","        new_tokens = []\n","        for t in self.tokens:\n","            if t == token:\n","                new_tokens.extend(split)\n","            else:\n","                new_tokens.append(t)\n","        self.tokens = new_tokens\n","        self._recalculate(update_tokens=update_tokens)\n","\n","class PickyBPE:\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        pad_id: int = 3,\n","        unk_id: int = 0,\n","        bos_id: int = 1,\n","        eos_id: int = 2,\n","        coverage: float = 0.8,\n","        threshold: float = 0.8\n","    ):\n","        self.desired_vocab_size = vocab_size\n","        self.pad_token = Token(pad_id, PAD, 0, special=True)\n","        self.unk_token = Token(unk_id, UNK, 0, special=True)\n","        self.bos_token = Token(bos_id, BOS, 0, special=True)\n","        self.eos_token = Token(eos_id, EOS, 0, special=True)\n","\n","        self.id2token = {\n","            token.id: token for token in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n","        }\n","        self.str2token = {\n","            token.str: token for token in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n","        }\n","        self.str2token = defaultdict(lambda: self.unk_token, self.str2token)\n","        self.max_special_token_id = max(self.id2token.keys())\n","        self.actual_vocab_size = len(self.id2token)\n","        self.new_id = self.max_special_token_id + 1\n","        self.coverage = coverage\n","        self.threshold = threshold\n","        self.events = list()\n","\n","    def _preprocess_yoruba_text(self, text: str) -> str:\n","        text = unicodedata.normalize('NFC', text)\n","        text = text.replace(' ', f' {WHITESPACE}')\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text\n","\n","    def _get_words(self, file: str) -> list[Word]:\n","        \"\"\"load and preprocess corpus from file\"\"\"\n","        start_time = time.time()\n","\n","        counter = MCounter()\n","        with open(file, 'r', encoding='utf-8') as f:\n","            for i, line in enumerate(f):\n","                processed_line = self._preprocess_yoruba_text(line.strip())\n","                words = processed_line.split()\n","\n","                words = [WHITESPACE + word if not word.startswith(WHITESPACE) else word for word in words]\n","                counter.update(words)\n","\n","                if i > 0 and i % 50000 == 0:\n","                    logging.info(f'Processed {i} lines.')\n","\n","        num_words = len(counter)\n","        logging.info(f'Loaded {num_words} unique words in {time.time() - start_time:.2f}s.')\n","\n","        return [Word(i, word, freq) for i, (word, freq) in enumerate(counter.items())]\n","\n","    def _get_characters(self, words: list[Word]) -> MCounter:\n","        \"\"\"extract character frequencies from words\"\"\"\n","        counter = MCounter()\n","        for i, word in enumerate(words):\n","            counter.update(MCounter(word.str) * word.freq)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for character extraction.')\n","        return counter\n","\n","    def _filter_characters(self, characters: MCounter) -> MCounter:\n","        \"\"\"filter rare characters based on coverage threshold\"\"\"\n","        if self.coverage < 1:\n","            corpus_size = sum(characters.values())\n","            freq_to_remove = corpus_size - round(self.coverage * corpus_size)\n","            if freq_to_remove > 0:\n","                cum_sum = np.cumsum([freq for _, freq in reversed(characters.most_common())])\n","                num_to_remove = np.searchsorted(cum_sum, freq_to_remove)\n","                characters_to_remove = [c for c, _ in characters.most_common()[-num_to_remove:]]\n","                for c in characters_to_remove:\n","                    characters.pop(c)\n","                logging.info(f'Replaced {num_to_remove} rare characters with UNK.')\n","        return characters\n","\n","    def _initialize_vocab(self, words: list[Word]) -> None:\n","        \"\"\"initialize vocabulary with characters\"\"\"\n","        characters = self._get_characters(words)\n","        filtered_characters = self._filter_characters(characters)\n","\n","        for i, character in enumerate(filtered_characters):\n","            token = Token(self.new_id + i, character, filtered_characters[character])\n","            self.id2token[token.id] = token\n","            self.str2token[token.str] = token\n","\n","        self.new_id += len(filtered_characters)\n","        self.actual_vocab_size += len(filtered_characters)\n","        logging.info(f'Initialized vocabulary with {len(filtered_characters)} unique characters.')\n","\n","    @staticmethod\n","    def _validate_pair(pair) -> bool:\n","        \"\"\"check if pair contains only non-special tokens\"\"\"\n","        return not any(token.special for token in pair)\n","\n","    def _encode_words(self, words: list[Word]) -> None:\n","        \"\"\"encode words using current vocabulary\"\"\"\n","        logging.info('Encoding words...')\n","        for i, word in enumerate(words):\n","            word.encode(self.str2token)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for encoding.')\n","\n","    def _initialize_pairs(self, words: list[Word]) -> MCounter:\n","        \"\"\"initialize pair frequencies\"\"\"\n","        pairs = MCounter()\n","        logging.info('Counting character pairs...')\n","        for i, word in enumerate(words):\n","            pairs.update(word.pairs)\n","            if i > 0 and i % 100000 == 0:\n","                logging.info(f'Processed {i} words for pair counting.')\n","\n","        to_remove = set()\n","        for pair in pairs:\n","            if not self._validate_pair(pair):\n","                to_remove.add(pair)\n","        for pair in to_remove:\n","            pairs.pop(pair)\n","\n","        return pairs\n","\n","    def _remove_if_possible(self, token: Token, merged_freq: int, pairs: MCounter) -> bool:\n","        \"\"\"remove token if it meets the threshold criteria\"\"\"\n","        if merged_freq / (token.freq + merged_freq) > self.threshold:\n","            split = token.split_if_possible()\n","            if split is not None:\n","                self.actual_vocab_size -= 1\n","                for t in split:\n","                    t.freq += token.freq\n","                for pair in zip(split[:-1], split[1:]):\n","                    pairs[pair] += token.freq\n","\n","                pairs_for_update = MCounter()\n","                for word in token.words:\n","                    if token not in word.tokens:\n","                        raise ValueError(f'Token {token} not found in word {word}.')\n","                    pairs_for_update.update({\n","                        pair: freq for pair, freq in word.pairs.items()\n","                        if self._validate_pair(pair) and token in pair\n","                    })\n","                    word.split_token(token, split)\n","\n","                self._update_pairs_on_remove(token, split, pairs_for_update, pairs)\n","                token.remove()\n","                return True\n","        return False\n","\n","    @staticmethod\n","    def _update_pairs_on_merge(new_token: Token, pair: tuple[Token, Token],\n","                              pairs_for_update: MCounter, pairs: MCounter):\n","        \"\"\"update pair frequencies after merge operation\"\"\"\n","        pairs.update(pairs_for_update)\n","        for p, freq in pairs_for_update.items():\n","            if new_token not in p:\n","                raise ValueError(f'Pair {p} does not contain the new token {new_token}.')\n","            if new_token is p[0]:\n","                if new_token is p[1]:\n","                    to_update = (pair[1], pair[0])\n","                else:\n","                    to_update = (pair[1], p[1])\n","            else:\n","                to_update = (p[0], pair[0])\n","            if to_update in pairs:\n","                pairs[to_update] -= freq\n","                if pairs[to_update] <= 0:\n","                    pairs.pop(to_update)\n","\n","    @staticmethod\n","    def _update_pairs_on_remove(token: Token, split: list[Token],\n","                               pairs_for_update: MCounter, pairs: MCounter):\n","        \"\"\"Update pair frequencies after split operation\"\"\"\n","        for pair, freq in pairs_for_update.items():\n","            if token is pair[0]:\n","                if token is pair[1]:\n","                    to_update = (split[-1], split[0])\n","                else:\n","                    to_update = (split[-1], pair[1])\n","            else:\n","                to_update = (pair[0], split[0])\n","            pairs[to_update] += freq\n","            pairs.pop(pair)\n","\n","    def _merge_token_in_words(self, token_to_merge: Token, pair_to_merge: tuple[Token, Token],\n","                             pairs: MCounter) -> int:\n","        \"\"\"merge token in all relevant words\"\"\"\n","        actual_freq = 0\n","        pairs_for_update = MCounter()\n","\n","        for word in pair_to_merge[0].words & pair_to_merge[1].words:\n","            if pair_to_merge in word.pairs:\n","                word.pairs.pop(pair_to_merge)\n","                actual_freq += word.merge_pair(pair_to_merge, token_to_merge)\n","                pairs_for_update.update({\n","                    p: f for p, f in word.pairs.items()\n","                    if self._validate_pair(p) and token_to_merge in p\n","                })\n","\n","        self._update_pairs_on_merge(token_to_merge, pair_to_merge, pairs_for_update, pairs)\n","        token_to_merge.freq += actual_freq\n","\n","        if pair_to_merge[0] is pair_to_merge[1]:\n","            pair_to_merge[0].freq -= 2 * actual_freq\n","            removed = self._remove_if_possible(pair_to_merge[0], actual_freq, pairs)\n","            if removed:\n","                logging.info(f'Removed token {pair_to_merge[0].str} after merging into {token_to_merge.str}.')\n","                self.events.append(('SPLIT', pair_to_merge[0], pair_to_merge[0].walk()))\n","        else:\n","            for token in pair_to_merge:\n","                if not token.present:\n","                    raise ValueError(f'Token {token} is not present in vocabulary.')\n","                token.freq -= actual_freq\n","                token_freq = token.freq\n","                removed = self._remove_if_possible(token, actual_freq, pairs)\n","                if removed:\n","                    logging.info(f'Removed token {token.str} after merging into {token_to_merge.str}.')\n","                    self.events.append(('SPLIT', token, token.walk()))\n","\n","        return actual_freq\n","\n","    def _merge_pair(self, pair: tuple[Token, Token], pairs: MCounter) -> int:\n","        \"\"\"merge a token pair\"\"\"\n","        pairs.pop(pair)\n","        merged_str = pair[0].str + pair[1].str\n","\n","        if merged_str in self.str2token:\n","            new_token = self.str2token[merged_str]\n","            if not new_token.present:\n","                new_token.restore()\n","                logging.info(f'Restored previously removed token {new_token.str}.')\n","            else:\n","                logging.info(f'Additional merges for {new_token.str}.')\n","        else:\n","            new_token = Token(self.new_id, merged_str, 0, left=pair[0], right=pair[1])\n","            self.id2token[new_token.id] = new_token\n","            self.str2token[new_token.str] = new_token\n","            self.new_id += 1\n","\n","        self.events.append(('MERGE', pair, new_token))\n","        actual_freq = self._merge_token_in_words(new_token, pair, pairs)\n","        return actual_freq\n","\n","    def fit(self, input_file: str, output_dir: str, logging_step: int = 200) -> None:\n","        \"\"\"train PickyBPE tokenizer\"\"\"\n","        output_path = Path(output_dir)\n","        output_path.mkdir(parents=True, exist_ok=True)\n","\n","        # train PickyBPE\n","        words = self._get_words(input_file)\n","        self._initialize_vocab(words)\n","        self._encode_words(words)\n","        pairs = self._initialize_pairs(words)\n","\n","        merge_time = []\n","        while self.actual_vocab_size < self.desired_vocab_size:\n","            start_time = time.time()\n","            if not pairs:\n","                logging.info(f'No more pairs to merge. Stopping with vocab size of {self.actual_vocab_size}.')\n","                break\n","\n","            pair, count = pairs.most_common(1)[0]\n","            if count <= 0:\n","                logging.info(f'No more pairs to merge. Stopping with vocab size of {self.actual_vocab_size}.')\n","                break\n","\n","            freq = self._merge_pair(pair, pairs)\n","            self.actual_vocab_size += 1\n","            merge_time.append(time.time() - start_time)\n","\n","            if self.actual_vocab_size % logging_step == 0:\n","                logging.info(\n","                    f'VOCABULARY SIZE: {self.actual_vocab_size}. '\n","                    f'Merged {pair[0].str} + {pair[1].str} with frequency {freq}. '\n","                    f'Average merge time {np.mean(merge_time):.2f}s.'\n","                )\n","                merge_time = []\n","\n","        self._save_picky_model(output_path / 'picky_bpe_model.json')\n","\n","        self._save_huggingface_files(output_path)\n","\n","        logging.info(f'Training completed. Files saved to {output_path}')\n","\n","    def _save_picky_model(self, file_path: Path) -> None:\n","        logging.info(f'Saving PickyBPE model to {file_path}...')\n","\n","        assigned_ids = sorted(self.id2token.keys())\n","        id_mapping = {}\n","        id_counter = 0\n","\n","        for i in assigned_ids:\n","            if self.id2token[i].present:\n","                id_mapping[i] = id_counter\n","                id_counter += 1\n","\n","        model_data = {\n","            'tokens': [token.to_dict() for token in self.id2token.values()],\n","            'id2int': {str(k): v for k, v in id_mapping.items()},\n","            'int2id': {str(v): k for k, v in id_mapping.items()},\n","            'merges': [\n","                {'id': i, 'pair': [token.to_dict() for token in merge[1]], 'new_token': merge[2].to_dict()}\n","                for i, merge in enumerate(self.events) if merge[0] == 'MERGE'\n","            ],\n","            'splits': [\n","                {'id': i, 'token': merge[1].to_dict(), 'split': [token.to_dict() for token in merge[2]]}\n","                for i, merge in enumerate(self.events) if merge[0] == 'SPLIT'\n","            ],\n","        }\n","\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(model_data, f, indent=2, ensure_ascii=False)\n","\n","    def _save_huggingface_files(self, output_path: Path) -> None:\n","        logging.info('Converting to HuggingFace format...')\n","\n","        # extract present tokens and create vocabulary\n","        vocab = {}\n","        present_tokens = []\n","        for token_id in sorted(self.id2token.keys()):\n","            token = self.id2token[token_id]\n","            if token.present:\n","                vocab[token.str] = len(present_tokens)\n","                present_tokens.append(token)\n","\n","        # create merge rules from events\n","        merges = []\n","        for event in self.events:\n","            if event[0] == 'MERGE':\n","                pair = event[1]\n","                left_str = pair[0].str\n","                right_str = pair[1].str\n","                if left_str in vocab and right_str in vocab:\n","                    merges.append(f\"{left_str} {right_str}\")\n","\n","        # tokenizer.json\n","        tokenizer_data = {\n","            \"version\": \"1.0\",\n","            \"truncation\": None,\n","            \"padding\": None,\n","            \"added_tokens\": [],\n","            \"normalizer\": {\n","                \"type\": \"NFC\"\n","            },\n","            \"pre_tokenizer\": {\n","                \"type\": \"Sequence\",\n","                \"pretokenizers\": [\n","                    {\n","                        \"type\": \"WhitespaceSplit\"\n","                    },\n","                    {\n","                        \"type\": \"Metaspace\",\n","                        \"replacement\": WHITESPACE,\n","                        \"add_prefix_space\": True\n","                    }\n","                ]\n","            },\n","            \"post_processor\": {\n","                \"type\": \"TemplateProcessing\",\n","                \"single\": f\"{BOS}:1 $A:0 {EOS}:1\",\n","                \"pair\": f\"{BOS}:1 $A:0 {EOS}:1 $B:0 {EOS}:1\",\n","                \"special_tokens\": {\n","                    BOS: {\"id\": 1, \"type_id\": 1},\n","                    EOS: {\"id\": 2, \"type_id\": 1}\n","                }\n","            },\n","            \"decoder\": {\n","                \"type\": \"Metaspace\",\n","                \"replacement\": WHITESPACE,\n","                \"add_prefix_space\": True\n","            },\n","            \"model\": {\n","                \"type\": \"BPE\",\n","                \"dropout\": None,\n","                \"unk_token\": UNK,\n","                \"continuing_subword_prefix\": None,\n","                \"end_of_word_suffix\": None,\n","                \"fuse_unk\": False,\n","                \"vocab\": vocab,\n","                \"merges\": merges\n","            }\n","        }\n","\n","        with open(output_path / 'tokenizer.json', 'w', encoding='utf-8') as f:\n","            json.dump(tokenizer_data, f, indent=2, ensure_ascii=False)\n","\n","        # tokenizer_config.json\n","        config_data = {\n","            \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","            \"auto_map\": {\n","                \"AutoTokenizer\": [\"tokenizer.json\", None]\n","            },\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD,\n","            \"model_max_length\": 2048,\n","            \"padding_side\": \"left\",\n","            \"truncation_side\": \"right\",\n","            \"chat_template\": None,\n","            \"clean_up_tokenization_spaces\": True,\n","            \"spaces_between_special_tokens\": False\n","        }\n","\n","        with open(output_path / 'tokenizer_config.json', 'w', encoding='utf-8') as f:\n","            json.dump(config_data, f, indent=2, ensure_ascii=False)\n","\n","        # special_tokens_map.json\n","        special_tokens_data = {\n","            \"bos_token\": BOS,\n","            \"eos_token\": EOS,\n","            \"unk_token\": UNK,\n","            \"pad_token\": PAD\n","        }\n","\n","        with open(output_path / 'special_tokens_map.json', 'w', encoding='utf-8') as f:\n","            json.dump(special_tokens_data, f, indent=2, ensure_ascii=False)\n","\n","        # added_tokens.json (empty for now)\n","        with open(output_path / 'added_tokens.json', 'w', encoding='utf-8') as f:\n","            json.dump([], f, indent=2, ensure_ascii=False)\n","\n","        # vocab.json for compatibility\n","        with open(output_path / 'vocab.json', 'w', encoding='utf-8') as f:\n","            json.dump(vocab, f, indent=2, ensure_ascii=False)\n","\n","        logging.info(f'Final vocabulary size: {len(vocab)}')\n","        logging.info(f'Number of merge rules: {len(merges)}')\n","\n","def main():\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        format='%(asctime)s - %(levelname)s - %(message)s'\n","    )\n","\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/dataset/yo_train.txt\"\n","    output_dir = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/tokenizers/picky_bpe\"\n","    vocab_size = 10000\n","\n","    tokenizer = PickyBPE(\n","        vocab_size=vocab_size,\n","        coverage=0.8,\n","        threshold=0.8\n","    )\n","\n","    print(f\"Input file: {input_file}\")\n","    print(f\"Output directory: {output_dir}\")\n","    print(f\"Target vocabulary size: {vocab_size}\")\n","\n","    tokenizer.fit(input_file, output_dir)\n","\n","    print(\"Training completed.\")\n","    print(f\"Files saved to: {output_dir}\")\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"qZrGs09GyL2R"},"execution_count":null,"outputs":[]}]}