{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNOQEOQ5ffFjmz9evLMA0C2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VlQS58qO4ANM"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"82NpwpkC4E9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","SaGe tokenizer training with early stopping and adaptive pruning.\n","\"\"\"\n","\n","import os\n","import json\n","import numpy as np\n","from pathlib import Path\n","from collections import defaultdict, Counter\n","from typing import List, Dict, Tuple\n","import tempfile\n","import time\n","import logging\n","import sys\n","import shutil\n","import matplotlib.pyplot as plt\n","import sentencepiece as spm\n","import gensim.models\n","import random\n","import re\n","\n","def sigmoid(x):\n","    \"\"\"sigmoid function\"\"\"\n","    x = np.asarray(x)\n","    return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    datefmt='%Y-%m-%d %H:%M:%S',\n","    handlers=[\n","        logging.StreamHandler(sys.stdout),\n","    ],\n","    force=True\n",")\n","logger = logging.getLogger(__name__)\n","\n","class YorubaMonitoredSaGeTokenizer:\n","\n","    def __init__(self,\n","                 vocab_size: int = 10000,\n","                 initial_vocab_multiplier: float = 2.0,\n","                 max_token_length: int = 5,\n","                 embedding_dim: int = 50,\n","                 window_size: int = 5,\n","                 negative_samples: int = 10,\n","                 min_token_freq: int = 20,\n","                 pruning_batch_size: int = 800,\n","                 embedding_update_frequency: int = 2,\n","                 # gensim parameters\n","                 gensim_workers: int = 4,\n","                 gensim_epochs: int = 5,\n","                 gensim_min_count: int = 1,\n","                 # early stopping params\n","                 fertility_target: float = 1.4,\n","                 fertility_tolerance: float = 0.05,\n","                 patience: int = 8,\n","                 min_improvement: float = 0.005):\n","\n","        self.vocab_size = vocab_size\n","        self.initial_vocab_multiplier = initial_vocab_multiplier\n","        self.max_token_length = max_token_length\n","        self.embedding_dim = embedding_dim\n","        self.window_size = window_size\n","        self.negative_samples = negative_samples\n","        self.min_token_freq = min_token_freq\n","        self.pruning_batch_size = pruning_batch_size\n","        self.embedding_update_frequency = embedding_update_frequency\n","\n","        # gensim parameters\n","        self.gensim_workers = gensim_workers\n","        self.gensim_epochs = gensim_epochs\n","        self.gensim_min_count = gensim_min_count\n","\n","        # early stopping parameters\n","        self.fertility_target = fertility_target\n","        self.fertility_tolerance = fertility_tolerance\n","        self.patience = patience\n","        self.min_improvement = min_improvement\n","\n","        # data structures\n","        self.vocabulary = {}\n","        self.inv_vocabulary = {}\n","        self.token_frequencies = Counter()\n","        self.embeddings = None\n","        self.context_embeddings = None\n","\n","        # yoruba-specific patterns\n","        self.yoruba_diacritics = set('áàéèíìóòúùṣẹọ́ẹ̀ọ́ọ̀ṣ́ṣ̀')\n","        self.yoruba_nasals = {'m', 'n', 'ṅ'}\n","\n","        # monitoring\n","        self.training_history = {\n","            'iteration': [],\n","            'vocab_size': [],\n","            'fertility': [],\n","            'skip_gram_loss': [],\n","            'ablation_loss': [],\n","            'tokens_per_char': [],\n","            'coverage': [],\n","            'diacritic_preservation': []\n","        }\n","        self.validation_lines = []\n","        self.total_lines = 0\n","        self.initial_vocab_size = None\n","\n","    def normalize_yoruba_text(self, text: str) -> str:\n","        text = text.strip()\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text\n","\n","    def analyze_diacritic_preservation(self, vocab: Dict[str, int], validation_lines: List[str]) -> float:\n","        \"\"\"calculate how well the vocabulary preserves tone marks\"\"\"\n","        diacritic_tokens = 0\n","        total_tokens_with_diacritics = 0\n","\n","        for token in vocab.keys():\n","            if any(char in self.yoruba_diacritics for char in token):\n","                diacritic_tokens += 1\n","\n","        for line in validation_lines[:100]:\n","            if any(char in self.yoruba_diacritics for char in line):\n","                total_tokens_with_diacritics += 1\n","\n","        if total_tokens_with_diacritics == 0:\n","            return 1.0\n","\n","        return min(diacritic_tokens / max(total_tokens_with_diacritics, 1), 1.0)\n","\n","    def count_corpus_lines(self, corpus_file: str) -> int:\n","        total = 0\n","        with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            for _ in f:\n","                total += 1\n","        return total\n","\n","    def load_validation_set(self, corpus_file: str, num_lines: int = 1000) -> List[str]:\n","        validation_start = max(int(self.total_lines * 0.9), self.total_lines - num_lines)\n","        lines = []\n","        with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            for i, line in enumerate(f):\n","                if i >= validation_start:\n","                    line = self.normalize_yoruba_text(line)\n","                    if line:\n","                        lines.append(line)\n","\n","        logger.info(f\"Loaded {len(lines)} validation lines (from line {validation_start})\")\n","        return lines\n","\n","    def initialize_vocabulary(self, corpus_file: str) -> Dict[str, int]:\n","        \"\"\"initialize vocabulary with frequency-based selection\"\"\"\n","        target_based_size = int(self.vocab_size * self.initial_vocab_multiplier)\n","        corpus_based_size = self.total_lines // 4\n","\n","        self.initial_vocab_size = min(\n","            max(target_based_size, self.vocab_size + 8000),\n","            target_based_size * 2\n","        )\n","\n","        logger.info(f\"Initializing vocabulary with size {self.initial_vocab_size} (target: {self.vocab_size})\")\n","\n","        if self.initial_vocab_size <= self.vocab_size:\n","            raise ValueError(f\"Initial vocab size {self.initial_vocab_size} must be > target {self.vocab_size}\")\n","\n","        ngram_counts = Counter()\n","        max_lines = int(self.total_lines * 0.9)\n","\n","        with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            for line_num, line in enumerate(f):\n","                if line_num >= max_lines:\n","                    break\n","                if line_num % 10000 == 0 and line_num > 0:\n","                    logger.info(f\"Processing line {line_num}...\")\n","\n","                line = self.normalize_yoruba_text(line)\n","                line = '▁' + line.replace(' ', '▁')\n","\n","                for n in range(1, min(len(line) + 1, self.max_token_length + 1)):\n","                    for i in range(len(line) - n + 1):\n","                        ngram = line[i:i+n]\n","                        boost = 1.2 if any(char in self.yoruba_diacritics for char in ngram) else 1.0\n","                        ngram_counts[ngram] += boost\n","\n","        current_threshold = self.min_token_freq\n","        while current_threshold > 5:\n","            filtered_ngrams = {\n","                ngram: count for ngram, count in ngram_counts.items()\n","                if count >= current_threshold\n","            }\n","            if len(filtered_ngrams) >= self.initial_vocab_size:\n","                break\n","            current_threshold -= 5\n","            logger.info(f\"Lowering frequency threshold to {current_threshold} to get more tokens\")\n","\n","        logger.info(f\"Found {len(filtered_ngrams)} n-grams with freq >= {current_threshold}\")\n","\n","        essential_tokens = ['<unk>', '<s>', '</s>', '<pad>', '▁']\n","\n","        for i in range(256):\n","            try:\n","                essential_tokens.append(chr(i))\n","            except:\n","                pass\n","\n","        essential_tokens.extend(list(self.yoruba_diacritics))\n","\n","        # build vocabulary\n","        vocabulary = {}\n","        token_id = 0\n","\n","        for token in essential_tokens:\n","            if token not in vocabulary:\n","                vocabulary[token] = token_id\n","                self.token_frequencies[token] = ngram_counts.get(token, 1)\n","                token_id += 1\n","\n","        sorted_ngrams = sorted(filtered_ngrams.items(),\n","                              key=lambda x: (any(char in self.yoruba_diacritics for char in x[0]), x[1]),\n","                              reverse=True)\n","\n","        for ngram, freq in sorted_ngrams:\n","            if ngram not in vocabulary:\n","                vocabulary[ngram] = token_id\n","                self.token_frequencies[ngram] = freq\n","                token_id += 1\n","\n","                if len(vocabulary) >= self.initial_vocab_size:\n","                    break\n","\n","        if len(vocabulary) <= self.vocab_size:\n","            all_ngrams = sorted(ngram_counts.items(), key=lambda x: -x[1])\n","            for ngram, freq in all_ngrams:\n","                if ngram not in vocabulary and freq >= 2:\n","                    vocabulary[ngram] = token_id\n","                    self.token_frequencies[ngram] = freq\n","                    token_id += 1\n","                    if len(vocabulary) >= self.initial_vocab_size:\n","                        break\n","\n","        self.vocabulary = vocabulary\n","        self.inv_vocabulary = {v: k for k, v in vocabulary.items()}\n","\n","        diacritic_tokens = sum(1 for token in vocabulary.keys()\n","                             if any(char in self.yoruba_diacritics for char in token))\n","        logger.info(f\"Initialized vocabulary with {len(vocabulary)} tokens\")\n","\n","        return vocabulary\n","\n","    def tokenize_with_vocabulary(self, text: str, vocab: Dict[str, int]) -> List[int]:\n","        text = self.normalize_yoruba_text(text)\n","        text = '▁' + text.replace(' ', '▁')\n","        tokens = []\n","        i = 0\n","\n","        while i < len(text):\n","            matched = False\n","            for length in range(min(self.max_token_length, len(text) - i), 0, -1):\n","                substr = text[i:i+length]\n","                if substr in vocab:\n","                    tokens.append(vocab[substr])\n","                    i += length\n","                    matched = True\n","                    break\n","\n","            if not matched:\n","                char = text[i]\n","                tokens.append(vocab.get(char, vocab.get('<unk>', 0)))\n","                i += 1\n","\n","        return tokens\n","\n","    def compute_fertility(self, vocab: Dict[str, int], validation_lines: List[str]) -> Dict[str, float]:\n","\n","        total_words = 0\n","        total_tokens = 0\n","        total_chars = 0\n","        covered_chars = 0\n","\n","        for line in validation_lines[:100]:\n","            words = line.split()\n","            if not words:\n","                continue\n","            total_words += len(words)\n","\n","            processed_line = '▁' + line.replace(' ', '▁')\n","            total_chars += len(processed_line)\n","\n","            tokens = self.tokenize_with_vocabulary(line, vocab)\n","            total_tokens += len(tokens)\n","\n","            for token_id in tokens:\n","                token = self.inv_vocabulary.get(token_id, '<unk>')\n","                if token != '<unk>':\n","                    covered_chars += len(token)\n","\n","        diacritic_preservation = self.analyze_diacritic_preservation(vocab, validation_lines)\n","\n","        metrics = {\n","            'fertility': total_tokens / max(total_words, 1),\n","            'tokens_per_char': total_tokens / max(total_chars, 1),\n","            'coverage': min(covered_chars / max(total_chars, 1), 1.0),\n","            'avg_tokens_per_line': total_tokens / max(min(len(validation_lines), 100), 1),\n","            'diacritic_preservation': diacritic_preservation\n","        }\n","\n","        return metrics\n","\n","    def compute_skip_gram_loss(self, token_ids: List[int], target_emb: np.ndarray, context_emb: np.ndarray) -> float:\n","        \"\"\"compute Skip-gram loss\"\"\"\n","\n","        if len(token_ids) < 2:\n","            return 0.0\n","\n","        loss = 0.0\n","        count = 0\n","\n","        for i, target_id in enumerate(token_ids):\n","            if target_id >= len(target_emb):\n","                continue\n","\n","            context_start = max(0, i - self.window_size)\n","            context_end = min(len(token_ids), i + self.window_size + 1)\n","\n","            for j in range(context_start, context_end):\n","                if i == j:\n","                    continue\n","\n","                context_id = token_ids[j]\n","                if context_id >= len(context_emb):\n","                    continue\n","\n","                # positive sample loss\n","                score = np.dot(target_emb[target_id], context_emb[context_id])\n","                loss -= np.log(sigmoid(score) + 1e-10)\n","                count += 1\n","\n","                # negative samples loss\n","                for _ in range(self.negative_samples):\n","                    neg_id = np.random.randint(0, len(context_emb))\n","                    score = np.dot(target_emb[target_id], context_emb[neg_id])\n","                    loss -= np.log(1 - sigmoid(score) + 1e-10)\n","                    count += 1\n","\n","        return loss / max(count, 1)\n","\n","    def train_embeddings_with_gensim(self, corpus_file: str, vocab: Dict[str, int]) -> Tuple[np.ndarray, np.ndarray, float]:\n","        \"\"\"train embeddings using Gensim Word2Vec\"\"\"\n","        logger.info(f\"Training embeddings with Gensim for Yoruba vocabulary of size {len(vocab)}\")\n","\n","        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False,\n","                                                suffix='.txt', encoding='utf-8')\n","\n","        max_training_lines = int(self.total_lines * 0.9)\n","        tokenized_lines = []\n","\n","        with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            for line_num, line in enumerate(f):\n","                if line_num >= max_training_lines:\n","                    break\n","                if line_num % 10000 == 0 and line_num > 0:\n","                    logger.info(f\"Tokenizing line {line_num} for Gensim...\")\n","\n","                line = self.normalize_yoruba_text(line)\n","                token_ids = self.tokenize_with_vocabulary(line, vocab)\n","                token_strings = [self.inv_vocabulary.get(tid, '<unk>') for tid in token_ids]\n","\n","                if token_strings:\n","                    temp_file.write(' '.join(token_strings) + '\\n')\n","                    if len(tokenized_lines) < 1000:\n","                        tokenized_lines.append(token_ids)\n","\n","        temp_file.close()\n","\n","        logger.info(\"Training Word2Vec model...\")\n","        start_time = time.time()\n","\n","        model = gensim.models.Word2Vec(\n","            corpus_file=temp_file.name,\n","            vector_size=self.embedding_dim,\n","            window=self.window_size,\n","            min_count=self.gensim_min_count,\n","            workers=self.gensim_workers,\n","            sg=1,\n","            negative=self.negative_samples,\n","            alpha=0.025,\n","            min_alpha=0.0001,\n","            epochs=self.gensim_epochs,\n","            seed=42\n","        )\n","\n","        logger.info(f\"Word2Vec training completed in {time.time() - start_time:.2f}s\")\n","\n","        vocab_size = len(vocab)\n","        target_embeddings = np.random.uniform(-0.5/self.embedding_dim, 0.5/self.embedding_dim,\n","                                             (vocab_size, self.embedding_dim))\n","        context_embeddings = np.random.uniform(-0.5/self.embedding_dim, 0.5/self.embedding_dim,\n","                                              (vocab_size, self.embedding_dim))\n","\n","        learned_tokens = 0\n","        for token, token_id in vocab.items():\n","            if token in model.wv:\n","                target_embeddings[token_id] = model.wv[token]\n","                if hasattr(model.wv, 'syn1neg') and model.wv.syn1neg is not None:\n","                    word_index = model.wv.key_to_index[token]\n","                    context_embeddings[token_id] = model.wv.syn1neg[word_index]\n","                else:\n","                    context_embeddings[token_id] = model.wv[token]\n","                learned_tokens += 1\n","\n","        logger.info(f\"Learned embeddings for {learned_tokens}/{vocab_size} tokens \"\n","                   f\"({learned_tokens/vocab_size*100:.1f}%)\")\n","\n","        total_loss = 0.0\n","        for token_ids in tokenized_lines[:100]:\n","            loss = self.compute_skip_gram_loss(token_ids, target_embeddings, context_embeddings)\n","            total_loss += loss\n","\n","        avg_loss = total_loss / max(len(tokenized_lines[:100]), 1)\n","        os.unlink(temp_file.name)\n","\n","        return target_embeddings, context_embeddings, avg_loss\n","\n","    def compute_ablation_scores(self, corpus_file: str, vocab: Dict[str, int], target_emb: np.ndarray,\n","                                context_emb: np.ndarray, sample_size: int = 15000) -> Tuple[Dict[str, float], float]:\n","        logger.info(f\"Computing ablation scores (sample size: {sample_size})...\")\n","\n","        token_contexts = defaultdict(list)\n","        total_loss = 0.0\n","        total_pairs = 0\n","\n","        max_lines = min(sample_size, int(self.total_lines * 0.9))\n","\n","        with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            lines_processed = 0\n","\n","            for line in f:\n","                if lines_processed >= max_lines:\n","                    break\n","\n","                line = self.normalize_yoruba_text(line)\n","                token_ids = self.tokenize_with_vocabulary(line, vocab)\n","\n","                for i in range(len(token_ids)):\n","                    target_id = token_ids[i]\n","                    if target_id >= len(self.inv_vocabulary):\n","                        continue\n","\n","                    target_token = self.inv_vocabulary[target_id]\n","\n","                    for j in range(max(0, i - self.window_size),\n","                                  min(len(token_ids), i + self.window_size + 1)):\n","                        if i != j:\n","                            context_id = token_ids[j]\n","                            if context_id >= len(context_emb):\n","                                continue\n","\n","                            token_contexts[target_token].append((target_id, context_id))\n","\n","                            score = np.dot(target_emb[target_id], context_emb[context_id])\n","                            total_loss -= np.log(sigmoid(score) + 1e-10)\n","                            total_pairs += 1\n","\n","                lines_processed += 1\n","\n","                if lines_processed % 5000 == 0:\n","                    logger.info(f\"  Processed {lines_processed}/{max_lines} lines for ablation scores\")\n","\n","        avg_loss = total_loss / max(total_pairs, 1)\n","\n","        ablation_scores = {}\n","\n","        for token, token_id in vocab.items():\n","            is_essential = token in ['<unk>', '<s>', '</s>', '<pad>', '▁']\n","            is_single_char = len(token) == 1\n","            is_protected_diacritic = (\n","                any(char in self.yoruba_diacritics for char in token) and\n","                (len(token) == 1 or self.token_frequencies.get(token, 0) > 300)\n","            )\n","\n","            if is_essential or is_single_char or is_protected_diacritic:\n","                ablation_scores[token] = float('-inf')\n","                continue\n","\n","            contexts = token_contexts.get(token, [])\n","\n","            if not contexts:\n","                ablation_scores[token] = float('inf')\n","                continue\n","\n","            likelihood_with = 0.0\n","            for target_id, context_id in contexts:\n","                score = np.dot(target_emb[target_id], context_emb[context_id])\n","                likelihood_with += np.log(sigmoid(score) + 1e-10)\n","\n","            likelihood_without = 0.0\n","            sample_contexts = random.sample(contexts, min(100, len(contexts)))\n","            for target_id, context_id in sample_contexts:\n","                similar_score = np.mean([\n","                    np.dot(target_emb[tid], context_emb[context_id])\n","                    for tid in range(min(5, len(target_emb)))\n","                    if tid != target_id\n","                ])\n","                likelihood_without += np.log(sigmoid(similar_score) + 1e-10)\n","\n","            if sample_contexts:\n","                likelihood_without *= len(contexts) / len(sample_contexts)\n","\n","            ablation_scores[token] = likelihood_without - likelihood_with\n","\n","        logger.info(f\"Computed ablation scores. Average loss: {avg_loss:.4f}\")\n","        return ablation_scores, avg_loss\n","\n","    def get_adaptive_pruning_size(self, metrics: Dict[str, float], current_vocab_size: int, base_pruning_size: int) -> int:\n","        \"\"\"Get adaptive pruning size based on current metrics\"\"\"\n","\n","        # start with base size\n","        pruning_multiplier = 1.0\n","\n","        if metrics['fertility'] > self.fertility_target + 0.3:\n","            pruning_multiplier = 1.2\n","        elif metrics['fertility'] < self.fertility_target - 0.15:\n","            pruning_multiplier = 0.7\n","\n","        if metrics['diacritic_preservation'] < 0.7:\n","            pruning_multiplier *= 0.6\n","        elif metrics['diacritic_preservation'] > 0.85:\n","            pruning_multiplier *= 1.1\n","\n","        # slow down when approaching target\n","        remaining_to_target = current_vocab_size - self.vocab_size\n","        if remaining_to_target < 3000:\n","            pruning_multiplier *= 0.5\n","        elif remaining_to_target < 8000:\n","            pruning_multiplier *= 0.8\n","\n","        tokens_to_remove = int(base_pruning_size * pruning_multiplier)\n","\n","        # never remove more than what's needed to reach target\n","        tokens_to_remove = min(tokens_to_remove, remaining_to_target)\n","\n","        # minimum removal when far from target to ensure progress\n","        if remaining_to_target > 10000 and tokens_to_remove < 100:\n","            tokens_to_remove = 100\n","\n","        return tokens_to_remove\n","\n","    def prune_vocabulary(self, vocab: Dict[str, int], ablation_scores: Dict[str, float], num_to_remove: int) -> Dict[str, int]:\n","\n","        if num_to_remove <= 0:\n","            return vocab.copy()\n","\n","        # reduce tokens to remove if close to target\n","        current_vocab_size = len(vocab)\n","        remaining_to_target = current_vocab_size - self.vocab_size\n","\n","        if remaining_to_target < 5000:\n","            num_to_remove = min(num_to_remove, max(50, remaining_to_target // 10))\n","\n","        scored_tokens = [(score, token) for token, score in ablation_scores.items()\n","                        if score != float('-inf')]\n","        scored_tokens.sort(reverse=True)\n","\n","        tokens_to_remove = set()\n","\n","        for score, token in scored_tokens:\n","            if len(tokens_to_remove) >= num_to_remove:\n","                break\n","\n","            is_essential = token in ['<unk>', '<s>', '</s>', '<pad>', '▁']\n","            is_single_char = len(token) == 1\n","            is_high_value_diacritic = (\n","                any(char in self.yoruba_diacritics for char in token) and\n","                (len(token) == 1 or self.token_frequencies.get(token, 0) > 200)\n","            )\n","\n","            if not (is_essential or is_single_char or is_high_value_diacritic):\n","                tokens_to_remove.add(token)\n","\n","        # if still not enough tokens, be more selective rather than aggressive\n","        if len(tokens_to_remove) < num_to_remove:\n","            remaining_needed = num_to_remove - len(tokens_to_remove)\n","            logger.info(f\"Only found {len(tokens_to_remove)} safe tokens to remove out of {num_to_remove} needed\")\n","\n","            # only remove a fraction of what's still needed to avoid being too aggressive\n","            additional_to_remove = min(remaining_needed, remaining_needed // 2)\n","\n","            for score, token in scored_tokens:\n","                if len(tokens_to_remove) >= len(tokens_to_remove) + additional_to_remove:\n","                    break\n","                if token not in tokens_to_remove:\n","                    is_absolutely_essential = (\n","                        token in ['<unk>', '<s>', '</s>', '<pad>', '▁'] or\n","                        (len(token) == 1 and token in self.yoruba_diacritics) or\n","                        self.token_frequencies.get(token, 0) > 500\n","                    )\n","                    if not is_absolutely_essential:\n","                        tokens_to_remove.add(token)\n","\n","        new_vocab = {}\n","        new_id = 0\n","\n","        for token in sorted(vocab.keys()):\n","            if token not in tokens_to_remove:\n","                new_vocab[token] = new_id\n","                new_id += 1\n","\n","        logger.info(f\"Pruned vocabulary from {len(vocab)} to {len(new_vocab)} tokens\")\n","\n","        if len(new_vocab) == 0:\n","            logger.error(\"Created empty vocabulary. Returning original vocabulary.\")\n","            return vocab.copy()\n","\n","        return new_vocab\n","\n","    def should_stop_early(self) -> Tuple[bool, str]:\n","\n","        if len(self.training_history['iteration']) < 2:\n","            return False, \"\"\n","\n","        current_fertility = self.training_history['fertility'][-1]\n","        current_vocab_size = self.training_history['vocab_size'][-1]\n","        current_diacritic = self.training_history['diacritic_preservation'][-1]\n","        current_iteration = len(self.training_history['iteration'])\n","\n","        # add minimum iteration requirement - don't stop too early\n","        MIN_ITERATIONS = 8\n","        if current_iteration < MIN_ITERATIONS:\n","            return False, \"\"\n","\n","        # primary goal is vocabulary size - only stop when much closer to target\n","        if current_vocab_size <= self.vocab_size * 1.02:\n","            return True, f\"Very close to target vocab size: {current_vocab_size} (target: {self.vocab_size})\"\n","\n","        # only stop on fertility if also reasonably close to vocab target\n","        if (abs(current_fertility - self.fertility_target) <= self.fertility_tolerance and\n","            current_vocab_size <= self.vocab_size * 1.15):\n","            if current_diacritic > 0.7:\n","                return True, f\"Reached target fertility: {current_fertility:.3f} with reasonable vocab size\"\n","\n","        PATIENCE = 8\n","        if len(self.training_history['fertility']) >= PATIENCE:\n","            recent_fertilities = self.training_history['fertility'][-PATIENCE:]\n","\n","            # stop if fertility is getting much worse AND not close to vocab target\n","            if all(recent_fertilities[i] > recent_fertilities[i-1] + 0.015 for i in range(1, len(recent_fertilities))):\n","                if current_fertility > self.fertility_target + 0.4 and current_vocab_size > self.vocab_size * 1.3:\n","                    return True, f\"Fertility increasing too much: {current_fertility:.3f} and vocab still large\"\n","\n","            # stop on no improvement if very close to target vocab size\n","            fertility_improvement = abs(recent_fertilities[0] - recent_fertilities[-1])\n","            if fertility_improvement < self.min_improvement:\n","                if current_vocab_size <= self.vocab_size * 1.05:\n","                    return True, f\"No fertility improvement in {PATIENCE} iterations and close to target\"\n","\n","        MAX_ITERATIONS = 50\n","        if current_iteration >= MAX_ITERATIONS:\n","            return True, f\"Reached maximum iterations ({MAX_ITERATIONS})\"\n","\n","        return False, \"\"\n","\n","    def train(self, corpus_file: str, output_dir: str) -> str:\n","        logger.info(\"=\" * 70)\n","        logger.info(\"Starting SaGe tokenizer training\")\n","        logger.info(f\"Target vocabulary size: {self.vocab_size}\")\n","        logger.info(f\"Initial vocabulary multiplier: {self.initial_vocab_multiplier}\")\n","        logger.info(f\"Target fertility: {self.fertility_target} ± {self.fertility_tolerance}\")\n","        logger.info(\"=\" * 70)\n","\n","        self.total_lines = self.count_corpus_lines(corpus_file)\n","        logger.info(f\"Yoruba corpus has {self.total_lines} lines\")\n","\n","        self.validation_lines = self.load_validation_set(corpus_file)\n","\n","        current_vocab = self.initialize_vocabulary(corpus_file)\n","\n","        initial_metrics = self.compute_fertility(current_vocab, self.validation_lines)\n","        logger.info(f\"Initial metrics: Fertility={initial_metrics['fertility']:.3f}, \"\n","                   f\"Coverage={initial_metrics['coverage']:.3f}, \"\n","\n","        # training loop\n","        iteration = 0\n","        embeddings_trained = False\n","        best_fertility_distance = float('inf')\n","        best_vocab = current_vocab.copy()\n","\n","        while len(current_vocab) > self.vocab_size:\n","            iteration += 1\n","            logger.info(f\"\\n--- Yoruba Iteration {iteration} ---\")\n","            logger.info(f\"Current vocabulary size: {len(current_vocab)}\")\n","\n","            # train embeddings with Gensim\n","            if not embeddings_trained or iteration % self.embedding_update_frequency == 0:\n","                self.embeddings, self.context_embeddings, skip_gram_loss = \\\n","                    self.train_embeddings_with_gensim(corpus_file, current_vocab)\n","                embeddings_trained = True\n","            else:\n","                skip_gram_loss = 0.0\n","                num_val_lines = min(100, len(self.validation_lines))\n","                for line in self.validation_lines[:num_val_lines]:\n","                    token_ids = self.tokenize_with_vocabulary(line, current_vocab)\n","                    skip_gram_loss += self.compute_skip_gram_loss(\n","                        token_ids, self.embeddings, self.context_embeddings\n","                    )\n","                skip_gram_loss /= max(num_val_lines, 1)\n","\n","            # Compute ablation scores\n","            ablation_scores, ablation_loss = self.compute_ablation_scores(\n","                corpus_file, current_vocab,\n","                self.embeddings, self.context_embeddings,\n","                sample_size=min(15000, self.total_lines)\n","            )\n","\n","            metrics = self.compute_fertility(current_vocab, self.validation_lines)\n","\n","            self.training_history['iteration'].append(iteration)\n","            self.training_history['vocab_size'].append(len(current_vocab))\n","            self.training_history['fertility'].append(metrics['fertility'])\n","            self.training_history['skip_gram_loss'].append(skip_gram_loss)\n","            self.training_history['ablation_loss'].append(ablation_loss)\n","            self.training_history['tokens_per_char'].append(metrics['tokens_per_char'])\n","            self.training_history['coverage'].append(metrics['coverage'])\n","            self.training_history['diacritic_preservation'].append(metrics['diacritic_preservation'])\n","\n","            logger.info(f\"Metrics: Fertility={metrics['fertility']:.3f}, \"\n","                       f\"Coverage={metrics['coverage']:.3f}, \"\n","                       f\"Diacritic preservation={metrics['diacritic_preservation']:.3f}, \"\n","                       f\"Skip-gram Loss={skip_gram_loss:.4f}\")\n","\n","            current_fertility_distance = abs(metrics['fertility'] - self.fertility_target)\n","\n","            if (current_fertility_distance < best_fertility_distance or\n","                (current_fertility_distance == best_fertility_distance and\n","                 metrics['diacritic_preservation'] > 0.8)):\n","                best_fertility_distance = current_fertility_distance\n","                best_vocab = current_vocab.copy()\n","                logger.info(f\"New best fertility distance: {current_fertility_distance:.3f}\")\n","\n","            should_stop, reason = self.should_stop_early()\n","            if should_stop:\n","                logger.info(f\"\\nEarly stopping: {reason}\")\n","                break\n","\n","            tokens_to_remove = self.get_adaptive_pruning_size(\n","                metrics, len(current_vocab), self.pruning_batch_size\n","            )\n","\n","            max_removable = len(current_vocab) - self.vocab_size\n","            if max_removable <= 0:\n","                logger.info(\"Already at or below target vocabulary size\")\n","                break\n","\n","            tokens_to_remove = min(tokens_to_remove, max_removable)\n","\n","            if tokens_to_remove == 0 and max_removable > 1000:\n","                tokens_to_remove = min(50, max_removable)\n","                logger.info(f\"Forcing minimum progress: removing {tokens_to_remove} tokens\")\n","\n","            # prune vocabulary\n","            current_vocab = self.prune_vocabulary(\n","                current_vocab, ablation_scores, tokens_to_remove\n","            )\n","\n","            # update internal state\n","            self.vocabulary = current_vocab\n","            self.inv_vocabulary = {v: k for k, v in current_vocab.items()}\n","\n","            logger.info(f\"Pruned {tokens_to_remove} tokens. New size: {len(current_vocab)}\")\n","            logger.info(f\"Remaining to target: {len(current_vocab) - self.vocab_size}\")\n","\n","        final_metrics = self.compute_fertility(current_vocab, self.validation_lines)\n","        logger.info(f\"\\nYoruba tokenizer training completed!\")\n","        logger.info(f\"Training iterations: {iteration}\")\n","        logger.info(f\"Final vocabulary size: {len(current_vocab)}\")\n","        logger.info(f\"Final fertility: {final_metrics['fertility']:.3f}\")\n","        logger.info(f\"Final coverage: {final_metrics['coverage']:.3f}\")\n","        logger.info(f\"Final diacritic preservation: {final_metrics['diacritic_preservation']:.3f}\")\n","\n","        os.makedirs(output_dir, exist_ok=True)\n","        history_file = os.path.join(output_dir, 'yoruba_training_history.json')\n","        with open(history_file, 'w') as f:\n","            json.dump(self.training_history, f, indent=2)\n","\n","        model_file = self.convert_to_sentencepiece(current_vocab, corpus_file, output_dir)\n","\n","        return model_file\n","\n","    def convert_to_sentencepiece(self, final_vocab: Dict[str, int], corpus_file: str, output_dir: str) -> str:\n","        logger.info(f\"\\nConverting tokenizer to SentencePiece format...\")\n","\n","        max_reasonable_vocab = min(len(final_vocab), max(8000, self.total_lines // 8))\n","\n","        if len(final_vocab) > max_reasonable_vocab:\n","            logger.warning(f\"Vocabulary size {len(final_vocab)} too large for corpus, reducing to {max_reasonable_vocab}\")\n","\n","            sorted_tokens = sorted(final_vocab.keys(), key=lambda x: self.token_frequencies.get(x, 0), reverse=True)\n","\n","            reduced_vocab = {}\n","            for i, token in enumerate(sorted_tokens[:max_reasonable_vocab]):\n","                reduced_vocab[token] = i\n","\n","            final_vocab = reduced_vocab\n","            logger.info(f\"Reduced vocabulary to {len(final_vocab)} tokens\")\n","\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        vocab_file = os.path.join(output_dir, \"yoruba_vocab.txt\")\n","        total_freq = sum(self.token_frequencies.values())\n","\n","        with open(vocab_file, 'w', encoding='utf-8') as f:\n","            sorted_tokens = sorted(final_vocab.keys(),\n","                                 key=lambda x: (\n","                                     not any(char in self.yoruba_diacritics for char in x),\n","                                     -self.token_frequencies.get(x, 0)\n","                                 ))\n","\n","            for token in sorted_tokens:\n","                freq = self.token_frequencies.get(token, 1)\n","                score = np.log(freq / max(total_freq, 1))\n","                if any(char in self.yoruba_diacritics for char in token):\n","                    score += 0.05\n","                f.write(f\"{token}\\t{score}\\n\")\n","\n","        try:\n","            with tempfile.TemporaryDirectory(prefix='sage_yoruba_', dir='/tmp') as temp_dir:\n","                temp_corpus = os.path.join(temp_dir, 'yoruba_corpus.txt')\n","\n","                line_count = 0\n","                with open(corpus_file, 'r', encoding='utf-8') as src, \\\n","                     open(temp_corpus, 'w', encoding='utf-8') as dst:\n","                    for line in src:\n","                        normalized = self.normalize_yoruba_text(line)\n","                        if normalized:\n","                            dst.write(normalized + '\\n')\n","                            line_count += 1\n","\n","                temp_model_prefix = os.path.join(temp_dir, 'yoruba_tokenizer')\n","\n","                adaptive_vocab_size = min(len(final_vocab), max(6000, line_count // 10))\n","                logger.info(f\"Using adaptive vocabulary size: {adaptive_vocab_size}\")\n","\n","                spm.SentencePieceTrainer.train(\n","                    input=temp_corpus,\n","                    model_prefix=temp_model_prefix,\n","                    vocab_size=adaptive_vocab_size,\n","                    model_type='unigram',\n","                    character_coverage=0.9995,\n","                    normalization_rule_name='nmt_nfkc_cf',\n","                    add_dummy_prefix=False,\n","                    unk_id=0,\n","                    bos_id=1,\n","                    eos_id=2,\n","                    pad_id=3,\n","                    input_sentence_size=min(20000, line_count),\n","                    shuffle_input_sentence=True,\n","                    num_threads=8,\n","                    user_defined_symbols=['á', 'à', 'é', 'è', 'í', 'ì', 'ó', 'ò', 'ú', 'ù', 'ṣ', 'ẹ', 'ọ']\n","                )\n","\n","                for ext in ['.model', '.vocab']:\n","                    src = f\"{temp_model_prefix}{ext}\"\n","                    if os.path.exists(src):\n","                        dst = os.path.join(output_dir, f\"yoruba_tokenizer{ext}\")\n","                        shutil.copy(src, dst)\n","\n","        except Exception as e:\n","            logger.error(f\"Error converting tokenizer to SentencePiece: {e}\")\n","            return None\n","\n","        self.create_huggingface_configs(output_dir, len(final_vocab))\n","\n","        model_file = os.path.join(output_dir, \"yoruba_tokenizer.model\")\n","        if os.path.exists(model_file):\n","            logger.info(f\"Created SentencePiece model: {model_file}\")\n","        else:\n","            logger.error(\"Failed to create SentencePiece model\")\n","\n","        return model_file\n","\n","    def create_huggingface_configs(self, output_dir: str, vocab_size: int):\n","        configs = {\n","            \"tokenizer_config.json\": {\n","                \"tokenizer_class\": \"LlamaTokenizer\",\n","                \"model_max_length\": 4096,\n","                \"padding_side\": \"left\",\n","                \"bos_token\": \"<s>\",\n","                \"eos_token\": \"</s>\",\n","                \"unk_token\": \"<unk>\",\n","                \"pad_token\": \"<pad>\",\n","                \"add_bos_token\": True,\n","                \"add_eos_token\": False,\n","                \"clean_up_tokenization_spaces\": False,\n","                \"legacy\": False,\n","                \"name_or_path\": \"yoruba_sage_tokenizer\",\n","                \"special_tokens_map_file\": \"special_tokens_map.json\"\n","            },\n","            \"special_tokens_map.json\": {\n","                \"bos_token\": \"<s>\",\n","                \"eos_token\": \"</s>\",\n","                \"unk_token\": \"<unk>\",\n","                \"pad_token\": \"<pad>\"\n","            }\n","        }\n","\n","        for filename, config in configs.items():\n","            with open(os.path.join(output_dir, filename), 'w') as f:\n","                json.dump(config, f, indent=2)\n","\n","\n","def run_sage(corpus_file: str, output_dir: str):\n","\n","    tokenizer = YorubaMonitoredSaGeTokenizer(\n","        vocab_size=10000,\n","        initial_vocab_multiplier=1.5,\n","        max_token_length=5,\n","        embedding_dim=50,\n","        window_size=5,\n","        negative_samples=10,\n","        min_token_freq=20,\n","        pruning_batch_size=1500,\n","        embedding_update_frequency=2,\n","        gensim_workers=4,\n","        gensim_epochs=5,\n","        gensim_min_count=1,\n","        fertility_target=1.4,\n","        fertility_tolerance=0.05,\n","        patience=8,\n","        min_improvement=0.005\n","    )\n","\n","    model_file = tokenizer.train(corpus_file, output_dir)\n","\n","    # test tokenizer\n","    if model_file and os.path.exists(model_file):\n","        sp = spm.SentencePieceProcessor()\n","        sp.load(model_file)\n","\n","        logger.info(f\"\\nFinal Yoruba Statistics:\")\n","        logger.info(f\"   Vocabulary size: {sp.vocab_size()}\")\n","\n","        if tokenizer.training_history['iteration']:\n","            logger.info(f\"   Training iterations: {len(tokenizer.training_history['iteration'])}\")\n","            logger.info(f\"   Final fertility: {tokenizer.training_history['fertility'][-1]:.3f}\")\n","            logger.info(f\"   Final coverage: {tokenizer.training_history['coverage'][-1]:.3f}\")\n","            logger.info(f\"   Final diacritic preservation: {tokenizer.training_history['diacritic_preservation'][-1]:.3f}\")\n","\n","        test_sentences = [\n","            \"Pẹlẹ o, báwo ni ẹ ṣe wa\",  # Hello, how are you\n","            \"Mo fẹ́ kọ́ èdè Yorùbá\",     # I want to learn Yoruba language\n","            \"Ilẹ̀ wa lọ́wọ́ wa\",          # Our land is in our hands\n","            \"Ẹ̀kọ́ ní koko ayé\",        # Education is the essence of life\n","            \"Òrèwá lórúkọ mi\"          # My name is Orewa\n","        ]\n","\n","        logger.info(\"\\nYoruba Tokenization Examples:\")\n","        for sentence in test_sentences:\n","            tokens = sp.encode_as_pieces(sentence)\n","            logger.info(f\"   '{sentence}' -> {tokens[:10]}...\" if len(tokens) > 10 else f\"   '{sentence}' -> {tokens}\")\n","\n","    return model_file\n","\n","if __name__ == \"__main__\":\n","    corpus_file = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/dataset/yo_train.txt\"\n","    output_dir = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/tokenizers/sage\"\n","\n","    model = run_sage(corpus_file, output_dir)\n","\n","    if model:\n","        logger.info(f\"\\nSaGe model saved to: {output_dir}\")"],"metadata":{"id":"xQH1_gg_4VHk"},"execution_count":null,"outputs":[]}]}