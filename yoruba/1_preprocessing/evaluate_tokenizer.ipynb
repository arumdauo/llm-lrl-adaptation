{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPlgDjA4odWz84Q7gZGkPHz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jaz2Xk9E90rt"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install -q sentencepiece transformers datasets huggingface_hub"],"metadata":{"id":"Bp97YLmz96WB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import time\n","import random\n","import logging\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from collections import Counter, defaultdict\n","from typing import List, Dict, Union, Tuple, Optional\n","import unicodedata\n","import re\n","import getpass\n","from transformers import AutoTokenizer\n","import sentencepiece as spm\n","\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","class PickyBPETokenizer:\n","\n","    def __init__(self, model_path: str):\n","        self.model_path = model_path\n","        self.model = self._load_model()\n","\n","    def _load_model(self):\n","        try:\n","            if self.model_path.endswith('tokenizer.json'):\n","                try:\n","                    from transformers import PreTrainedTokenizerFast\n","                    tokenizer = PreTrainedTokenizerFast(tokenizer_file=self.model_path)\n","                    logger.info(f\"Loaded as HuggingFace tokenizer: {self.model_path}\")\n","                    return tokenizer\n","                except Exception as e:\n","                    logger.warning(f\"Failed to load as HuggingFace tokenizer: {e}\")\n","            with open(self.model_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            logger.info(f\"Loaded JSON data with keys: {list(data.keys())}\")\n","\n","            if 'model' in data and 'vocab' in data['model']:\n","                vocab = data['model']['vocab']\n","                tokens_data = []\n","                for token_str, token_id in vocab.items():\n","                    tokens_data.append({\n","                        'id': token_id,\n","                        'str': token_str,\n","                        'freq': 0,\n","                        'special': token_str in ['<pad>', '<unk>', '<s>', '</s>', '[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n","                        'present': True\n","                    })\n","            elif 'tokens' in data:\n","                tokens_data = data['tokens']\n","            elif 'vocab' in data:\n","                vocab = data['vocab']\n","                tokens_data = []\n","                for token_str, token_id in vocab.items():\n","                    tokens_data.append({\n","                        'id': token_id,\n","                        'str': token_str,\n","                        'freq': 0,\n","                        'special': token_str in ['<pad>', '<unk>', '<s>', '</s>'],\n","                        'present': True\n","                    })\n","            else:\n","                tokens_data = []\n","                for i, (token_str, token_id) in enumerate(data.items()):\n","                    if isinstance(token_id, int):\n","                        tokens_data.append({\n","                            'id': token_id,\n","                            'str': token_str,\n","                            'freq': 0,\n","                            'special': token_str in ['<pad>', '<unk>', '<s>', '</s>'],\n","                            'present': True\n","                        })\n","\n","            class SimplePickyBPEModel:\n","                def __init__(self, tokens_data):\n","                    self.id2token = {}\n","                    self.str2token = {}\n","\n","                    for token in tokens_data:\n","                        if isinstance(token, dict):\n","                            token_id = token['id']\n","                            token_str = token['str']\n","                        else:\n","                            continue\n","\n","                        self.id2token[token_id] = token_str\n","                        self.str2token[token_str] = token_id\n","\n","                    self.unk_id = self.str2token.get('<unk>', self.str2token.get('[UNK]', 1))\n","                    logger.info(f\"Created model with {len(self.id2token)} tokens, UNK ID: {self.unk_id}\")\n","\n","                def encode(self, text: str) -> List[int]:\n","                    \"\"\"Simple word + character-level encoding\"\"\"\n","                    tokens = []\n","                    words = text.split()\n","\n","                    for word in words:\n","                        # Try word-level first\n","                        if word in self.str2token:\n","                            tokens.append(self.str2token[word])\n","                        else:\n","                            # Fall back to character-level\n","                            for char in word:\n","                                if char in self.str2token:\n","                                    tokens.append(self.str2token[char])\n","                                else:\n","                                    tokens.append(self.unk_id)\n","\n","                    return tokens\n","\n","                def tokenize(self, text: str) -> List[str]:\n","                    \"\"\"Tokenize text to token strings\"\"\"\n","                    ids = self.encode(text)\n","                    return [self.id2token.get(token_id, '<unk>') for token_id in ids]\n","\n","            return SimplePickyBPEModel(tokens_data)\n","\n","        except Exception as e:\n","            logger.error(f\"Error loading Picky BPE model: {e}\")\n","            return None\n","\n","    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n","        \"\"\"Encode text to token IDs\"\"\"\n","        if self.model is None:\n","            return []\n","        return self.model.encode(text)\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        \"\"\"Tokenize text to token strings\"\"\"\n","        if self.model is None:\n","            return []\n","        return self.model.tokenize(text)\n","\n","    @property\n","    def unk_token_id(self) -> int:\n","        \"\"\"Get UNK token ID\"\"\"\n","        if hasattr(self.model, 'unk_id'):\n","            return self.model.unk_id\n","        elif hasattr(self.model, 'unk_token_id'):\n","            return self.model.unk_token_id\n","        return 1\n","\n","class SentencePieceTokenizer:\n","\n","    def __init__(self, model_path: str):\n","        self.model_path = model_path\n","        self.sp = spm.SentencePieceProcessor()\n","        self.load_model()\n","\n","    def load_model(self):\n","        try:\n","            if not os.path.exists(self.model_path):\n","                logger.error(f\"SentencePiece model file not found: {self.model_path}\")\n","                self.sp = None\n","                return\n","\n","            self.sp.load(self.model_path)\n","\n","            test_result = self.sp.encode(\"test\", out_type=int)\n","            logger.info(f\"SentencePiece model loaded. Test encoding: {test_result}\")\n","\n","        except Exception as e:\n","            logger.error(f\"Error loading SentencePiece model: {e}\")\n","            self.sp = None\n","\n","    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n","        \"\"\"encode text to token IDs\"\"\"\n","        if self.sp is None:\n","            return []\n","        try:\n","            return self.sp.encode(text, out_type=int)\n","        except Exception as e:\n","            logger.warning(f\"Error encoding text: {e}\")\n","            return []\n","\n","    def encode_as_ids(self, text: str) -> List[int]:\n","        \"\"\"encode text to token IDs\"\"\"\n","        return self.encode(text)\n","\n","    def encode_as_pieces(self, text: str) -> List[str]:\n","        \"\"\"encode text to token pieces\"\"\"\n","        if self.sp is None:\n","            return []\n","        try:\n","            return self.sp.encode(text, out_type=str)\n","        except Exception as e:\n","            logger.warning(f\"Error encoding text to pieces: {e}\")\n","            return []\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        return self.encode_as_pieces(text)\n","\n","    @property\n","    def unk_token_id(self) -> int:\n","        if self.sp is None:\n","            return 1\n","        try:\n","            return self.sp.unk_id()\n","        except:\n","            return 1\n","\n","class SimpleBPETokenizer:\n","\n","    def __init__(self, vocab_path: str, merges_path: str = None):\n","        self.vocab_path = vocab_path\n","        self.merges_path = merges_path\n","        self.vocab = {}\n","        self.merges = []\n","        self.load_model()\n","\n","    def load_model(self):\n","        try:\n","            if self.vocab_path.endswith('.json'):\n","                with open(self.vocab_path, 'r', encoding='utf-8') as f:\n","                    self.vocab = json.load(f)\n","            else:\n","                with open(self.vocab_path, 'r', encoding='utf-8') as f:\n","                    for i, line in enumerate(f):\n","                        token = line.strip().split('\\t')[0] if '\\t' in line else line.strip()\n","                        self.vocab[token] = i\n","\n","            if self.merges_path and os.path.exists(self.merges_path):\n","                with open(self.merges_path, 'r', encoding='utf-8') as f:\n","                    for line in f:\n","                        if line.strip() and not line.startswith('#'):\n","                            parts = line.strip().split()\n","                            if len(parts) >= 2:\n","                                self.merges.append((parts[0], parts[1]))\n","\n","            logger.info(f\"Loaded BPE model with {len(self.vocab)} tokens and {len(self.merges)} merges\")\n","        except Exception as e:\n","            logger.error(f\"Error loading BPE model: {e}\")\n","\n","    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n","        \"\"\"encode text to token IDs\"\"\"\n","        tokens = self.tokenize(text)\n","        return [self.vocab.get(token, self.vocab.get('<unk>', 1)) for token in tokens]\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        if not self.vocab:\n","            return list(text)\n","        words = text.split()\n","        tokens = []\n","\n","        for word in words:\n","            word_tokens = self._tokenize_word(word)\n","            tokens.extend(word_tokens)\n","\n","        return tokens\n","\n","    def _tokenize_word(self, word: str) -> List[str]:\n","        if word in self.vocab:\n","            return [word]\n","\n","        tokens = []\n","        for char in word:\n","            if char in self.vocab:\n","                tokens.append(char)\n","            else:\n","                tokens.append('<unk>')\n","\n","        return tokens\n","\n","    @property\n","    def unk_token_id(self) -> int:\n","        \"\"\"get UNK token ID\"\"\"\n","        return self.vocab.get('<unk>', 1)\n","\n","class TokenizerEvaluator:\n","\n","    def __init__(self, test_texts: List[str]):\n","        self.test_texts = test_texts\n","\n","    def calculate_fertility(self, tokenizer, texts: List[str]) -> float:\n","        \"\"\"calculate average tokens per word\"\"\"\n","        total_tokens = 0\n","        total_words = 0\n","\n","        for text in texts:\n","            words = text.split()\n","            total_words += len(words)\n","\n","            try:\n","                tokens = self._get_tokens(tokenizer, text)\n","                total_tokens += len(tokens)\n","            except Exception as e:\n","                logger.warning(f\"Error tokenizing text with {type(tokenizer)}: {e}\")\n","                continue\n","\n","        return total_tokens / total_words if total_words > 0 else 0\n","\n","    def calculate_compression_ratio(self, tokenizer, texts: List[str]) -> float:\n","        \"\"\"calculate compression ratio (chars per token)\"\"\"\n","        total_chars = 0\n","        total_tokens = 0\n","\n","        for text in texts:\n","            total_chars += len(text)\n","\n","            try:\n","                tokens = self._get_tokens(tokenizer, text)\n","                total_tokens += len(tokens)\n","            except Exception as e:\n","                logger.warning(f\"Error tokenizing text with {type(tokenizer)}: {e}\")\n","                continue\n","\n","        return total_chars / total_tokens if total_tokens > 0 else 0\n","\n","    def calculate_coverage(self, tokenizer, texts: List[str]) -> float:\n","        \"\"\"calculate vocabulary coverage (1 - UNK ratio)\"\"\"\n","        total_tokens = 0\n","        unk_tokens = 0\n","\n","        for text in texts:\n","            try:\n","                tokens = self._get_tokens(tokenizer, text)\n","                total_tokens += len(tokens)\n","\n","                # count UNK tokens\n","                if hasattr(tokenizer, 'unk_token_id'):\n","                    unk_id = tokenizer.unk_token_id\n","                    unk_tokens += sum(1 for t in tokens if t == unk_id)\n","                elif isinstance(tokens[0], str) if tokens else False:\n","                    unk_tokens += sum(1 for t in tokens if '<unk>' in str(t).lower() or '[unk]' in str(t).lower())\n","                else:\n","                    unk_tokens += sum(1 for t in tokens if t in {0, 1})\n","\n","            except Exception as e:\n","                logger.warning(f\"Error calculating coverage with {type(tokenizer)}: {e}\")\n","                continue\n","\n","        coverage = 1 - (unk_tokens / total_tokens) if total_tokens > 0 else 0\n","        return max(0, coverage)\n","\n","    def _get_tokens(self, tokenizer, text: str) -> List:\n","        if hasattr(tokenizer, 'encode_as_ids') and hasattr(tokenizer, 'sp'):\n","            return tokenizer.encode_as_ids(text)\n","        elif hasattr(tokenizer, 'encode') and hasattr(tokenizer, 'model') and hasattr(tokenizer.model, 'encode'):\n","            return tokenizer.encode(text)\n","        elif hasattr(tokenizer, 'encode'):\n","            try:\n","                return tokenizer.encode(text, add_special_tokens=False)\n","            except TypeError:\n","                return tokenizer.encode(text)\n","        elif hasattr(tokenizer, 'tokenize'):\n","            return tokenizer.tokenize(text)\n","        else:\n","            raise ValueError(f\"Unknown tokenizer interface: {type(tokenizer)}\")\n","\n","    def calculate_token_length_distribution(self, tokenizer, texts: List[str]) -> Dict[str, float]:\n","\n","        token_lengths = []\n","        sample_texts = texts[:100]\n","\n","        for text in sample_texts:\n","            try:\n","                if hasattr(tokenizer, 'encode_as_pieces'):\n","                    tokens = tokenizer.encode_as_pieces(text)\n","                elif hasattr(tokenizer, 'tokenize'):\n","                    tokens = tokenizer.tokenize(text)\n","                else:\n","                    continue\n","\n","                for token in tokens:\n","                    token_str = str(token).replace('▁', '').replace('Ġ', '')\n","                    token_lengths.append(len(token_str))\n","            except Exception as e:\n","                continue\n","\n","        if not token_lengths:\n","            return {'mean_length': 0, 'std_length': 0}\n","\n","        return {\n","            'mean_length': np.mean(token_lengths),\n","            'std_length': np.std(token_lengths)\n","        }\n","\n","    def evaluate_tokenizer(self, tokenizer, name: str) -> Dict[str, float]:\n","        logger.info(f\"Evaluating tokenizer: {name}\")\n","\n","        eval_texts = self.test_texts[:500] if len(self.test_texts) > 500 else self.test_texts\n","\n","        metrics = {\n","            'fertility': self.calculate_fertility(tokenizer, eval_texts),\n","            'compression_ratio': self.calculate_compression_ratio(tokenizer, eval_texts),\n","            'coverage': self.calculate_coverage(tokenizer, eval_texts)\n","        }\n","\n","        length_stats = self.calculate_token_length_distribution(tokenizer, eval_texts)\n","        metrics.update(length_stats)\n","\n","        metrics['composite_score'] = (\n","            metrics['compression_ratio'] * metrics['coverage'] / metrics['fertility']\n","            if metrics['fertility'] > 0 else 0\n","        )\n","\n","        logger.info(f\"Metrics for {name}: {metrics}\")\n","        return metrics\n","\n","class YorubaCorpusProcessor:\n","    \"\"\"handles Yoruba corpus loading and preprocessing\"\"\"\n","\n","    def __init__(self, max_sentences: int = 5000):\n","        self.max_sentences = max_sentences\n","        self.yoruba_diacritics = re.compile(r'[àáâéèêíìîóòôúùû]')\n","        self.yoruba_pattern = re.compile(r'[abdeẹfghijklmnoprstuwy]', re.IGNORECASE)\n","\n","    def load_custom_dataset(self, dataset_path: str, file_format: str = \"txt\", text_column: str = \"text\",\n","                            max_sentences: int = None) -> List[str]:\n","\n","        if max_sentences is None:\n","            max_sentences = self.max_sentences\n","\n","        dataset_path = Path(dataset_path)\n","\n","        if file_format == \"txt\":\n","            sentences = self.load_text_file(dataset_path, max_sentences)\n","\n","        valid_sentences = [s for s in sentences if self.is_valid_yoruba_text(s)]\n","        logger.info(f\"Loaded {len(valid_sentences)} valid sentences from {len(sentences)} total\")\n","\n","        return valid_sentences[:max_sentences]\n","\n","    def load_text_file(self, file_path: Path, max_sentences: int) -> List[str]:\n","        sentences = []\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for i, line in enumerate(f):\n","                if i >= max_sentences:\n","                    break\n","                line = line.strip()\n","                if line:\n","                    sentences.append(line)\n","        return sentences\n","\n","    def is_valid_yoruba_text(self, text: str) -> bool:\n","        \"\"\"Check if text is valid Yoruba\"\"\"\n","        if len(text) < 10 or len(text) > 1000:\n","            return False\n","\n","        latin_chars = len(re.findall(r'[a-zA-ZàáâéèêíìîóòôúùûẹọṣẹṇÀÁÂÉÈÊÍÌÎÓÒÔÚÙÛẸỌṢẸṆ]', text))\n","        total_chars = len([c for c in text if c.isalpha()])\n","\n","        if total_chars == 0:\n","            return False\n","\n","        latin_ratio = latin_chars / total_chars\n","        if latin_ratio < 0.8:\n","            return False\n","\n","        common_yoruba_words = ['ni', 'wa', 'ti', 'si', 'ko', 'lo', 'ba', 'se', 'ati', 'ninu', 'lati', 'won', 'ile']\n","        words = text.lower().split()\n","        yoruba_word_count = sum(1 for word in words if any(yw in word for yw in common_yoruba_words))\n","\n","        return yoruba_word_count > 0 or self.yoruba_diacritics.search(text) is not None\n","\n","class TokenizerLoader:\n","    \"\"\"Loads different types of pre-trained tokenizers\"\"\"\n","\n","    def __init__(self, hf_token: str = None):\n","        self.hf_token = hf_token\n","\n","    def load_huggingface_tokenizer(self, model_name: str):\n","        \"\"\"load HF tokenizer\"\"\"\n","        try:\n","            tokenizer = AutoTokenizer.from_pretrained(\n","                model_name,\n","                token=self.hf_token,\n","                trust_remote_code=True\n","            )\n","            logger.info(f\"Loaded HuggingFace tokenizer: {model_name}\")\n","            return tokenizer\n","        except Exception as e:\n","            logger.error(f\"Error loading HuggingFace tokenizer {model_name}: {e}\")\n","            return None\n","\n","    def load_picky_bpe_tokenizer(self, model_path: str):\n","        \"\"\"Load Picky BPE tokenizer\"\"\"\n","        try:\n","            tokenizer = PickyBPETokenizer(model_path)\n","            if tokenizer.model is not None:\n","                logger.info(f\"Loaded Picky BPE tokenizer: {model_path}\")\n","                return tokenizer\n","            else:\n","                return None\n","        except Exception as e:\n","            logger.error(f\"Error loading Picky BPE tokenizer {model_path}: {e}\")\n","            return None\n","\n","    def load_sentencepiece_tokenizer(self, model_path: str):\n","        \"\"\"Load SentencePiece tokenizer (SaGe or BPE)\"\"\"\n","        try:\n","            tokenizer = SentencePieceTokenizer(model_path)\n","            if tokenizer.sp is not None:\n","                logger.info(f\"Loaded SentencePiece tokenizer: {model_path}\")\n","                return tokenizer\n","            else:\n","                return None\n","        except Exception as e:\n","            logger.error(f\"Error loading SentencePiece tokenizer {model_path}: {e}\")\n","            return None\n","\n","    def load_simple_bpe_tokenizer(self, vocab_path: str, merges_path: str = None):\n","        \"\"\"Load simple BPE tokenizer\"\"\"\n","        try:\n","            tokenizer = SimpleBPETokenizer(vocab_path, merges_path)\n","            logger.info(f\"Loaded simple BPE tokenizer: {vocab_path}\")\n","            return tokenizer\n","        except Exception as e:\n","            logger.error(f\"Error loading simple BPE tokenizer {vocab_path}: {e}\")\n","            return None\n","\n","def evaluate_tokenizers(test_texts: List[str], tokenizer_configs: List[Dict], hf_token: str = None) -> pd.DataFrame:\n","\n","    evaluator = TokenizerEvaluator(test_texts)\n","    loader = TokenizerLoader(hf_token=hf_token)\n","\n","    results = []\n","\n","    for config in tokenizer_configs:\n","        tokenizer_name = config['name']\n","        tokenizer_type = config['type']\n","        tokenizer_path = config['path']\n","\n","        logger.info(f\"Loading tokenizer: {tokenizer_name}\")\n","\n","        tokenizer = None\n","        if tokenizer_type == 'huggingface':\n","            tokenizer = loader.load_huggingface_tokenizer(tokenizer_path)\n","        elif tokenizer_type == 'picky_bpe':\n","            tokenizer = loader.load_picky_bpe_tokenizer(tokenizer_path)\n","        elif tokenizer_type == 'sentencepiece':\n","            tokenizer = loader.load_sentencepiece_tokenizer(tokenizer_path)\n","        elif tokenizer_type == 'standard_bpe':\n","            vocab_path = tokenizer_path\n","            merges_path = config.get('merges_path', None)\n","            tokenizer = loader.load_simple_bpe_tokenizer(vocab_path, merges_path)\n","        else:\n","            logger.error(f\"Unknown tokenizer type: {tokenizer_type}\")\n","            continue\n","\n","        if tokenizer is None:\n","            logger.warning(f\"Skipping {tokenizer_name} - failed to load\")\n","            continue\n","\n","        try:\n","            metrics = evaluator.evaluate_tokenizer(tokenizer, tokenizer_name)\n","            results.append({\"tokenizer\": tokenizer_name, \"type\": tokenizer_type, **metrics})\n","        except Exception as e:\n","            logger.error(f\"Error evaluating {tokenizer_name}: {e}\")\n","            continue\n","\n","    return pd.DataFrame(results)\n","\n","def main(dataset_path: str = None, file_format: str = \"auto\", text_column: str = \"text\", max_sentences: int = 2000,\n","         hf_token: str = None, custom_tokenizer_paths: Dict = None):\n","    logger.info(\"Starting Yoruba tokenizer evaluation...\")\n","\n","    random.seed(42)\n","    np.random.seed(42)\n","\n","    processor = YorubaCorpusProcessor(max_sentences=max_sentences)\n","\n","    if dataset_path:\n","        logger.info(f\"Using custom dataset: {dataset_path}\")\n","        test_texts = processor.load_custom_dataset(dataset_path, file_format, text_column, max_sentences)\n","    else:\n","        logger.error(\"No dataset path provided!\")\n","        return\n","\n","    logger.info(f\"Loaded {len(test_texts)} sentences for evaluation\")\n","\n","    tokenizer_configs = [\n","        {\n","            'name': 'Llama-2-7B',\n","            'type': 'huggingface',\n","            'path': 'meta-llama/Llama-2-7b-hf'\n","        },\n","        {\n","            'name': 'Gemma-7B',\n","            'type': 'huggingface',\n","            'path': 'google/gemma-7b'\n","        }\n","    ]\n","\n","    if custom_tokenizer_paths:\n","        if 'standard_bpe' in custom_tokenizer_paths:\n","            tokenizer_configs.append({\n","                'name': 'Simple BPE (HF)',\n","                'type': 'huggingface',\n","                'path': custom_tokenizer_paths['standard_bpe']\n","            })\n","\n","        if 'picky_bpe' in custom_tokenizer_paths:\n","            tokenizer_configs.append({\n","                'name': 'Picky BPE (HF)',\n","                'type': 'huggingface',\n","                'path': custom_tokenizer_paths['picky_bpe']\n","            })\n","\n","        if 'picky_bpe_custom' in custom_tokenizer_paths:\n","            tokenizer_configs.append({\n","                'name': 'Picky BPE (Custom)',\n","                'type': 'picky_bpe',\n","                'path': custom_tokenizer_paths['picky_bpe_custom']\n","            })\n","\n","        if 'sage_sp' in custom_tokenizer_paths:\n","            tokenizer_configs.append({\n","                'name': 'SentencePiece Model',\n","                'type': 'sentencepiece',\n","                'path': custom_tokenizer_paths['sage_sp']\n","            })\n","\n","        if 'sage' in custom_tokenizer_paths and not '/path/to/your/' in custom_tokenizer_paths['sage']:\n","            tokenizer_configs.append({\n","                'name': 'SaGe',\n","                'type': 'sentencepiece',\n","                'path': custom_tokenizer_paths['sage']\n","            })\n","\n","    logger.info(f\"Configured {len(tokenizer_configs)} tokenizers for evaluation\")\n","\n","    print(\"Evaluating tokenizers...\")\n","    results_df = evaluate_tokenizers(test_texts, tokenizer_configs, hf_token=hf_token)\n","\n","    print(\"\\n\" + \"=\"*120)\n","    print(\"EVALUATION RESULTS\")\n","    print(\"=\"*120)\n","\n","    display_df = results_df.copy()\n","    numeric_cols = ['fertility', 'compression_ratio', 'coverage', 'composite_score', 'mean_length']\n","    for col in numeric_cols:\n","        if col in display_df.columns:\n","            display_df[col] = display_df[col].round(4)\n","\n","    print(display_df[['tokenizer', 'type', 'fertility', 'compression_ratio', 'coverage', 'composite_score']].to_string(index=False))\n","    print(\"=\"*120)\n","\n","    best_tokenizer = results_df.loc[results_df['composite_score'].idxmax()]\n","    print(f\"\\nBest tokenizer: {best_tokenizer['tokenizer']}\")\n","    print(f\"  - Composite Score: {best_tokenizer['composite_score']:.4f}\")\n","    print(f\"  - Fertility: {best_tokenizer['fertility']:.4f} (lower is better)\")\n","    print(f\"  - Compression Ratio: {best_tokenizer['compression_ratio']:.4f} (higher is better)\")\n","    print(f\"  - Coverage: {best_tokenizer['coverage']:.4f} (higher is better)\")\n","\n","    custom_tokenizers = results_df[results_df['type'].isin(['picky_bpe', 'sentencepiece', 'standard_bpe'])]\n","    if len(custom_tokenizers) > 0:\n","        print(f\"\\nCUSTOM TOKENIZER INSIGHTS:\")\n","        for _, row in custom_tokenizers.iterrows():\n","            print(f\"  - {row['tokenizer']}: Fertility={row['fertility']:.3f}, Coverage={row['coverage']:.3f}, Compression={row['compression_ratio']:.3f}\")\n","\n","    results_df.to_csv('yoruba_tokenizer_evaluation_results.csv', index=False)\n","    logger.info(\"Results saved to yoruba_tokenizer_evaluation_results.csv\")\n","\n","if __name__ == \"__main__\":\n","\n","    hf_token = getpass.getpass(\"Hugging Face token : \")\n","\n","    CUSTOM_TOKENIZER_PATHS = {\n","        'simple_bpe': '/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/tokenizers/standard_bpe',\n","        'picky_bpe': '/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/tokenizers/picky_bpe',\n","        'sage': '/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/tokenizers/sage,\n","    }\n","\n","    DATASET_CONFIG = {\n","        'dataset_path': '/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/dataset/yo_eval.txt',\n","        'file_format': 'txt',\n","        'text_column': 'text',\n","        'max_sentences': 5000,\n","        'hf_token': hf_token,\n","        'custom_tokenizer_paths': CUSTOM_TOKENIZER_PATHS\n","    }\n","\n","    main(**DATASET_CONFIG)"],"metadata":{"id":"sRdYq1WE-CmI"},"execution_count":null,"outputs":[]}]}