{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNUluzjowi7eHlzxKD1c9yP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_o8Dd_LvEKwM"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import json\n","import math\n","import torch\n","import getpass\n","import re\n","import numpy as np\n","from pathlib import Path\n","import traceback\n","from typing import List, Dict, Optional, Tuple\n","from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer\n","from transformers.tokenization_utils import AddedToken\n","\n","def round_to_nearest_multiple(vocabulary_size: int, multiple: int = 8) -> int:\n","    return math.ceil(vocabulary_size / multiple) * multiple\n","\n","class BPETokenizer(PreTrainedTokenizer):\n","    def __init__(self, model_path: str, **kwargs):\n","        self.model_path = Path(model_path)\n","        self.model_data = self._load_model_data()\n","        self.vocab = self.model_data['vocab']\n","        self.id2token = {int(k): v for k, v in self.model_data['id2token'].items()}\n","        self.merges = [(m['left'], m['right']) for m in self.model_data['merges']]\n","\n","        super().__init__(\n","            unk_token=AddedToken('<unk>', lstrip=False, rstrip=False),\n","            bos_token=AddedToken('<bos>', lstrip=False, rstrip=False),\n","            eos_token=AddedToken('<eos>', lstrip=False, rstrip=False),\n","            pad_token=AddedToken('<pad>', lstrip=False, rstrip=False),\n","            **kwargs\n","        )\n","\n","        print(f\"BPE tokenizer loaded: {len(self.vocab):,} tokens, {len(self.merges):,} merges\")\n","\n","    def _load_model_data(self):\n","        model_file = self.model_path / 'model.json'\n","        if not model_file.exists():\n","            raise FileNotFoundError(f\"BPE model not found: {model_file}\")\n","\n","        with open(model_file, 'r', encoding='utf-8') as f:\n","            return json.load(f)\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.vocab)\n","\n","    def get_vocab(self):\n","        return self.vocab.copy()\n","\n","    def _tokenize(self, text: str, **kwargs):\n","        if not text:\n","            return []\n","\n","        tokens = [char if char in self.vocab else self.unk_token for char in text]\n","\n","        for left, right in self.merges:\n","            tokens = self._apply_merge(tokens, left, right)\n","\n","        return tokens\n","\n","    def _apply_merge(self, tokens, left, right):\n","        new_tokens = []\n","        i = 0\n","        while i < len(tokens):\n","            if i < len(tokens) - 1 and tokens[i] == left and tokens[i + 1] == right:\n","                new_tokens.append(left + right)\n","                i += 2\n","            else:\n","                new_tokens.append(tokens[i])\n","                i += 1\n","        return new_tokens\n","\n","    def _convert_token_to_id(self, token):\n","        return self.vocab.get(token, self.vocab.get(self.unk_token, 0))\n","\n","    def _convert_id_to_token(self, index):\n","        return self.id2token.get(index, self.unk_token)\n","\n","    def convert_tokens_to_string(self, tokens):\n","        return ''.join(tokens)\n","\n","class ExtendedTokenizer:\n","    def __init__(self, base_tokenizer: AutoTokenizer, bpe_tokenizer: BPETokenizer, combined_vocab: Dict[str, int]):\n","        self.base_tokenizer = base_tokenizer\n","        self.bpe_tokenizer = bpe_tokenizer\n","        self.vocab = combined_vocab\n","        self.id2token = {token_id: token for token, token_id in combined_vocab.items()}\n","\n","        self.yoruba_pattern = re.compile(r'[ẹọṣáàéèíìóòúùńǹāēīōū]')\n","        self.english_pattern = re.compile(r'[a-zA-Z]')\n","\n","        self.unk_token = base_tokenizer.unk_token\n","        self.bos_token = base_tokenizer.bos_token\n","        self.eos_token = base_tokenizer.eos_token\n","        self.pad_token = base_tokenizer.pad_token\n","\n","    def __len__(self):\n","        return len(self.vocab)\n","\n","    def get_vocab(self):\n","        return self.vocab.copy()\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.vocab)\n","\n","    @property\n","    def bos_token_id(self):\n","        return self.vocab.get(self.bos_token, 1)\n","\n","    @property\n","    def eos_token_id(self):\n","        return self.vocab.get(self.eos_token, 2)\n","\n","    @property\n","    def pad_token_id(self):\n","        return self.vocab.get(self.pad_token, 0)\n","\n","    @property\n","    def unk_token_id(self):\n","        return self.vocab.get(self.unk_token, 3)\n","\n","    def _classify_text_type(self, text: str) -> str:\n","        if not text.strip():\n","            return \"base\"\n","\n","        analyzable_chars = re.sub(r'[^\\w]', '', text, flags=re.UNICODE)\n","\n","        if not analyzable_chars:\n","            return \"base\"\n","\n","        yoruba_chars = len(self.yoruba_pattern.findall(analyzable_chars))\n","        english_chars = len(self.english_pattern.findall(analyzable_chars))\n","        total_chars = len(analyzable_chars)\n","\n","        if yoruba_chars == 0:\n","            return \"base\"\n","\n","        if english_chars == 0 and yoruba_chars / total_chars > 0.3:\n","            return \"bpe\"\n","\n","        return \"base\"\n","\n","    def tokenize(self, text: str):\n","        if not text:\n","            return []\n","\n","        text_type = self._classify_text_type(text)\n","\n","        if text_type == \"bpe\":\n","            try:\n","                bpe_tokens = self.bpe_tokenizer._tokenize(text)\n","                if all(token in self.vocab for token in bpe_tokens):\n","                    return bpe_tokens\n","            except Exception:\n","                pass\n","\n","        return self.base_tokenizer.tokenize(text)\n","\n","    def encode(self, text: str, add_special_tokens=True, return_tensors=None):\n","        tokens = self.tokenize(text)\n","        token_ids = [self.vocab.get(token, self.unk_token_id) for token in tokens]\n","\n","        if add_special_tokens:\n","            if self.bos_token and self.bos_token in self.vocab:\n","                token_ids = [self.vocab[self.bos_token]] + token_ids\n","            if self.eos_token and self.eos_token in self.vocab:\n","                token_ids = token_ids + [self.vocab[self.eos_token]]\n","\n","        if return_tensors == \"pt\":\n","            return torch.tensor([token_ids])\n","\n","        return token_ids\n","\n","    def decode(self, token_ids, skip_special_tokens=True):\n","        if hasattr(token_ids, 'tolist'):\n","            token_ids = token_ids.tolist()\n","\n","        if isinstance(token_ids[0], list):\n","            token_ids = token_ids[0]\n","\n","        tokens = []\n","        for token_id in token_ids:\n","            token = self.id2token.get(token_id, self.unk_token)\n","            if not skip_special_tokens or token not in [self.bos_token, self.eos_token, self.pad_token]:\n","                tokens.append(token)\n","\n","        base_vocab = set(self.base_tokenizer.get_vocab().keys())\n","        non_base_tokens = sum(1 for token in tokens if token not in base_vocab)\n","\n","        if non_base_tokens > len(tokens) * 0.5:\n","            return ''.join(tokens)\n","        else:\n","            return self.base_tokenizer.convert_tokens_to_string(tokens)\n","\n","    def convert_ids_to_tokens(self, token_id):\n","        return self.id2token.get(token_id, self.unk_token)\n","\n","    def convert_tokens_to_ids(self, tokens):\n","        if isinstance(tokens, str):\n","            return self.vocab.get(tokens, self.unk_token_id)\n","        return [self.vocab.get(token, self.unk_token_id) for token in tokens]\n","\n","    def save_pretrained(self, save_directory: str):\n","        save_dir = Path(save_directory)\n","        save_dir.mkdir(parents=True, exist_ok=True)\n","\n","        with open(save_dir / 'vocab.json', 'w', encoding='utf-8') as f:\n","            json.dump(self.vocab, f, indent=2, ensure_ascii=False)\n","\n","        config = {\n","            'tokenizer_class': 'ExtendedTokenizer',\n","            'vocab_size': len(self.vocab),\n","            'base_vocab_size': len(self.base_tokenizer.get_vocab()),\n","            'strategy': 'content_routing',\n","            'language': 'yoruba',\n","            'special_tokens': {\n","                'unk_token': self.unk_token,\n","                'bos_token': self.bos_token,\n","                'eos_token': self.eos_token,\n","                'pad_token': self.pad_token\n","            }\n","        }\n","\n","        with open(save_dir / 'tokenizer_config.json', 'w', encoding='utf-8') as f:\n","            json.dump(config, f, indent=2, ensure_ascii=False)\n","\n","        import shutil\n","        bpe_source = self.bpe_tokenizer.model_path / 'simple_bpe_model.json'\n","        if bpe_source.exists():\n","            shutil.copy2(bpe_source, save_dir / 'simple_bpe_model.json')\n","\n","        print(f\"Tokenizer saved to {save_directory}\")\n","\n","def create_combined_vocabulary(base_tokenizer: AutoTokenizer, bpe_tokenizer: BPETokenizer) -> Dict[str, int]:\n","    print(\"Creating combined vocabulary...\")\n","\n","    combined_vocab = base_tokenizer.get_vocab().copy()\n","    base_size = len(combined_vocab)\n","    bpe_vocab = bpe_tokenizer.get_vocab()\n","\n","    overlapping_tokens = []\n","    for bpe_token in bpe_vocab:\n","        if bpe_token in combined_vocab:\n","            overlapping_tokens.append(bpe_token)\n","\n","    print(f\"Overlapping tokens between base and BPE: {len(overlapping_tokens):,}\")\n","    if len(overlapping_tokens) <= 20:\n","        print(f\"  Sample overlapping tokens: {overlapping_tokens[:10]}\")\n","    else:\n","        print(f\"  Sample overlapping tokens: {overlapping_tokens[:10]} ...\")\n","\n","    id_offset = base_size\n","    new_tokens_added = 0\n","\n","    for bpe_token in bpe_vocab:\n","        if bpe_token not in combined_vocab:\n","            combined_vocab[bpe_token] = id_offset + new_tokens_added\n","            new_tokens_added += 1\n","\n","    print(f\"Base tokens (preserved): {base_size:,}\")\n","    print(f\"New BPE tokens added: {new_tokens_added:,}\")\n","    print(f\"Combined vocabulary: {len(combined_vocab):,}\")\n","\n","    return combined_vocab\n","\n","def embedding_initialization(\n","    source_model: AutoModelForCausalLM,\n","    source_tokenizer: AutoTokenizer,\n","    target_tokenizer: ExtendedTokenizer,\n","    alignment_multiple: int = 8,\n","    tie_word_embeddings: bool = False\n",") -> AutoModelForCausalLM:\n","    print(\"Embedding initialization...\")\n","\n","    aligned_vocab_size = round_to_nearest_multiple(len(target_tokenizer), alignment_multiple)\n","\n","    source_embeddings = source_model.get_input_embeddings().weight.detach().numpy()\n","    target_embeddings = np.random.normal(\n","        np.mean(source_embeddings, axis=0),\n","        np.std(source_embeddings, axis=0),\n","        (aligned_vocab_size, source_embeddings.shape[1])\n","    )\n","\n","    if not tie_word_embeddings:\n","        print(\"You are using the output projection init.\")\n","        source_head_embeddings = source_model.get_output_embeddings().weight.detach().numpy()\n","        target_head_embeddings = np.random.normal(\n","            np.mean(source_head_embeddings, axis=0),\n","            np.std(source_head_embeddings, axis=0),\n","            (aligned_vocab_size, source_head_embeddings.shape[1])\n","        )\n","\n","    source_model.resize_token_embeddings(aligned_vocab_size, pad_to_multiple_of=alignment_multiple)\n","    source_model.get_input_embeddings().weight.data = torch.from_numpy(target_embeddings)\n","    source_model.config.vocab_size = aligned_vocab_size\n","\n","    if not tie_word_embeddings:\n","        source_model.get_output_embeddings().weight.data = torch.from_numpy(target_head_embeddings)\n","        source_model.config.tie_word_embeddings = False\n","    else:\n","        source_model.tie_weights()\n","\n","    print(\"Initialization completed\")\n","    return source_model\n","\n","def create_gemma_yoruba_model(\n","    source_model_name: str,\n","    bpe_model_path: str,\n","    hf_token: Optional[str] = None,\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","    tie_word_embeddings: bool = False\n",") -> Tuple[AutoModelForCausalLM, ExtendedTokenizer]:\n","\n","    print(\"Loading base model and tokenizer...\")\n","    source_model = AutoModelForCausalLM.from_pretrained(\n","        source_model_name,\n","        token=hf_token,\n","        torch_dtype=torch.float32,\n","        low_cpu_mem_usage=True\n","    )\n","    source_tokenizer = AutoTokenizer.from_pretrained(source_model_name, token=hf_token)\n","\n","    print(\"Loading BPE tokenizer...\")\n","    bpe_tokenizer = BPETokenizer(model_path=bpe_model_path)\n","\n","    if device != \"cpu\":\n","        source_model = source_model.to(device)\n","\n","    combined_vocab = create_combined_vocabulary(source_tokenizer, bpe_tokenizer)\n","\n","    extended_tokenizer = ExtendedTokenizer(\n","        source_tokenizer, bpe_tokenizer, combined_vocab\n","    )\n","\n","    extended_model = embedding_initialization(\n","        source_model, source_tokenizer, extended_tokenizer, tie_word_embeddings=tie_word_embeddings\n","    )\n","\n","    return extended_model, extended_tokenizer\n","\n","if __name__ == \"__main__\":\n","    CONFIG = {\n","        'source_model': \"google/gemma-7b\",\n","        'bpe_model_path': \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/tokenizers/grapheme_picky_bpe\",\n","        'output_path': \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/initialized/gemma_bpe_random_init\",\n","        'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    }\n","\n","    try:\n","        hf_token = getpass.getpass(\"Enter HuggingFace token: \")\n","\n","        extended_model, extended_tokenizer = create_gemma_yoruba_model(\n","            source_model_name=CONFIG['source_model'],\n","            bpe_model_path=CONFIG['bpe_model_path'],\n","            hf_token=hf_token,\n","            device=CONFIG['device'],\n","            tie_word_embeddings=False\n","        )\n","\n","        print(f\"\\nSaving to {CONFIG['output_path']}\")\n","        output_dir = Path(CONFIG['output_path'])\n","        output_dir.mkdir(parents=True, exist_ok=True)\n","\n","        extended_model.save_pretrained(CONFIG['output_path'])\n","        extended_tokenizer.save_pretrained(CONFIG['output_path'])\n","\n","        extension_config = {\n","            'approach': 'random',\n","            'language': 'yoruba',\n","            'source_model': CONFIG['source_model'],\n","            'bpe_model_path': CONFIG['bpe_model_path'],\n","            'embeddings_tied': False,\n","            'base_embeddings_preserved': False,\n","            'final_vocab_size': extended_model.config.vocab_size,\n","            'embedding_dim': extended_model.get_input_embeddings().embedding_dim,\n","            'total_parameters': sum(p.numel() for p in extended_model.parameters())\n","        }\n","\n","        with open(output_dir / 'extension_config.json', 'w') as f:\n","            json.dump(extension_config, f, indent=2)\n","\n","        print(f\"Final vocabulary: {extended_model.config.vocab_size:,} tokens\")\n","\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n","        traceback.print_exc()"],"metadata":{"id":"P1OYKxpZEQFQ"},"execution_count":null,"outputs":[]}]}