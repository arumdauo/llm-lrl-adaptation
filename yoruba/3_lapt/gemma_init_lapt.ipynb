{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMdfbDNs2wMfe6V6Opub7xf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eRdx08_T6MQO"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import json\n","import logging\n","import sys\n","import pickle\n","import hashlib\n","import getpass\n","import math\n","import re\n","from pathlib import Path\n","from datetime import datetime\n","from tqdm.notebook import tqdm\n","from typing import Dict, Optional, Tuple, List, Union\n","from functools import lru_cache\n","import numpy as np\n","import torch\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils import clip_grad_norm_\n","from transformers import (\n","    GemmaForCausalLM,\n","    AutoTokenizer,\n","    PreTrainedTokenizer,\n","    get_linear_schedule_with_warmup,\n","    set_seed\n",")\n","from transformers.tokenization_utils import AddedToken\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_kbit_training,\n","    TaskType,\n","    PeftModel\n",")\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    handlers=[logging.StreamHandler(sys.stdout)],\n","    force=True\n",")\n","logger = logging.getLogger(__name__)\n","\n","class BPETokenizer(PreTrainedTokenizer):\n","\n","    def __init__(self, model_path: str, **kwargs):\n","        self.model_path = Path(model_path)\n","        self.model_data = self._load_model_data()\n","        self.vocab = self.model_data['vocab']\n","        self.id2token = {int(k): v for k, v in self.model_data['id2token'].items()}\n","        self.merges = [(m['left'], m['right']) for m in self.model_data['merges']]\n","        self._special_tokens = self.model_data.get('special_tokens', {})\n","\n","        self._token_cache = {}\n","        self._max_cache_size = 10000\n","        self._merge_pairs = set(self.merges)\n","\n","        super().__init__(\n","            unk_token=AddedToken('<unk>', lstrip=False, rstrip=False),\n","            bos_token=AddedToken('<s>', lstrip=False, rstrip=False),\n","            eos_token=AddedToken('</s>', lstrip=False, rstrip=False),\n","            pad_token=AddedToken('<pad>', lstrip=False, rstrip=False),\n","            **kwargs\n","        )\n","\n","        logger.info(f\"BPE tokenizer loaded: {len(self.vocab):,} tokens, {len(self.merges):,} merges\")\n","\n","    def _load_model_data(self):\n","        model_file = self.model_path / 'simple_bpe_model.json'\n","        if not model_file.exists():\n","            raise FileNotFoundError(f\"BPE model not found: {model_file}\")\n","\n","        with open(model_file, 'r', encoding='utf-8') as f:\n","            return json.load(f)\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.vocab)\n","\n","    def get_vocab(self):\n","        return self.vocab.copy()\n","\n","    def _tokenize(self, text: str, **kwargs):\n","        if not text:\n","            return []\n","\n","        if text in self._token_cache:\n","            return self._token_cache[text]\n","\n","        if len(text) > 5000:\n","            result = self._tokenize_long_text(text)\n","        else:\n","            result = self._tokenize_standard(text)\n","\n","        if len(self._token_cache) < self._max_cache_size:\n","            self._token_cache[text] = result\n","\n","        return result\n","\n","    def _tokenize_standard(self, text: str):\n","        tokens = [char if char in self.vocab else self.unk_token for char in text]\n","        return self._apply_all_merges(tokens)\n","\n","    def _tokenize_long_text(self, text: str):\n","        chunk_size = 1000\n","        tokens = []\n","\n","        for i in range(0, len(text), chunk_size):\n","            chunk = text[i:i + chunk_size]\n","            chunk_tokens = self._tokenize_standard(chunk)\n","            tokens.extend(chunk_tokens)\n","\n","        return tokens\n","\n","    def _apply_all_merges(self, tokens):\n","        changed = True\n","        iteration_count = 0\n","        max_iterations = len(self.merges)\n","\n","        while changed and iteration_count < max_iterations:\n","            changed = False\n","            iteration_count += 1\n","\n","            i = 0\n","            while i < len(tokens) - 1:\n","                current_pair = (tokens[i], tokens[i + 1])\n","                if current_pair in self._merge_pairs:\n","                    merged_token = tokens[i] + tokens[i + 1]\n","                    tokens[i:i + 2] = [merged_token]\n","                    changed = True\n","                else:\n","                    i += 1\n","\n","        return tokens\n","\n","    @lru_cache(maxsize=10000)\n","    def _convert_token_to_id(self, token):\n","        return self.vocab.get(token, self.vocab.get(self.unk_token, 0))\n","\n","    @lru_cache(maxsize=10000)\n","    def _convert_id_to_token(self, index):\n","        return self.id2token.get(index, self.unk_token)\n","\n","    def convert_tokens_to_string(self, tokens):\n","        return ''.join(tokens)\n","\n","class ExtendedTokenizer:\n","\n","    def __init__(self, base_tokenizer: AutoTokenizer, bpe_tokenizer: BPETokenizer, combined_vocab: Dict[str, int]):\n","        self.base_tokenizer = base_tokenizer\n","        self.bpe_tokenizer = bpe_tokenizer\n","        self.vocab = combined_vocab\n","        self.id2token = {token_id: token for token, token_id in combined_vocab.items()}\n","        self.base_vocab_size = len(base_tokenizer.get_vocab())\n","\n","        self._encode_cache = {}\n","        self._tokenize_cache = {}\n","        self._max_cache_size = 10000\n","\n","        self.yoruba_pattern = re.compile(r'[ẹọṣáàéèíìóòúùńǹāēīōū]')\n","        self.english_pattern = re.compile(r'[a-zA-Z]')\n","\n","        self.unk_token = base_tokenizer.unk_token\n","        self.bos_token = base_tokenizer.bos_token\n","        self.eos_token = base_tokenizer.eos_token\n","        self.pad_token = '<pad>'\n","\n","    def __len__(self):\n","        return len(self.vocab)\n","\n","    def get_vocab(self):\n","        return self.vocab.copy()\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.vocab)\n","\n","    @property\n","    def bos_token_id(self):\n","        return self.vocab.get(self.bos_token, 1)\n","\n","    @property\n","    def eos_token_id(self):\n","        return self.vocab.get(self.eos_token, 2)\n","\n","    @property\n","    def pad_token_id(self):\n","        return self.vocab.get(self.pad_token, 32000)\n","\n","    @property\n","    def unk_token_id(self):\n","        return self.vocab.get(self.unk_token, 0)\n","\n","    @property\n","    def pretrained_token_ids(self):\n","        return set(range(self.base_vocab_size))\n","\n","    @property\n","    def new_token_ids(self):\n","        return set(range(self.base_vocab_size, len(self.vocab)))\n","\n","    def _classify_text_type(self, text: str) -> str:\n","        if not text.strip():\n","            return \"base\"\n","\n","        analyzable_chars = re.sub(r'[^\\w]', '', text, flags=re.UNICODE)\n","        if not analyzable_chars:\n","            return \"base\"\n","\n","        yoruba_chars = len(self.yoruba_pattern.findall(analyzable_chars))\n","        english_chars = len(self.english_pattern.findall(analyzable_chars))\n","        total_chars = len(analyzable_chars)\n","\n","        yoruba_ratio = yoruba_chars / total_chars if total_chars > 0 else 0\n","        english_ratio = english_chars / total_chars if total_chars > 0 else 0\n","\n","        if yoruba_chars > 0:\n","            if english_ratio < 0.8:\n","                return \"bpe\"\n","\n","        return \"base\"\n","\n","    def tokenize(self, text: str):\n","\n","        if not text:\n","            return []\n","\n","        if text in self._tokenize_cache:\n","            return self._tokenize_cache[text].copy()\n","\n","        text_type = self._classify_text_type(text)\n","\n","        tokens = []\n","\n","        if text_type == \"bpe\":\n","            try:\n","                bpe_tokens = self.bpe_tokenizer._tokenize(text)\n","                if all(token in self.vocab for token in bpe_tokens):\n","                    tokens = bpe_tokens\n","                else:\n","                    tokens = self.base_tokenizer.tokenize(text)\n","            except Exception:\n","                tokens = self.base_tokenizer.tokenize(text)\n","        else:\n","            tokens = self.base_tokenizer.tokenize(text)\n","\n","        if len(self._tokenize_cache) < self._max_cache_size:\n","            self._tokenize_cache[text] = tokens.copy()\n","\n","        return tokens\n","\n","    def encode(self, text: str, add_special_tokens=True, return_tensors=None, truncation=None, max_length=None, padding=False):\n","        cache_key = (text, add_special_tokens, truncation, max_length)\n","\n","        if cache_key in self._encode_cache:\n","            token_ids = self._encode_cache[cache_key].copy()\n","        else:\n","            tokens = self.tokenize(text)\n","            token_ids = [self.vocab.get(token, self.unk_token_id) for token in tokens]\n","\n","            if add_special_tokens:\n","                if self.bos_token and self.bos_token in self.vocab:\n","                    token_ids = [self.vocab[self.bos_token]] + token_ids\n","                if self.eos_token and self.eos_token in self.vocab:\n","                    token_ids = token_ids + [self.vocab[self.eos_token]]\n","\n","            if truncation and max_length and len(token_ids) > max_length:\n","                token_ids = token_ids[:max_length]\n","\n","            if len(self._encode_cache) < self._max_cache_size:\n","                self._encode_cache[cache_key] = token_ids.copy()\n","\n","        if return_tensors == \"pt\":\n","            return torch.tensor([token_ids])\n","\n","        return token_ids\n","\n","    def decode(self, token_ids, skip_special_tokens=True):\n","        if hasattr(token_ids, 'tolist'):\n","            token_ids = token_ids.tolist()\n","\n","        if isinstance(token_ids[0], list):\n","            token_ids = token_ids[0]\n","\n","        tokens = []\n","        for token_id in token_ids:\n","            token = self.id2token.get(token_id, self.unk_token)\n","            if not skip_special_tokens or token not in [self.bos_token, self.eos_token, self.pad_token]:\n","                tokens.append(token)\n","\n","        base_vocab = set(self.base_tokenizer.get_vocab().keys())\n","        non_base_tokens = sum(1 for token in tokens if token not in base_vocab)\n","\n","        if non_base_tokens > len(tokens) * 0.5:\n","            return ''.join(tokens)\n","        else:\n","            return self.base_tokenizer.convert_tokens_to_string(tokens)\n","\n","    def analyze_token_types(self, token_ids):\n","        if hasattr(token_ids, 'tolist'):\n","            token_ids = token_ids.tolist()\n","\n","        pretrained_count = sum(1 for token_id in token_ids if token_id < self.base_vocab_size)\n","        new_count = len(token_ids) - pretrained_count\n","        return pretrained_count, new_count\n","\n","    def save_pretrained(self, save_directory: str):\n","        save_dir = Path(save_directory)\n","        save_dir.mkdir(parents=True, exist_ok=True)\n","\n","        with open(save_dir / 'vocab.json', 'w', encoding='utf-8') as f:\n","            json.dump(self.vocab, f, indent=2, ensure_ascii=False)\n","\n","        config = {\n","            'tokenizer_class': 'ExtendedTokenizer',\n","            'vocab_size': len(self.vocab),\n","            'base_vocab_size': len(self.base_tokenizer.get_vocab()),\n","            'strategy': 'mean',\n","            'language': 'yoruba',\n","            'special_tokens': {\n","                'unk_token': self.unk_token,\n","                'bos_token': self.bos_token,\n","                'eos_token': self.eos_token,\n","                'pad_token': self.pad_token\n","            }\n","        }\n","\n","        with open(save_dir / 'tokenizer_config.json', 'w', encoding='utf-8') as f:\n","            json.dump(config, f, indent=2, ensure_ascii=False)\n","\n","        import shutil\n","        bpe_source = self.bpe_tokenizer.model_path / 'simple_bpe_model.json'\n","        if bpe_source.exists():\n","            shutil.copy2(bpe_source, save_dir / 'simple_bpe_model.json')\n","\n","class TrainingConfig:\n","    MODEL_PATH = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/initialized/gemma_mean_init\"\n","    TOKENIZER_PATH = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/initialized/gemma_mean_init\"\n","    BPE_MODEL_PATH = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/tokenizers/grapheme_picky_bpe\"\n","    OUTPUT_DIR = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/models/gemma7b_mean_init_lapt\"\n","\n","    TRAIN_DATASET_PATH = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/dataset/dataset_lapt/train\"\n","    VAL_DATASET_PATH = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/dataset/dataset_lapt/val\"\n","    TEXT_COLUMN_NAME = \"text\"\n","\n","    USE_CACHED_DATASET = True\n","    CACHE_DIR = \"/content/drive/My Drive/Colab Notebooks/LRLs/yoruba/cached_datasets\"\n","    FORCE_RETOKENIZE = True\n","    MAX_EXAMPLES = 100000\n","\n","    LORA_R = 16\n","    LORA_ALPHA = 32\n","    LORA_DROPOUT = 0.1\n","    LORA_TARGET_MODULES = [\n","        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","        \"gate_proj\", \"up_proj\", \"down_proj\",\n","        \"lm_head\", \"embed_tokens\"\n","    ]\n","\n","    PRETRAINED_EMBEDDING_LR = 5e-6\n","    NEW_EMBEDDING_LR = 2e-5\n","    LORA_LR = 5e-5\n","    BASE_MODEL_LR = 5e-5\n","\n","    BATCH_SIZE = 4\n","    GRADIENT_ACCUMULATION_STEPS = 4\n","    MAX_SEQ_LENGTH = 1024\n","    WEIGHT_DECAY = 0.01\n","    MAX_GRAD_NORM = 1.0\n","    NUM_EPOCHS = 1\n","    MAX_STEPS = 10000\n","\n","    EMBEDDING_REGULARIZATION_WEIGHT = 0.0001\n","    GRADIENT_SCALING_FACTOR = 0.5\n","\n","    SAVE_STEPS = 200\n","    EVAL_STEPS = 100\n","    LOGGING_STEPS = 10\n","    SAVE_TOTAL_LIMIT = 3\n","    EARLY_STOPPING_PATIENCE = 8\n","    EVAL_BATCH_SIZE = 2\n","    MAX_EVAL_SAMPLES = 200\n","\n","    SEED = 42\n","    USE_GRADIENT_CHECKPOINTING = True\n","    MERGE_AND_SAVE_FINAL = True\n","    RESUME_FROM_CHECKPOINT = None\n","\n","class TextDataset(Dataset):\n","    def __init__(\n","        self,\n","        data_path: str,\n","        tokenizer: ExtendedTokenizer,\n","        max_length: int = 512,\n","        stride: int = 256,\n","        text_column: str = \"text\",\n","        cache_dir: Optional[str] = None,\n","        use_cache: bool = True,\n","        force_retokenize: bool = False,\n","        max_examples: Optional[int] = None\n","    ):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.stride = stride\n","        self.text_column = text_column\n","        self.examples = []\n","\n","        logger.info(f\"Loading dataset from {data_path}\")\n","\n","        if cache_dir:\n","            Path(cache_dir).mkdir(parents=True, exist_ok=True)\n","\n","        cache_loaded = False\n","        if use_cache and cache_dir and not force_retokenize:\n","            cache_loaded = self._try_load_from_cache(data_path, cache_dir)\n","\n","        if not cache_loaded:\n","            self._load_dataset(data_path)\n","            if use_cache and cache_dir and self.examples:\n","                self._save_to_cache(data_path, cache_dir)\n","\n","        if max_examples and len(self.examples) > max_examples:\n","            self.examples = self.examples[:max_examples]\n","\n","        logger.info(f\"Loaded {len(self.examples)} examples\")\n","\n","    def _get_cache_key(self, data_path: str) -> str:\n","        cache_string = f\"{data_path}_{self.max_length}_{self.stride}_{self.tokenizer.vocab_size}\"\n","        return hashlib.md5(cache_string.encode()).hexdigest()\n","\n","    def _try_load_from_cache(self, data_path: str, cache_dir: str) -> bool:\n","        try:\n","            cache_key = self._get_cache_key(data_path)\n","            cache_path = Path(cache_dir) / f\"{Path(data_path).name}_{cache_key}.pkl\"\n","\n","            if not cache_path.exists():\n","                return False\n","\n","            logger.info(f\"Loading from cache: {cache_path}\")\n","            with open(cache_path, 'rb') as f:\n","                cache_data = pickle.load(f)\n","\n","            if cache_data.get('vocab_size') != self.tokenizer.vocab_size:\n","                logger.warning(\"Cache vocab size mismatch, will retokenize\")\n","                return False\n","\n","            self.examples = cache_data['examples']\n","            logger.info(f\"Loaded {len(self.examples)} examples from cache\")\n","            return True\n","\n","        except Exception as e:\n","            logger.warning(f\"Failed to load cache: {e}\")\n","            return False\n","\n","    def _save_to_cache(self, data_path: str, cache_dir: str):\n","        try:\n","            cache_key = self._get_cache_key(data_path)\n","            cache_path = Path(cache_dir) / f\"{Path(data_path).name}_{cache_key}.pkl\"\n","\n","            cache_data = {\n","                'examples': self.examples,\n","                'vocab_size': self.tokenizer.vocab_size,\n","                'max_length': self.max_length,\n","                'stride': self.stride,\n","                'timestamp': datetime.now().isoformat()\n","            }\n","\n","            with open(cache_path, 'wb') as f:\n","                pickle.dump(cache_data, f)\n","\n","            logger.info(f\"Saved cache: {cache_path}\")\n","        except Exception as e:\n","            logger.warning(f\"Failed to save cache: {e}\")\n","\n","    def _load_dataset(self, data_path: str):\n","        path = Path(data_path)\n","        if not path.exists():\n","            raise FileNotFoundError(f\"Dataset path {data_path} does not exist\")\n","\n","        try:\n","            from datasets import load_from_disk\n","            dataset = load_from_disk(data_path)\n","            if hasattr(dataset, 'column_names'):\n","                self._process_hf_dataset(dataset)\n","            elif isinstance(dataset, dict):\n","                split_data = next(iter(dataset.values()))\n","                self._process_hf_dataset(split_data)\n","            return\n","        except Exception:\n","            pass\n","\n","        text_files = list(path.glob(\"*.txt\"))\n","        if text_files:\n","            all_texts = []\n","            for file_path in text_files:\n","                with open(file_path, 'r', encoding='utf-8') as f:\n","                    text = f.read().strip()\n","                    if text:\n","                        all_texts.append(text)\n","            self._tokenize_texts(all_texts)\n","            return\n","\n","        json_files = list(path.glob(\"*.json\")) + list(path.glob(\"*.jsonl\"))\n","        if json_files:\n","            all_texts = []\n","            for file_path in json_files:\n","                with open(file_path, 'r', encoding='utf-8') as f:\n","                    if file_path.suffix == '.jsonl':\n","                        for line in f:\n","                            data = json.loads(line.strip())\n","                            text = self._extract_text_from_json(data)\n","                            if text:\n","                                all_texts.append(text)\n","                    else:\n","                        data = json.load(f)\n","                        if isinstance(data, list):\n","                            for item in data:\n","                                text = self._extract_text_from_json(item)\n","                                if text:\n","                                    all_texts.append(text)\n","                        else:\n","                            text = self._extract_text_from_json(data)\n","                            if text:\n","                                all_texts.append(text)\n","            self._tokenize_texts(all_texts)\n","            return\n","\n","        raise ValueError(f\"Could not load dataset from {data_path}\")\n","\n","    def _extract_text_from_json(self, data: dict) -> str:\n","        possible_keys = [self.text_column, 'text', 'content', 'document', 'sentence']\n","        for key in possible_keys:\n","            if key in data and data[key]:\n","                return str(data[key]).strip()\n","\n","        for key, value in data.items():\n","            if isinstance(value, str) and value.strip():\n","                return value.strip()\n","        return \"\"\n","\n","    def _process_hf_dataset(self, dataset):\n","        text_column = None\n","        possible_columns = [self.text_column, 'text', 'content', 'document']\n","\n","        for col in possible_columns:\n","            if col in dataset.column_names:\n","                text_column = col\n","                break\n","\n","        if text_column is None:\n","            raise ValueError(f\"No text column found in {dataset.column_names}\")\n","\n","        texts = [item[text_column] for item in dataset if item.get(text_column)]\n","        self._tokenize_texts(texts)\n","\n","    def _tokenize_texts(self, texts: List[str]):\n","        logger.info(f\"Tokenizing {len(texts)} texts...\")\n","\n","        valid_texts = []\n","        for text in texts:\n","            text = text.strip() if text else \"\"\n","            if len(text) >= 10:\n","                valid_texts.append(text)\n","\n","        if not valid_texts:\n","            logger.warning(\"No valid texts found for tokenization\")\n","            return\n","\n","        logger.info(f\"Processing {len(valid_texts)} valid texts after filtering\")\n","\n","        batch_size = 100\n","        total_batches = (len(valid_texts) + batch_size - 1) // batch_size\n","\n","        processed_count = 0\n","        error_count = 0\n","\n","        batch_progress = tqdm(range(total_batches), desc=\"Tokenizing batches\", unit=\"batch\")\n","\n","        for batch_idx in batch_progress:\n","            start_idx = batch_idx * batch_size\n","            end_idx = min(start_idx + batch_size, len(valid_texts))\n","            batch_texts = valid_texts[start_idx:end_idx]\n","\n","            batch_examples = []\n","\n","            for text in batch_texts:\n","                try:\n","                    tokens = self.tokenizer.encode(text, add_special_tokens=True, truncation=False)\n","                    if len(tokens) < 10:\n","                        continue\n","\n","                    pretrained_count, new_count = self.tokenizer.analyze_token_types(tokens)\n","\n","                    if len(tokens) <= self.max_length:\n","                        batch_examples.append({\n","                            'tokens': tokens,\n","                            'pretrained_count': pretrained_count,\n","                            'new_count': new_count\n","                        })\n","                    else:\n","                        for i in range(0, len(tokens) - self.max_length + 1, self.stride):\n","                            chunk = tokens[i:i + self.max_length]\n","                            chunk_pretrained, chunk_new = self.tokenizer.analyze_token_types(chunk)\n","                            batch_examples.append({\n","                                'tokens': chunk,\n","                                'pretrained_count': chunk_pretrained,\n","                                'new_count': chunk_new\n","                            })\n","\n","                    processed_count += 1\n","\n","                except Exception as e:\n","                    error_count += 1\n","                    if error_count <= 5:\n","                        logger.warning(f\"Error tokenizing text {processed_count + error_count}: {str(e)[:100]}\")\n","\n","            self.examples.extend(batch_examples)\n","\n","            batch_progress.set_postfix({\n","                'examples': len(self.examples),\n","                'processed': processed_count,\n","                'errors': error_count\n","            })\n","\n","        logger.info(f\"Tokenization complete: {len(self.examples)} examples created from {processed_count} texts\")\n","        if error_count > 0:\n","            logger.warning(f\"Encountered {error_count} tokenization errors\")\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, idx):\n","        example = self.examples[idx]\n","        tokens = example['tokens']\n","\n","        if len(tokens) > self.max_length:\n","            tokens = tokens[:self.max_length]\n","        elif len(tokens) < self.max_length:\n","            pad_token = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n","            tokens = tokens + [pad_token] * (self.max_length - len(tokens))\n","\n","        input_ids = torch.tensor(tokens, dtype=torch.long)\n","        labels = input_ids.clone()\n","        attention_mask = torch.ones_like(input_ids)\n","\n","        original_length = len(example['tokens'])\n","        if original_length < self.max_length:\n","            attention_mask[original_length:] = 0\n","            labels[original_length:] = -100\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels,\n","            'pretrained_count': example.get('pretrained_count', 0),\n","            'new_count': example.get('new_count', 0)\n","        }\n","\n","def collate_fn(batch):\n","    max_len = max(len(item['input_ids']) for item in batch)\n","\n","    input_ids = []\n","    attention_mask = []\n","    labels = []\n","    pretrained_counts = []\n","    new_counts = []\n","\n","    for item in batch:\n","        seq_len = len(item['input_ids'])\n","        if seq_len < max_len:\n","            pad_length = max_len - seq_len\n","            input_ids.append(torch.cat([item['input_ids'], torch.zeros(pad_length, dtype=torch.long)]))\n","            attention_mask.append(torch.cat([item['attention_mask'], torch.zeros(pad_length, dtype=torch.long)]))\n","            labels.append(torch.cat([item['labels'], torch.full((pad_length,), -100, dtype=torch.long)]))\n","        else:\n","            input_ids.append(item['input_ids'])\n","            attention_mask.append(item['attention_mask'])\n","            labels.append(item['labels'])\n","\n","        pretrained_counts.append(item.get('pretrained_count', 0))\n","        new_counts.append(item.get('new_count', 0))\n","\n","    return {\n","        'input_ids': torch.stack(input_ids),\n","        'attention_mask': torch.stack(attention_mask),\n","        'labels': torch.stack(labels),\n","        'pretrained_counts': torch.tensor(pretrained_counts),\n","        'new_counts': torch.tensor(new_counts)\n","    }\n","\n","def setup_peft_model(model, config: TrainingConfig):\n","    lora_config = LoraConfig(\n","        r=config.LORA_R,\n","        lora_alpha=config.LORA_ALPHA,\n","        target_modules=config.LORA_TARGET_MODULES,\n","        lora_dropout=config.LORA_DROPOUT,\n","        bias=\"none\",\n","        task_type=TaskType.CAUSAL_LM,\n","        modules_to_save=[\"embed_tokens\", \"lm_head\"]\n","    )\n","\n","    model = get_peft_model(model, lora_config)\n","\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    total_params = sum(p.numel() for p in model.parameters())\n","\n","    logger.info(f\"Trainable params: {trainable_params:,}\")\n","    logger.info(f\"Total params: {total_params:,}\")\n","    logger.info(f\"Trainable %: {100 * trainable_params / total_params:.2f}\")\n","\n","    return model\n","\n","def setup_multi_optimizer(model, tokenizer: ExtendedTokenizer, config: TrainingConfig):\n","    pretrained_embedding_params = []\n","    lora_params = []\n","    other_params = []\n","\n","    for name, param in model.named_parameters():\n","        if not param.requires_grad:\n","            continue\n","\n","        if 'lora' in name.lower():\n","            lora_params.append(param)\n","        elif any(emb in name.lower() for emb in ['embed_tokens', 'lm_head']):\n","            pretrained_embedding_params.append(param)\n","        else:\n","            other_params.append(param)\n","\n","    param_groups = []\n","\n","    if pretrained_embedding_params:\n","        param_groups.append({\n","            'params': pretrained_embedding_params,\n","            'lr': config.PRETRAINED_EMBEDDING_LR,\n","            'weight_decay': config.WEIGHT_DECAY * 0.5,\n","            'name': 'embeddings'\n","        })\n","\n","    if lora_params:\n","        param_groups.append({\n","            'params': lora_params,\n","            'lr': config.LORA_LR,\n","            'weight_decay': config.WEIGHT_DECAY,\n","            'name': 'lora'\n","        })\n","\n","    if other_params:\n","        param_groups.append({\n","            'params': other_params,\n","            'lr': config.BASE_MODEL_LR,\n","            'weight_decay': config.WEIGHT_DECAY,\n","            'name': 'base'\n","        })\n","\n","    optimizer = torch.optim.AdamW(param_groups, eps=1e-8)\n","\n","    logger.info(f\"Optimizer groups: {len(param_groups)}\")\n","    for group in param_groups:\n","        logger.info(f\"  {group['name']}: LR={group['lr']}\")\n","\n","    return optimizer\n","\n","def test_sample_perplexity(model, tokenizer, device):\n","    model.eval()\n","\n","    test_sentences = [\n","        \"The quick brown fox jumps over the lazy dog.\",\n","        \"Èdè Yorùbá jẹ́ èdè àbínibí wa lórí ilẹ̀ Yorùbá.\"\n","    ]\n","\n","    logger.info(\"Testing sample perplexity:\")\n","\n","    with torch.no_grad():\n","        for sentence in test_sentences:\n","            inputs = tokenizer.encode(sentence, return_tensors=\"pt\", add_special_tokens=True)\n","            if hasattr(inputs, 'input_ids'):\n","                input_ids = inputs.input_ids.to(device)\n","                attention_mask = inputs.attention_mask.to(device)\n","            else:\n","                input_ids = inputs.to(device)\n","                attention_mask = torch.ones_like(input_ids)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n","            loss = outputs.loss\n","            ppl = torch.exp(loss).item()\n","\n","            logger.info(f\"  '{sentence}' -> PPL: {ppl:.2f}\")\n","\n","    model.train()\n","\n","def evaluate_model(model, eval_dataloader, device, tokenizer, config):\n","    model.eval()\n","\n","    test_sample_perplexity(model, tokenizer, device)\n","\n","    total_loss = 0\n","    total_tokens = 0\n","    pretrained_tokens = 0\n","    new_tokens = 0\n","\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(tqdm(eval_dataloader, desc=\"Evaluating\", leave=True, dynamic_ncols=True, miniters=1)):\n","            if config.MAX_EVAL_SAMPLES > 0 and batch_idx * config.EVAL_BATCH_SIZE >= config.MAX_EVAL_SAMPLES:\n","                break\n","\n","            batch_device = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n","\n","            outputs = model(\n","                input_ids=batch_device['input_ids'],\n","                attention_mask=batch_device['attention_mask'],\n","                labels=batch_device['labels']\n","            )\n","\n","            loss = outputs.loss\n","            if not torch.isnan(loss):\n","                num_tokens = (batch_device['labels'] != -100).sum().item()\n","                total_loss += loss.item() * num_tokens\n","                total_tokens += num_tokens\n","\n","                pretrained_tokens += batch.get('pretrained_counts', torch.zeros(len(batch['input_ids']))).sum().item()\n","                new_tokens += batch.get('new_counts', torch.zeros(len(batch['input_ids']))).sum().item()\n","\n","    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n","    perplexity = np.exp(avg_loss)\n","\n","    model.train()\n","\n","    return {\n","        'eval_loss': avg_loss,\n","        'eval_perplexity': perplexity,\n","        'total_tokens': total_tokens,\n","        'pretrained_tokens': pretrained_tokens,\n","        'new_tokens': new_tokens\n","    }\n","\n","def apply_gradient_scaling(model, tokenizer: ExtendedTokenizer, config: TrainingConfig):\n","    if not hasattr(model, 'get_input_embeddings'):\n","        return\n","\n","    input_embeddings = model.get_input_embeddings()\n","    if input_embeddings is None or not hasattr(input_embeddings, 'weight') or input_embeddings.weight.grad is None:\n","        return\n","\n","    with torch.no_grad():\n","        pretrained_indices = list(tokenizer.pretrained_token_ids)\n","        if pretrained_indices and len(pretrained_indices) < input_embeddings.weight.size(0):\n","            input_embeddings.weight.grad[pretrained_indices] *= config.GRADIENT_SCALING_FACTOR\n","\n","def compute_regularization_loss(model, config: TrainingConfig):\n","    reg_loss = torch.tensor(0.0, device=next(model.parameters()).device)\n","\n","    if hasattr(model, 'get_input_embeddings'):\n","        input_embeddings = model.get_input_embeddings()\n","        if input_embeddings is not None and hasattr(input_embeddings, 'weight'):\n","            reg_loss += torch.norm(input_embeddings.weight, p=2) * config.EMBEDDING_REGULARIZATION_WEIGHT\n","\n","    return reg_loss\n","\n","def save_checkpoint(model, tokenizer, optimizer, scheduler, epoch, step, best_eval_loss, config, is_best=False):\n","    checkpoint_dir = Path(config.OUTPUT_DIR) / f\"checkpoint-{step}\"\n","    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n","\n","    model.save_pretrained(checkpoint_dir)\n","    tokenizer.save_pretrained(checkpoint_dir)\n","\n","    training_state = {\n","        'epoch': epoch,\n","        'step': step,\n","        'best_eval_loss': best_eval_loss,\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","    }\n","\n","    torch.save(training_state, checkpoint_dir / 'training_state.pt')\n","\n","    if is_best:\n","        best_dir = Path(config.OUTPUT_DIR) / \"best_model\"\n","        if best_dir.exists():\n","            import shutil\n","            shutil.rmtree(best_dir)\n","        import shutil\n","        shutil.copytree(checkpoint_dir, best_dir)\n","\n","    checkpoints = sorted([d for d in Path(config.OUTPUT_DIR).glob(\"checkpoint-*\")],\n","                        key=lambda x: int(x.name.split(\"-\")[1]))\n","\n","    if len(checkpoints) > config.SAVE_TOTAL_LIMIT:\n","        for checkpoint in checkpoints[:-config.SAVE_TOTAL_LIMIT]:\n","            import shutil\n","            shutil.rmtree(checkpoint)\n","\n","    logger.info(f\"Saved checkpoint at step {step}\")\n","\n","def create_combined_vocabulary(base_tokenizer: AutoTokenizer, bpe_tokenizer: BPETokenizer):\n","    logger.info(\"Creating combined vocabulary...\")\n","\n","    combined_vocab = base_tokenizer.get_vocab().copy()\n","    base_size = len(combined_vocab)\n","    bpe_vocab = bpe_tokenizer.get_vocab()\n","\n","    id_offset = base_size\n","    new_tokens_added = 0\n","\n","    for bpe_token, _ in bpe_vocab.items():\n","        if bpe_token not in combined_vocab:\n","            combined_vocab[bpe_token] = id_offset + new_tokens_added\n","            new_tokens_added += 1\n","\n","    logger.info(f\"Base tokens: {base_size:,}, New tokens: {new_tokens_added:,}, Combined: {len(combined_vocab):,}\")\n","    return combined_vocab\n","\n","def train():\n","    hf_token = getpass.getpass(\"Enter Hugging Face token: \")\n","\n","    config = TrainingConfig()\n","    set_seed(config.SEED)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    logger.info(f\"Device: {device}\")\n","\n","    Path(config.OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n","\n","    logger.info(\"Loading initialized model and tokenizers...\")\n","\n","    base_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\", token=hf_token)\n","    bpe_tokenizer = BPETokenizer(model_path=config.BPE_MODEL_PATH)\n","\n","    vocab_file = Path(config.TOKENIZER_PATH) / 'vocab.json'\n","\n","    if vocab_file.exists():\n","        logger.info(\"Loading existing combined vocabulary...\")\n","        with open(vocab_file, 'r', encoding='utf-8') as f:\n","            combined_vocab = json.load(f)\n","        logger.info(f\"Loaded combined vocabulary: {len(combined_vocab):,} tokens\")\n","    else:\n","        logger.info(\"Creating combined vocabulary from base + BPE...\")\n","        combined_vocab = create_combined_vocabulary(base_tokenizer, bpe_tokenizer)\n","\n","    tokenizer = ExtendedTokenizer(base_tokenizer, bpe_tokenizer, combined_vocab)\n","\n","    model_config_file = Path(config.MODEL_PATH) / 'config.json'\n","    if not model_config_file.exists():\n","        raise FileNotFoundError(f\"Model config not found at {model_config_file}\")\n","\n","    with open(model_config_file, 'r') as f:\n","        model_config = json.load(f)\n","\n","    model_vocab_size = model_config.get('vocab_size', 0)\n","    tokenizer_vocab_size = len(combined_vocab)\n","\n","    logger.info(f\"Model vocab size: {model_vocab_size:,}\")\n","    logger.info(f\"Tokenizer vocab size: {tokenizer_vocab_size:,}\")\n","\n","    if abs(model_vocab_size - tokenizer_vocab_size) <= 10:\n","        logger.info(\"Vocabulary sizes match (within alignment tolerance)\")\n","        model = GemmaForCausalLM.from_pretrained(\n","            config.MODEL_PATH,\n","            dtype=torch.float32,\n","            device_map=\"auto\",\n","            trust_remote_code=True,\n","            local_files_only=True\n","        )\n","    else:\n","        raise ValueError(f\"Vocabulary size mismatch: model={model_vocab_size:,}, tokenizer={tokenizer_vocab_size:,}\")\n","\n","    logger.info(f\"Model vocab size: {model.config.vocab_size}\")\n","    logger.info(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n","\n","    test_text = \"Hello Báwo ni world\"\n","    tokens1 = tokenizer.tokenize(test_text)\n","    tokens2 = tokenizer.tokenize(test_text)\n","    tokens3 = tokenizer.tokenize(test_text)\n","    assert tokens1 == tokens2 == tokens3, \"Tokenizer is not consistent\"\n","\n","    tokens = tokenizer.encode(test_text, add_special_tokens=False)\n","    decoded = tokenizer.decode(tokens)\n","    pretrained, new = tokenizer.analyze_token_types(tokens)\n","    logger.info(f\"Test: '{test_text}' -> {len(tokens)} tokens -> '{decoded}'\")\n","    logger.info(f\"Token analysis: {pretrained} pretrained, {new} new\")\n","    logger.info(\"Tokenizer consistency verified\")\n","\n","    model = setup_peft_model(model, config)\n","\n","    if config.USE_GRADIENT_CHECKPOINTING:\n","        model.enable_input_require_grads()\n","        model.gradient_checkpointing_enable()\n","\n","    logger.info(\"Loading datasets...\")\n","\n","    train_dataset = TextDataset(\n","        config.TRAIN_DATASET_PATH,\n","        tokenizer,\n","        max_length=config.MAX_SEQ_LENGTH,\n","        text_column=config.TEXT_COLUMN_NAME,\n","        cache_dir=config.CACHE_DIR,\n","        use_cache=config.USE_CACHED_DATASET,\n","        force_retokenize=config.FORCE_RETOKENIZE,\n","        max_examples=config.MAX_EXAMPLES\n","    )\n","\n","    eval_dataset = TextDataset(\n","        config.VAL_DATASET_PATH,\n","        tokenizer,\n","        max_length=config.MAX_SEQ_LENGTH,\n","        text_column=config.TEXT_COLUMN_NAME,\n","        cache_dir=config.CACHE_DIR,\n","        use_cache=config.USE_CACHED_DATASET,\n","        max_examples=1000\n","    )\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=config.BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=0,\n","        collate_fn=collate_fn\n","    )\n","\n","    eval_dataloader = DataLoader(\n","        eval_dataset,\n","        batch_size=config.EVAL_BATCH_SIZE,\n","        shuffle=False,\n","        num_workers=0,\n","        collate_fn=collate_fn\n","    )\n","\n","    num_update_steps = len(train_dataloader) // config.GRADIENT_ACCUMULATION_STEPS\n","    max_steps = min(config.MAX_STEPS, num_update_steps * config.NUM_EPOCHS) if config.MAX_STEPS > 0 else num_update_steps * config.NUM_EPOCHS\n","\n","    optimizer = setup_multi_optimizer(model, tokenizer, config)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=500,\n","        num_training_steps=max_steps\n","    )\n","\n","    logger.info(f\"Total training steps: {max_steps}\")\n","\n","    global_step = 0\n","    best_eval_loss = float('inf')\n","    patience_counter = 0\n","\n","    logger.info(\"Starting training...\")\n","\n","    initial_metrics = evaluate_model(model, eval_dataloader, device, tokenizer, config)\n","    logger.info(f\"Initial eval loss: {initial_metrics['eval_loss']:.4f}, PPL: {initial_metrics['eval_perplexity']:.2f}\")\n","\n","    model.train()\n","\n","    for epoch in range(config.NUM_EPOCHS):\n","        epoch_loss = 0\n","        epoch_tokens = 0\n","\n","        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{config.NUM_EPOCHS}\")\n","\n","        for step, batch in enumerate(progress_bar):\n","            batch_device = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n","\n","            outputs = model(\n","                input_ids=batch_device['input_ids'],\n","                attention_mask=batch_device['attention_mask'],\n","                labels=batch_device['labels']\n","            )\n","\n","            loss = outputs.loss / config.GRADIENT_ACCUMULATION_STEPS\n","            reg_loss = compute_regularization_loss(model, config)\n","            total_loss = loss + reg_loss\n","\n","            if torch.isnan(total_loss):\n","                logger.warning(\"NaN loss detected, skipping batch\")\n","                optimizer.zero_grad()\n","                continue\n","\n","            total_loss.backward()\n","\n","            num_tokens = (batch_device['labels'] != -100).sum().item()\n","            epoch_loss += loss.item() * config.GRADIENT_ACCUMULATION_STEPS * num_tokens\n","            epoch_tokens += num_tokens\n","\n","            if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n","                apply_gradient_scaling(model, tokenizer, config)\n","                clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n","\n","                optimizer.step()\n","                scheduler.step()\n","                optimizer.zero_grad()\n","                global_step += 1\n","\n","                if global_step % config.LOGGING_STEPS == 0:\n","                    avg_loss = epoch_loss / epoch_tokens if epoch_tokens > 0 else 0\n","                    perplexity = np.exp(avg_loss)\n","\n","                    progress_bar.set_postfix({\n","                        'loss': f'{avg_loss:.4f}',\n","                        'ppl': f'{perplexity:.2f}',\n","                        'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n","                    })\n","\n","                    logger.info(f\"Step {global_step} | Loss: {avg_loss:.4f} | PPL: {perplexity:.2f}\")\n","\n","                if global_step % config.EVAL_STEPS == 0:\n","                    eval_metrics = evaluate_model(model, eval_dataloader, device, tokenizer, config)\n","\n","                    logger.info(f\"Eval - Loss: {eval_metrics['eval_loss']:.4f}, PPL: {eval_metrics['eval_perplexity']:.2f}\")\n","                    logger.info(f\"Tokens - Pretrained: {eval_metrics['pretrained_tokens']:,}, New: {eval_metrics['new_tokens']:,}\")\n","\n","                    is_best = eval_metrics['eval_loss'] < best_eval_loss\n","                    if is_best:\n","                        best_eval_loss = eval_metrics['eval_loss']\n","                        patience_counter = 0\n","                        logger.info(f\"New best model! Loss: {best_eval_loss:.4f}\")\n","                    else:\n","                        patience_counter += 1\n","\n","                    save_checkpoint(model, tokenizer, optimizer, scheduler, epoch, global_step, best_eval_loss, config, is_best)\n","\n","                    if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n","                        logger.info(\"Early stopping triggered!\")\n","                        break\n","\n","                elif global_step % config.SAVE_STEPS == 0:\n","                    save_checkpoint(model, tokenizer, optimizer, scheduler, epoch, global_step, best_eval_loss, config)\n","\n","                if global_step >= max_steps:\n","                    break\n","\n","        if patience_counter >= config.EARLY_STOPPING_PATIENCE or global_step >= max_steps:\n","            break\n","\n","    logger.info(\"Saving final model...\")\n","    final_dir = Path(config.OUTPUT_DIR) / \"final_model\"\n","    model.save_pretrained(final_dir)\n","    tokenizer.save_pretrained(final_dir)\n","\n","    if config.MERGE_AND_SAVE_FINAL:\n","        logger.info(\"Merging LoRA and saving...\")\n","        merged_model = model.merge_and_unload()\n","        merged_dir = Path(config.OUTPUT_DIR) / \"final_merged_model\"\n","        merged_model.save_pretrained(merged_dir)\n","        tokenizer.save_pretrained(merged_dir)\n","\n","    final_metrics = evaluate_model(model, eval_dataloader, device, tokenizer, config)\n","\n","    summary = {\n","        'total_steps': global_step,\n","        'best_eval_loss': best_eval_loss,\n","        'final_eval_loss': final_metrics['eval_loss'],\n","        'final_eval_perplexity': final_metrics['eval_perplexity'],\n","        'model_vocab_size': model.config.vocab_size,\n","        'tokenizer_vocab_size': tokenizer.vocab_size,\n","        'pretrained_tokens': len(tokenizer.pretrained_token_ids),\n","        'new_tokens': len(tokenizer.new_token_ids)\n","    }\n","\n","    with open(final_dir / 'training_summary.json', 'w') as f:\n","        json.dump(summary, f, indent=2)\n","\n","    logger.info(\"Training completed.\")\n","    logger.info(f\"Best eval loss: {best_eval_loss:.4f}\")\n","    logger.info(f\"Final eval loss: {final_metrics['eval_loss']:.4f}\")\n","    logger.info(f\"Final perplexity: {final_metrics['eval_perplexity']:.2f}\")\n","    logger.info(f\"Pretrained tokens: {len(tokenizer.pretrained_token_ids):,}\")\n","    logger.info(f\"New tokens: {len(tokenizer.new_token_ids):,}\")\n","\n","if __name__ == \"__main__\":\n","    train()"],"metadata":{"id":"-wcAWCqT6Mwh"},"execution_count":null,"outputs":[]}]}